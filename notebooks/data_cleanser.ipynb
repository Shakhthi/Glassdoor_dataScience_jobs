{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25095b61",
   "metadata": {},
   "source": [
    "<h3> DS jobs Dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ed2ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3873957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"D:\\4.Data Wrangling\\Datasets\\Ds_jobs\\Glassdoor_Job_Postings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "376adceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "company",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "job_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "job_description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "location",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "salary_avg_estimate",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "salary_estimate_payperiod",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "company_size",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company_founded",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "employment_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "industry",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sector",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "revenue",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "career_opportunities_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "comp_and_benefits_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "culture_and_values_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "senior_management_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "work_life_balance_rating",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e8c44fa4-b45d-4c32-a176-c4e3241a7832",
       "rows": [
        [
         "0",
         "ABB",
         "Junior Data Analyst",
         "4.0",
         "Junior Data Analyst\nTake your next career step at ABB with a global team that is energizing the transformation of society and industry to achieve a more productive, sustainable future. At ABB, we have the clear goal of driving diversity and inclusion across all dimensions: gender, LGBTQ+, abilities, ethnicity and generations. Together, we are embarking on a journey where each and every one of us, individually and collectively, welcomes and celebrates individual differences.\n\nYou will be working as Junior Data Analyst and will be part of Process Automation Business Area for Measurement and Analytics division based in Bangalore, India. In this role you will be reporting to COE Data Management and Analytics Manager and will be responsible for performing data analytics activities and developing analytical solutions to enable business in strategy development for existing and potential products, systems or services.\nYour responsibilities\nUsing data systems to help gather, measure, organize and analyze data, providing sound market and competitive intelligence analysis related to market and trends\nPreparing analysis of internal sales, technical and financial data, interprets resulting data using statistical tools and making recommendations to Sales, Marketing, Finance and Product Management\nMaking diagnostic and predictive recommendations to management of existing gaps and new opportunities for growth, identifying global trends and patterns across various dimensions\nExtracting and analyzing data from sales or financial tools and preparing reports for management to give insights on trends, patterns and predictions across geographies\nAssisting in the development of analytical tools and solutions to support management to drive key business decisions. Training business stakeholders on proper usage of dashboards and monitors on-going data quality\nYour background\nB.E or B. Tech or BCA or Bachelor's in Data Science\nMinimum 1 to 2 years of experience in Data Analytics and visualization\nHands on experience in Basic or Advanced Excel, Basic Statistics, ETL tools, Power BI(Basic),Basic SQL,ML and Python\nAbility to adapt, problem solving skills and give recommendations to stakeholders\nEffective time management while handling business critical tasks\nGood teamwork and collaboration with cross functional teams to meet business deliverables\nGood communication skills\nMore about us\nABB's Measurement & Analytics division is among the world's leading manufacturers and suppliers of smart instrumentation and analyzers. With thousands of experts around the world and high-performance digital technology, ABB's team is dedicated to making measurement easy for its industrial and energy customers to let them operate more efficiently and profitably. We look forward to receiving your application (documents submitted in English are appreciated). If you want to discover more about ABB, take another look at our website www.abb.com. It has come to our attention that the name of ABB is being used for asking candidates to make payments for job opportunities (interviews, offers). Please be advised that ABB makes no such requests. All our open positions are made available on our career portal for all fitting the criteria to apply. ABB does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection to recruitment with ABB, even if is claimed that the money is refundable. ABB is not liable for such transactions. For current open positions you can visit our career website https://global.abb/group/en/careers and apply. Please refer to detailed recruitment fraud caution notice using the link https://global.abb/group/en/careers/how-to-apply/fraud-warning Work model: on site #LI-onsite",
         "Bengaluru",
         "₹3,25,236",
         "/yr (est.)",
         "10000+ Employees",
         "1883",
         "Company - Public",
         "Electronics Manufacturing",
         "Manufacturing",
         "$10+ billion (USD)",
         "3.7",
         "3.6",
         "4.0",
         "3.5",
         "3.9"
        ],
        [
         "1",
         "Philips",
         "Data Scientist - AI/ML",
         "4.0",
         "Job Title\nData Scientist - AI/ML\nJob Description\nJob title: Data Scientist - AI/ML\nOverall responsibilities\nProvide support with guidance to solve complex problems in medical image processing and deliver PoC with TRL level ready for NPI program.\nSupport collaboration with clinical scientists, AD leader and universities to drive advanced development projects on medical image analytics from R&D perspective.\nSupport NPI development and address time-pressing complex LCM challenges with image processing / IQ using expertise.\nHigher degree of learning attitude to absorb newer knowledge on medical image pre- and / or post-processing.\nSpecific responsibilities\nHands-on work in some of the following tools, packages, technical areas and modeling techniques:\nKeras, Tensorflow, Pytorch, Scikit-learn, Scikit-image, OpenCV, pydicom\nMatlab, python, R, Java/C/C++\nConvolution Neural Networks, other statistical modeling & classification approaches (regression, SVM, PCA)\nOpensource medical image visualization and segmentation tools like ITK, VTK\nMedical image post-processing techniques such as noise reduction, contrast enhancement, bone suppression, stitching, ranging. Knowledge of overall IQ (image quality) analysis of diagnostic X-ray images is a plus.\nMedical image pre-processing (gain, offset correction, EMI/EMC correction)\nAI/ML modeling / computer vision using video from 3\n\nAbout Philips\nWe are a health technology company. We built our entire company around the belief that every human matters, and we won't stop until everybody everywhere has access to the quality healthcare that we all deserve. Do the work of your life to help the lives of others.\nLearn more about our business.\nDiscover our rich and exciting history.\nLearn more about our purpose.\n\nIf you’re interested in this role and have many, but not all, of the experiences needed, we encourage you to apply. You may still be the right candidate for this or other opportunities at Philips. Learn more about our commitment to diversity and inclusion here.",
         "Bengaluru",
         null,
         null,
         "10000+ Employees",
         "1891",
         "Company - Public",
         "Healthcare Services & Hospitals",
         "Healthcare",
         "$10+ billion (USD)",
         "3.8",
         "3.7",
         "4.0",
         "3.5",
         "4.0"
        ],
        [
         "2",
         "HSBC",
         "Data Science GSC’s",
         "3.9",
         "Job description\nGraduate/ Post-graduate degree with relevant field (Finance/Economics/Operation Research/Statistics/Mathematics/ B.Tech/ MBA/ Data Science)\nCertification courses in Data Science/Risk Management or similar areas will be added advantage\nExcellent coding skills in latest statistical packages/apps SAS/R/Python\nTerm papers with demonstrated skills set in modelling, forecasting and optimization\nExcellent quantitative aptitude and hands-on proficiency with analytical tools such as SAS, R, Python\nHighly focused on project delivery, attention to detail\nExcellent written and verbal communication skills\nStrong collaborative, influencing skills\nStrong analytical and problem solving skills, open minded, flexible, pragmatic\nRequirements\nQualifications - External\n\nGraduate/ Post-graduate degree with relevant field (Finance/Economics/Operation Research/Statistics/Mathematics/ B.Tech/ MBA/ Data Science)\nCertification courses in Data Science/Risk Management or similar areas will be added advantage\nExcellent coding skills in latest statistical packages/apps SAS/R/Python\nTerm papers with demonstrated skills set in modelling, forecasting and optimization\nExcellent quantitative aptitude and hands-on proficiency with analytical tools such as SAS, R, Python\nHighly focused on project delivery, attention to detail\nExcellent written and verbal communication skills\nStrong collaborative, influencing skills\nStrong analytical and problem solving skills, open minded, flexible, pragmatic\nYou’ll achieve more at HSBC\n\n\nHSBC is an equal opportunity employer committed to building a culture where all employees are valued, respected and opinions count. We take pride in providing a workplace that fosters continuous professional development, flexible working and, opportunities to grow within an inclusive and diverse environment. We encourage applications from all suitably qualified persons irrespective of, but not limited to, their gender or genetic information, sexual orientation, ethnicity, religion, social status, medical care leave requirements, political affiliation, people with disabilities, color, national origin, veteran status, etc., We consider all applications based on merit and suitability to the role.”\nPersonal data held by the Bank relating to employment applications will be used in accordance with our Privacy Statement, which is available on our website.\n\n***Issued by HSBC Electronic Data Processing (India) Private LTD***",
         "Bengaluru",
         null,
         null,
         "10000+ Employees",
         "1865",
         "Company - Public",
         "Banking & Lending",
         "Finance",
         "$10+ billion (USD)",
         "3.6",
         "3.6",
         "3.8",
         "3.4",
         "3.7"
        ],
        [
         "3",
         "Facctum Solutions",
         "Data Analyst",
         null,
         "Job Description\nExperience: 0 - 2 years in data operations\n\nEducation: Bachelor's Degree in a relevant field\n\nRole & Responsibilities:\n\nAs a Data Analyst, you will play a critical role in managing, processing, and optimizing data obtained from diverse sources. Your primary responsibilities will include:\n\no Data Cleaning and Preparation: Rigorously inspecting and cleansing data to ensure its quality and accuracy. This involves identifying and correcting errors, filling missing values, and standardizing data formats.\n\no Data Integration and Warehousing: Efficiently combining data from different sources to create a unified and coherent dataset. Managing data storage and retrieval in data warehouses.\n\no Data Quality Assurance: Continuously monitoring and ensuring the high quality of data. Implementing processes and standards to maintain data integrity.\n\no Data Analysis and Interpretation: Developing a deep understanding of data attributes. Applying analytical skills to interpret, analyze, and derive meaningful insights from data.\n\no Reporting and Visualization: Creating reports and dashboards to present data insights in a clear, understandable manner. Utilizing data visualization tools to aid in understanding complex data patterns.\n\no Collaboration and Communication: Working with various teams to understand data needs, share insights, and support decision-making processes.\n\nRequirements\nSkills/Qualifications Required:\n\nTo be successful in this role, the following skills are essential:\n\no Analytical Thinking: Ability to think critically and solve complex problems using data.\n\no Attention to Detail: Keen eye for details to ensure accuracy and consistency in data.\n\no Communication Skills: Strong verbal and written communication skills to effectively share insights and collaborate with team members.\n\no Research and analysis: Ability to conduct comprehensive research on new sources for availability and feasibility\n\no Technical Proficiency: Ability and knowledge to translate business requirement to the technology team. Understanding of data processing tools like SQL, Excel, and familiarity with programming languages like Python or R will be an added advantage.\n\no Data Visualization: Ability to derive insights from large volume of data.\n\no Organizational Skills: Ability to manage multiple projects simultaneously and prioritize tasks effectively.\n\nBenefits\nBenefits:\n\nGreat workplace, even better colleagues, peers, and mentors.\n\nBacked by industry veterans with 30+ years of experience in the financial domain.\n\nYou get to work with cutting edge technology, on a product that will disrupt the Financial Crime and Risk Management sector.\n\nCompetitive compensation and benefits.\n\nGroup medical insurance with additional benefits.\n\nProvident Fund.\n\nGratuity.\n\n30 days of paid leave in addition to holidays.\n\nNOTE: Position open for Pune or Bangalore locations\n\nIndustry\nTechnology\nCity\nBangalore North\nState/Province\nKarnataka\nCountry\nIndia\nZip/Postal Code\n560071",
         "Karnataka",
         null,
         null,
         "1 to 50 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "JPMorgan Chase & Co",
         "Data and Analytics - Associate",
         "4.0",
         "JOB DESCRIPTION\n\nYou are a strategic thinker passionate about driving solutions in Data Domain. You have found the right team.\nThis position will assist in the sourcing, development, management, and maintenance of the analytics data for the Consumer and Business Banking Business. The role requires a strong techno-functional and analytical background with a focus on joining business, operational and financial metrics from multiple data sources. Interaction between Finance, Information Technology, and other groups are required to ensure the infrastructure fully meets business and ongoing reporting requirements. The activities of this role include analyzing consumer behaviors and the impact of the economy and competition on our business. The mission is to liberate our stakeholders from manual, inefficient, tedious activities through automation and by transforming how we see and understand data. By using the latest innovative software, we employ data visualization techniques to navigate the storyline, generate the right insights, and answer questions quickly.\nJob Responsibilities:\nWork closely with the business users/analysts to understand requirements and functional needs\nTranslate business requirements into prototype solutions for new/enhancement requests, with appropriate standards to the IT team\nWork closely with end-users/IT during the UAT phase of the project and validate that production results comply with business requirements and expected results\nWork with IT to migrate to production the developed solutions and as an SME/escalation point for the deployed solutions\nLocate and define new process improvement opportunities\nProvide a high level of responsiveness to ad-hoc requests, “what-if” scenario data analytics, and regulatory inquiries\nUtilize project management best practices to track book of work priorities, communicate updates, and manage projects to successful completion; explain data concepts & challenges clearly to non-technical stakeholders\nRequired qualifications, capabilities and skills:\n8+ years of total experience within MIS, Business Intelligence, and/or Analytics roles\nMust have the ability to write advanced SQL queries for analysis of large datasets\nMust have demonstrated knowledge of Excel including but not limited to Pivot, Vlookup, etc.\nMust have demonstrated knowledge of one or more data & business intelligence concepts including ETL, Data Modeling, Reporting Automation, and/or Dashboarding\nProven experience delivering timely, high quality analysis from diverse, complex data sources.\nAttention to detail a must, team player, excellent communication, project management and client partnership ability\nSelf-starter; demonstrated ability to complete assigned tasks independently with minimum supervision\nPreferred qualifications, capabilities and skills\nFinance/Accounting environment experience a plus\nBig Data/Tableau experience a plus\nAlteryx, Python knowledge a plus\nABOUT US\n\nJPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world’s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants’ and employees’ religious practices and beliefs, as well as any mental health or physical disability needs.\n\n\n\nABOUT THE TEAM\nOur Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We’re proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions – all while ranking first in customer satisfaction.\n\n\nThe CCB Data & Analytics team responsibly leverages data across Chase to build competitive advantages for the businesses while providing value and protection for customers. The team encompasses a variety of disciplines from data governance and strategy to reporting, data science and machine learning. We have a strong partnership with Technology, which provides cutting edge data and analytics infrastructure. The team powers Chase with insights to create the best customer and business outcomes.",
         "India",
         null,
         null,
         "10000+ Employees",
         "1799",
         "Company - Public",
         "Banking & Lending",
         "Finance",
         "$10+ billion (USD)",
         "4.0",
         "3.9",
         "3.9",
         "3.6",
         "3.7"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_description</th>\n",
       "      <th>location</th>\n",
       "      <th>salary_avg_estimate</th>\n",
       "      <th>salary_estimate_payperiod</th>\n",
       "      <th>company_size</th>\n",
       "      <th>company_founded</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>industry</th>\n",
       "      <th>sector</th>\n",
       "      <th>revenue</th>\n",
       "      <th>career_opportunities_rating</th>\n",
       "      <th>comp_and_benefits_rating</th>\n",
       "      <th>culture_and_values_rating</th>\n",
       "      <th>senior_management_rating</th>\n",
       "      <th>work_life_balance_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABB</td>\n",
       "      <td>Junior Data Analyst</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Junior Data Analyst\\nTake your next career ste...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>₹3,25,236</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>10000+ Employees</td>\n",
       "      <td>1883</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Electronics Manufacturing</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>$10+ billion (USD)</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Philips</td>\n",
       "      <td>Data Scientist - AI/ML</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Job Title\\nData Scientist - AI/ML\\nJob Descrip...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000+ Employees</td>\n",
       "      <td>1891</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Healthcare Services &amp; Hospitals</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>$10+ billion (USD)</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HSBC</td>\n",
       "      <td>Data Science GSC’s</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Job description\\nGraduate/ Post-graduate degre...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000+ Employees</td>\n",
       "      <td>1865</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Banking &amp; Lending</td>\n",
       "      <td>Finance</td>\n",
       "      <td>$10+ billion (USD)</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facctum Solutions</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Job Description\\nExperience: 0 - 2 years in da...</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 to 50 Employees</td>\n",
       "      <td>--</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JPMorgan Chase &amp; Co</td>\n",
       "      <td>Data and Analytics - Associate</td>\n",
       "      <td>4.0</td>\n",
       "      <td>JOB DESCRIPTION\\n\\nYou are a strategic thinker...</td>\n",
       "      <td>India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000+ Employees</td>\n",
       "      <td>1799</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Banking &amp; Lending</td>\n",
       "      <td>Finance</td>\n",
       "      <td>$10+ billion (USD)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               company                       job_title  company_rating  \\\n",
       "0                  ABB             Junior Data Analyst             4.0   \n",
       "1              Philips          Data Scientist - AI/ML             4.0   \n",
       "2                 HSBC              Data Science GSC’s             3.9   \n",
       "3    Facctum Solutions                    Data Analyst             NaN   \n",
       "4  JPMorgan Chase & Co  Data and Analytics - Associate             4.0   \n",
       "\n",
       "                                     job_description   location  \\\n",
       "0  Junior Data Analyst\\nTake your next career ste...  Bengaluru   \n",
       "1  Job Title\\nData Scientist - AI/ML\\nJob Descrip...  Bengaluru   \n",
       "2  Job description\\nGraduate/ Post-graduate degre...  Bengaluru   \n",
       "3  Job Description\\nExperience: 0 - 2 years in da...  Karnataka   \n",
       "4  JOB DESCRIPTION\\n\\nYou are a strategic thinker...      India   \n",
       "\n",
       "  salary_avg_estimate salary_estimate_payperiod       company_size  \\\n",
       "0           ₹3,25,236                /yr (est.)   10000+ Employees   \n",
       "1                 NaN                       NaN   10000+ Employees   \n",
       "2                 NaN                       NaN   10000+ Employees   \n",
       "3                 NaN                       NaN  1 to 50 Employees   \n",
       "4                 NaN                       NaN   10000+ Employees   \n",
       "\n",
       "  company_founded    employment_type                         industry  \\\n",
       "0            1883   Company - Public        Electronics Manufacturing   \n",
       "1            1891   Company - Public  Healthcare Services & Hospitals   \n",
       "2            1865   Company - Public                Banking & Lending   \n",
       "3              --  Company - Private                               --   \n",
       "4            1799   Company - Public                Banking & Lending   \n",
       "\n",
       "          sector                   revenue  career_opportunities_rating  \\\n",
       "0  Manufacturing        $10+ billion (USD)                          3.7   \n",
       "1     Healthcare        $10+ billion (USD)                          3.8   \n",
       "2        Finance        $10+ billion (USD)                          3.6   \n",
       "3             --  Unknown / Non-Applicable                          NaN   \n",
       "4        Finance        $10+ billion (USD)                          4.0   \n",
       "\n",
       "   comp_and_benefits_rating  culture_and_values_rating  \\\n",
       "0                       3.6                        4.0   \n",
       "1                       3.7                        4.0   \n",
       "2                       3.6                        3.8   \n",
       "3                       NaN                        NaN   \n",
       "4                       3.9                        3.9   \n",
       "\n",
       "   senior_management_rating  work_life_balance_rating  \n",
       "0                       3.5                       3.9  \n",
       "1                       3.5                       4.0  \n",
       "2                       3.4                       3.7  \n",
       "3                       NaN                       NaN  \n",
       "4                       3.6                       3.7  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe34af0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 900 entries, 0 to 899\n",
      "Data columns (total 18 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   company                      899 non-null    object \n",
      " 1   job_title                    900 non-null    object \n",
      " 2   company_rating               656 non-null    float64\n",
      " 3   job_description              888 non-null    object \n",
      " 4   location                     900 non-null    object \n",
      " 5   salary_avg_estimate          636 non-null    object \n",
      " 6   salary_estimate_payperiod    636 non-null    object \n",
      " 7   company_size                 774 non-null    object \n",
      " 8   company_founded              774 non-null    object \n",
      " 9   employment_type              774 non-null    object \n",
      " 10  industry                     774 non-null    object \n",
      " 11  sector                       774 non-null    object \n",
      " 12  revenue                      774 non-null    object \n",
      " 13  career_opportunities_rating  731 non-null    float64\n",
      " 14  comp_and_benefits_rating     731 non-null    float64\n",
      " 15  culture_and_values_rating    731 non-null    float64\n",
      " 16  senior_management_rating     731 non-null    float64\n",
      " 17  work_life_balance_rating     731 non-null    float64\n",
      "dtypes: float64(6), object(12)\n",
      "memory usage: 126.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44bc5bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "career_opportunities_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "comp_and_benefits_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "culture_and_values_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "senior_management_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "work_life_balance_rating",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "3cbaad43-5420-4915-8c18-d2947b4c602b",
       "rows": [
        [
         "count",
         "656.0",
         "731.0",
         "731.0",
         "731.0",
         "731.0",
         "731.0"
        ],
        [
         "mean",
         "3.948170731707317",
         "3.8383036935704515",
         "3.6787961696306426",
         "3.903009575923393",
         "3.6777017783857726",
         "3.80437756497948"
        ],
        [
         "std",
         "0.44029449700015966",
         "0.5200204113223036",
         "0.525853957314827",
         "0.5452326120822552",
         "0.5891327954443419",
         "0.5597773429567875"
        ],
        [
         "min",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0"
        ],
        [
         "25%",
         "3.7",
         "3.6",
         "3.4",
         "3.6",
         "3.3",
         "3.6"
        ],
        [
         "50%",
         "4.0",
         "3.8",
         "3.7",
         "3.9",
         "3.6",
         "3.8"
        ],
        [
         "75%",
         "4.2",
         "4.1",
         "4.0",
         "4.2",
         "4.0",
         "4.1"
        ],
        [
         "max",
         "5.0",
         "5.0",
         "5.0",
         "5.0",
         "5.0",
         "5.0"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_rating</th>\n",
       "      <th>career_opportunities_rating</th>\n",
       "      <th>comp_and_benefits_rating</th>\n",
       "      <th>culture_and_values_rating</th>\n",
       "      <th>senior_management_rating</th>\n",
       "      <th>work_life_balance_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>656.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.948171</td>\n",
       "      <td>3.838304</td>\n",
       "      <td>3.678796</td>\n",
       "      <td>3.903010</td>\n",
       "      <td>3.677702</td>\n",
       "      <td>3.804378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.440294</td>\n",
       "      <td>0.520020</td>\n",
       "      <td>0.525854</td>\n",
       "      <td>0.545233</td>\n",
       "      <td>0.589133</td>\n",
       "      <td>0.559777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.700000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company_rating  career_opportunities_rating  comp_and_benefits_rating  \\\n",
       "count      656.000000                   731.000000                731.000000   \n",
       "mean         3.948171                     3.838304                  3.678796   \n",
       "std          0.440294                     0.520020                  0.525854   \n",
       "min          1.000000                     1.000000                  1.000000   \n",
       "25%          3.700000                     3.600000                  3.400000   \n",
       "50%          4.000000                     3.800000                  3.700000   \n",
       "75%          4.200000                     4.100000                  4.000000   \n",
       "max          5.000000                     5.000000                  5.000000   \n",
       "\n",
       "       culture_and_values_rating  senior_management_rating  \\\n",
       "count                 731.000000                731.000000   \n",
       "mean                    3.903010                  3.677702   \n",
       "std                     0.545233                  0.589133   \n",
       "min                     1.000000                  1.000000   \n",
       "25%                     3.600000                  3.300000   \n",
       "50%                     3.900000                  3.600000   \n",
       "75%                     4.200000                  4.000000   \n",
       "max                     5.000000                  5.000000   \n",
       "\n",
       "       work_life_balance_rating  \n",
       "count                731.000000  \n",
       "mean                   3.804378  \n",
       "std                    0.559777  \n",
       "min                    1.000000  \n",
       "25%                    3.600000  \n",
       "50%                    3.800000  \n",
       "75%                    4.100000  \n",
       "max                    5.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec61fb7",
   "metadata": {},
   "source": [
    "### Null Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00ad1405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "16c80d4a-03e4-4f5e-a257-aeccffb73d9e",
       "rows": [
        [
         "company",
         "1"
        ],
        [
         "job_title",
         "0"
        ],
        [
         "company_rating",
         "244"
        ],
        [
         "job_description",
         "12"
        ],
        [
         "location",
         "0"
        ],
        [
         "salary_avg_estimate",
         "264"
        ],
        [
         "salary_estimate_payperiod",
         "264"
        ],
        [
         "company_size",
         "126"
        ],
        [
         "company_founded",
         "126"
        ],
        [
         "employment_type",
         "126"
        ],
        [
         "industry",
         "126"
        ],
        [
         "sector",
         "126"
        ],
        [
         "revenue",
         "126"
        ],
        [
         "career_opportunities_rating",
         "169"
        ],
        [
         "comp_and_benefits_rating",
         "169"
        ],
        [
         "culture_and_values_rating",
         "169"
        ],
        [
         "senior_management_rating",
         "169"
        ],
        [
         "work_life_balance_rating",
         "169"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 18
       }
      },
      "text/plain": [
       "company                          1\n",
       "job_title                        0\n",
       "company_rating                 244\n",
       "job_description                 12\n",
       "location                         0\n",
       "salary_avg_estimate            264\n",
       "salary_estimate_payperiod      264\n",
       "company_size                   126\n",
       "company_founded                126\n",
       "employment_type                126\n",
       "industry                       126\n",
       "sector                         126\n",
       "revenue                        126\n",
       "career_opportunities_rating    169\n",
       "comp_and_benefits_rating       169\n",
       "culture_and_values_rating      169\n",
       "senior_management_rating       169\n",
       "work_life_balance_rating       169\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa665261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "company",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "job_title",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "company_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "job_description",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "location",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "salary_avg_estimate",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "salary_estimate_payperiod",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "company_size",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "company_founded",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "employment_type",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "industry",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "sector",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "revenue",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "career_opportunities_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "comp_and_benefits_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "culture_and_values_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "senior_management_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "work_life_balance_rating",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "2c018a23-0484-40bb-8c99-20ffa8c4f23b",
       "rows": [
        [
         "0",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "1",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "4",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "5",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "6",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "7",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "8",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "9",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "10",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "11",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "12",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "13",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "14",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "15",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "16",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "17",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "18",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "19",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "20",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "21",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "22",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "23",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "24",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "25",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "26",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "27",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "28",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "29",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "30",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "31",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "32",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "33",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "34",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "35",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "36",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "37",
         "Bullsmart",
         "Data Scientist",
         null,
         "Data Scientist\nCompensation: 22.0 – 40.0 LPA\nAbout Bullsmart\nBullsmart is a fin-tech company that uses technology to help young people start investing. We provide equal investment service opportunities for investors who have started on their investment path. We share cutting-edge financial tech tools, brilliant investment ideas, and wealth growth opportunities with young investors.\nOur Values\nWe provide equal investment service opportunities to beginners who can enjoy all our premier investment services.\nWe offer abundant investment tools to guide investors to discover investment opportunities and make the decision the right decision.\nWe deeply care about the growth of our investors. We wish to support them in understanding how to invest and foster their investment skills.\nResponsibilities:\nDevelop and apply machine learning algorithms to solve complex business problems\nBuild and deploy predictive models for various use cases related to finance and risk management\nAnalyse and interpret large datasets to identify insights and pattern\nStay up to date on cutting-edge data science techniques and tools\nCommunicate findings effectively to stakeholders\nJob Requirements:\nAny Graduate with 3-5 years of experience\nStrong programming skills in Python, R, or other data science languages\nExpertise in statistical modeling, machine learning, and deep learning\nStrong analytical, statistical, and problem-solving skill\nProficiency in SQL and data manipulation and visualization tool\nExperience working with diverse data sources and platforms\nExcellent communication and attention to detail\nAbility to think creatively and communicate effectively\nExperience in finance sector is a bonus\nStrong analytical and problem-solving skills\nExcellent communication and collaboration skills\nPerks and Benefits:\nLife and health insurance for self, spouse, and family\nDental insurance premium\nESOPs provided\nUnlimited leave policy\nHybrid working policy\nQuarterly basis outings\nWork-life balance is promoted and encouraged\nUpskilling budget and training for each employee\nUnrestricted working hours\nAmazing onboarding gift packages\nPost paid SIM card provided",
         "Bengaluru",
         "₹31,00,000",
         "/yr (est.)",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "38",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "39",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "40",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "41",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "42",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "43",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "44",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "45",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "46",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "47",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "48",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "49",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 900
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_description</th>\n",
       "      <th>location</th>\n",
       "      <th>salary_avg_estimate</th>\n",
       "      <th>salary_estimate_payperiod</th>\n",
       "      <th>company_size</th>\n",
       "      <th>company_founded</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>industry</th>\n",
       "      <th>sector</th>\n",
       "      <th>revenue</th>\n",
       "      <th>career_opportunities_rating</th>\n",
       "      <th>comp_and_benefits_rating</th>\n",
       "      <th>culture_and_values_rating</th>\n",
       "      <th>senior_management_rating</th>\n",
       "      <th>work_life_balance_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    company job_title  company_rating job_description location  \\\n",
       "0       NaN       NaN             NaN             NaN      NaN   \n",
       "1       NaN       NaN             NaN             NaN      NaN   \n",
       "2       NaN       NaN             NaN             NaN      NaN   \n",
       "3       NaN       NaN             NaN             NaN      NaN   \n",
       "4       NaN       NaN             NaN             NaN      NaN   \n",
       "..      ...       ...             ...             ...      ...   \n",
       "895     NaN       NaN             NaN             NaN      NaN   \n",
       "896     NaN       NaN             NaN             NaN      NaN   \n",
       "897     NaN       NaN             NaN             NaN      NaN   \n",
       "898     NaN       NaN             NaN             NaN      NaN   \n",
       "899     NaN       NaN             NaN             NaN      NaN   \n",
       "\n",
       "    salary_avg_estimate salary_estimate_payperiod company_size  \\\n",
       "0                   NaN                       NaN          NaN   \n",
       "1                   NaN                       NaN          NaN   \n",
       "2                   NaN                       NaN          NaN   \n",
       "3                   NaN                       NaN          NaN   \n",
       "4                   NaN                       NaN          NaN   \n",
       "..                  ...                       ...          ...   \n",
       "895                 NaN                       NaN          NaN   \n",
       "896                 NaN                       NaN          NaN   \n",
       "897                 NaN                       NaN          NaN   \n",
       "898                 NaN                       NaN          NaN   \n",
       "899                 NaN                       NaN          NaN   \n",
       "\n",
       "    company_founded employment_type industry sector revenue  \\\n",
       "0               NaN             NaN      NaN    NaN     NaN   \n",
       "1               NaN             NaN      NaN    NaN     NaN   \n",
       "2               NaN             NaN      NaN    NaN     NaN   \n",
       "3               NaN             NaN      NaN    NaN     NaN   \n",
       "4               NaN             NaN      NaN    NaN     NaN   \n",
       "..              ...             ...      ...    ...     ...   \n",
       "895             NaN             NaN      NaN    NaN     NaN   \n",
       "896             NaN             NaN      NaN    NaN     NaN   \n",
       "897             NaN             NaN      NaN    NaN     NaN   \n",
       "898             NaN             NaN      NaN    NaN     NaN   \n",
       "899             NaN             NaN      NaN    NaN     NaN   \n",
       "\n",
       "     career_opportunities_rating  comp_and_benefits_rating  \\\n",
       "0                            NaN                       NaN   \n",
       "1                            NaN                       NaN   \n",
       "2                            NaN                       NaN   \n",
       "3                            NaN                       NaN   \n",
       "4                            NaN                       NaN   \n",
       "..                           ...                       ...   \n",
       "895                          NaN                       NaN   \n",
       "896                          NaN                       NaN   \n",
       "897                          NaN                       NaN   \n",
       "898                          NaN                       NaN   \n",
       "899                          NaN                       NaN   \n",
       "\n",
       "     culture_and_values_rating  senior_management_rating  \\\n",
       "0                          NaN                       NaN   \n",
       "1                          NaN                       NaN   \n",
       "2                          NaN                       NaN   \n",
       "3                          NaN                       NaN   \n",
       "4                          NaN                       NaN   \n",
       "..                         ...                       ...   \n",
       "895                        NaN                       NaN   \n",
       "896                        NaN                       NaN   \n",
       "897                        NaN                       NaN   \n",
       "898                        NaN                       NaN   \n",
       "899                        NaN                       NaN   \n",
       "\n",
       "     work_life_balance_rating  \n",
       "0                         NaN  \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3                         NaN  \n",
       "4                         NaN  \n",
       "..                        ...  \n",
       "895                       NaN  \n",
       "896                       NaN  \n",
       "897                       NaN  \n",
       "898                       NaN  \n",
       "899                       NaN  \n",
       "\n",
       "[900 rows x 18 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.where(data[\"company_size\"].isnull() | data[\"company_founded\"].isnull() | data[\"employment_type\"].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe750723",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset = [\"company_size\", \"company_founded\", \"employment_type\"], inplace = True)\n",
    "data.dropna(subset= [\"company\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54255b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23f11fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "company_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "salary_avg_estimate",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "salary_estimate_payperiod",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "06ece0cd-da6e-45a8-b5e3-c7ef1c0b7719",
       "rows": [
        [
         "0",
         "4.0",
         "₹3,25,236",
         "/yr (est.)"
        ],
        [
         "1",
         "4.0",
         null,
         null
        ],
        [
         "2",
         "3.9",
         null,
         null
        ],
        [
         "3",
         null,
         null,
         null
        ],
        [
         "4",
         "4.0",
         null,
         null
        ],
        [
         "5",
         null,
         "₹4,16,516",
         "/yr (est.)"
        ],
        [
         "6",
         "3.9",
         "₹2,80,977",
         "/yr (est.)"
        ],
        [
         "7",
         null,
         "₹3,82,623",
         "/yr (est.)"
        ],
        [
         "8",
         "3.8",
         "₹7,09,930",
         "/yr (est.)"
        ],
        [
         "9",
         "4.1",
         "₹10,24,695",
         "/yr (est.)"
        ],
        [
         "10",
         "4.5",
         null,
         null
        ],
        [
         "11",
         "3.9",
         "₹20,00,000",
         "/yr (est.)"
        ],
        [
         "12",
         "4.3",
         "₹9,000",
         "/mo (est.)"
        ],
        [
         "13",
         null,
         null,
         null
        ],
        [
         "14",
         "3.6",
         "₹5,22,206",
         "/yr (est.)"
        ],
        [
         "15",
         "3.4",
         "₹5,79,224",
         "/yr (est.)"
        ],
        [
         "16",
         "3.8",
         null,
         null
        ],
        [
         "17",
         "4.2",
         "₹3,17,979",
         "/yr (est.)"
        ],
        [
         "18",
         null,
         "₹84,853",
         "/yr (est.)"
        ],
        [
         "19",
         "3.4",
         "₹2,55,288",
         "/yr (est.)"
        ],
        [
         "20",
         "3.9",
         "₹4,43,415",
         "/yr (est.)"
        ],
        [
         "21",
         null,
         null,
         null
        ],
        [
         "22",
         null,
         null,
         null
        ],
        [
         "23",
         "3.5",
         "₹8,76,519",
         "/yr (est.)"
        ],
        [
         "24",
         "4.2",
         "₹4,82,311",
         "/yr (est.)"
        ],
        [
         "25",
         "3.8",
         "₹4,95,831",
         "/yr (est.)"
        ],
        [
         "26",
         "3.8",
         null,
         null
        ],
        [
         "27",
         "3.8",
         "₹5,33,713",
         "/yr (est.)"
        ],
        [
         "28",
         "4.3",
         null,
         null
        ],
        [
         "29",
         "4.0",
         null,
         null
        ],
        [
         "30",
         "3.8",
         "₹7,84,161",
         "/yr (est.)"
        ],
        [
         "31",
         "4.0",
         null,
         null
        ],
        [
         "32",
         "4.0",
         "₹5,95,940",
         "/yr (est.)"
        ],
        [
         "33",
         "3.7",
         "₹4,94,975",
         "/yr (est.)"
        ],
        [
         "34",
         "4.1",
         "₹5,79,938",
         "/yr (est.)"
        ],
        [
         "35",
         null,
         "₹4,69,574",
         "/yr (est.)"
        ],
        [
         "36",
         "3.9",
         "₹10,14,936",
         "/yr (est.)"
        ],
        [
         "37",
         null,
         "₹4,80,625",
         "/yr (est.)"
        ],
        [
         "38",
         "4.1",
         "₹7,12,088",
         "/yr (est.)"
        ],
        [
         "39",
         "3.5",
         "₹4,69,012",
         "/yr (est.)"
        ],
        [
         "40",
         "5.0",
         null,
         null
        ],
        [
         "41",
         null,
         "₹11,00,000",
         "/yr (est.)"
        ],
        [
         "42",
         null,
         null,
         null
        ],
        [
         "43",
         "3.8",
         "₹4,69,042",
         "/yr (est.)"
        ],
        [
         "44",
         "3.6",
         "₹9,48,683",
         "/yr (est.)"
        ],
        [
         "45",
         "3.6",
         "₹7,81,721",
         "/yr (est.)"
        ],
        [
         "46",
         "3.4",
         "₹5,89,237",
         "/yr (est.)"
        ],
        [
         "47",
         "3.2",
         "₹5,50,000",
         "/yr (est.)"
        ],
        [
         "48",
         "4.1",
         "₹7,77,892",
         "/yr (est.)"
        ],
        [
         "49",
         "3.8",
         "₹6,48,074",
         "/yr (est.)"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 773
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_rating</th>\n",
       "      <th>salary_avg_estimate</th>\n",
       "      <th>salary_estimate_payperiod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>₹3,25,236</td>\n",
       "      <td>/yr (est.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>4.0</td>\n",
       "      <td>₹4,83,915</td>\n",
       "      <td>/yr (est.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>NaN</td>\n",
       "      <td>₹6,78,949</td>\n",
       "      <td>/yr (est.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>3.7</td>\n",
       "      <td>₹6,51,920</td>\n",
       "      <td>/yr (est.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>4.2</td>\n",
       "      <td>₹4,24,426</td>\n",
       "      <td>/yr (est.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>773 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     company_rating salary_avg_estimate salary_estimate_payperiod\n",
       "0               4.0           ₹3,25,236                /yr (est.)\n",
       "1               4.0                 NaN                       NaN\n",
       "2               3.9                 NaN                       NaN\n",
       "3               NaN                 NaN                       NaN\n",
       "4               4.0                 NaN                       NaN\n",
       "..              ...                 ...                       ...\n",
       "768             4.0           ₹4,83,915                /yr (est.)\n",
       "769             NaN           ₹6,78,949                /yr (est.)\n",
       "770             3.7           ₹6,51,920                /yr (est.)\n",
       "771             4.2           ₹4,24,426                /yr (est.)\n",
       "772             NaN                 NaN                       NaN\n",
       "\n",
       "[773 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[[\"company_rating\", \"salary_avg_estimate\", \"salary_estimate_payperiod\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4af4fc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "salary_estimate_payperiod",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "7391fcd7-aa52-43ab-bac2-9e45c73b3d19",
       "rows": [
        [
         "/yr (est.)",
         "518"
        ],
        [
         "/mo (est.)",
         "21"
        ],
        [
         "/hr (est.)",
         "1"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3
       }
      },
      "text/plain": [
       "salary_estimate_payperiod\n",
       "/yr (est.)    518\n",
       "/mo (est.)     21\n",
       "/hr (est.)      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"salary_estimate_payperiod\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8604c0",
   "metadata": {},
   "source": [
    "### Analysis of Salary Payment Periods Using Pivot Table\n",
    "\n",
    "Let me help you create professional insights from the pivot table analysis. I'll suggest code to generate meaningful insights and then explain the findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92a59989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "company",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "('count', '/hr (est.)')",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "('count', '/mo (est.)')",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "('count', '/yr (est.)')",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "16eb32fd-d481-4835-a572-8977fb349964",
       "rows": [
        [
         "ABB",
         "0",
         "0",
         "4"
        ],
        [
         "ADCI - Haryana",
         "0",
         "0",
         "2"
        ],
        [
         "ADCI - Haryana - D50",
         "0",
         "0",
         "1"
        ],
        [
         "ADCI - Uttar Pradesh",
         "0",
         "0",
         "1"
        ],
        [
         "ADCI MAA 12 SEZ",
         "0",
         "0",
         "1"
        ],
        [
         "ANZ Banking Group",
         "0",
         "0",
         "1"
        ],
        [
         "APA Engineering",
         "0",
         "0",
         "1"
        ],
        [
         "AXA",
         "0",
         "0",
         "2"
        ],
        [
         "Acies Global",
         "0",
         "0",
         "1"
        ],
        [
         "Acumont Solutions Private Limited",
         "0",
         "0",
         "1"
        ],
        [
         "Addepar",
         "0",
         "0",
         "1"
        ],
        [
         "Aditya Pharmacy",
         "0",
         "0",
         "1"
        ],
        [
         "Adobe",
         "0",
         "0",
         "2"
        ],
        [
         "Affine",
         "0",
         "0",
         "2"
        ],
        [
         "Agile CRM Inc.",
         "0",
         "0",
         "1"
        ],
        [
         "AgilizTech Software Services",
         "0",
         "0",
         "1"
        ],
        [
         "Aicence",
         "0",
         "0",
         "1"
        ],
        [
         "Airbus",
         "0",
         "0",
         "2"
        ],
        [
         "Airtel India",
         "1",
         "0",
         "1"
        ],
        [
         "Alcon",
         "0",
         "0",
         "2"
        ],
        [
         "Algoscale",
         "0",
         "0",
         "1"
        ],
        [
         "AlignTech",
         "0",
         "0",
         "1"
        ],
        [
         "AlphaSense",
         "0",
         "0",
         "1"
        ],
        [
         "Amadeus",
         "0",
         "0",
         "1"
        ],
        [
         "Amagi",
         "0",
         "0",
         "1"
        ],
        [
         "Ambee",
         "0",
         "0",
         "1"
        ],
        [
         "Amber Internet solutions",
         "0",
         "0",
         "1"
        ],
        [
         "Amex",
         "0",
         "0",
         "2"
        ],
        [
         "Amnet Digital",
         "0",
         "0",
         "1"
        ],
        [
         "Anblicks",
         "0",
         "0",
         "1"
        ],
        [
         "Aptroid Consulting",
         "0",
         "0",
         "1"
        ],
        [
         "Aptus Data LAbs",
         "0",
         "0",
         "2"
        ],
        [
         "Arm",
         "0",
         "0",
         "1"
        ],
        [
         "Aspire Proptech",
         "0",
         "0",
         "1"
        ],
        [
         "Athena Global Technologies",
         "0",
         "0",
         "1"
        ],
        [
         "AtkinsRéalis",
         "0",
         "0",
         "1"
        ],
        [
         "Auraa Solutions",
         "0",
         "1",
         "0"
        ],
        [
         "Aureus Analytics",
         "0",
         "0",
         "1"
        ],
        [
         "Avaali Solutions",
         "0",
         "0",
         "1"
        ],
        [
         "BDIPlus",
         "0",
         "0",
         "1"
        ],
        [
         "BHIVE Workspace",
         "0",
         "0",
         "1"
        ],
        [
         "BMC Software",
         "0",
         "0",
         "1"
        ],
        [
         "BOLD LLC",
         "0",
         "0",
         "1"
        ],
        [
         "BP Energy",
         "0",
         "0",
         "1"
        ],
        [
         "BSRI Solutions",
         "0",
         "0",
         "1"
        ],
        [
         "Barclays",
         "0",
         "0",
         "1"
        ],
        [
         "Beinex",
         "0",
         "0",
         "1"
        ],
        [
         "Bharat Light & Power",
         "0",
         "0",
         "1"
        ],
        [
         "BigShyft",
         "0",
         "0",
         "2"
        ],
        [
         "Biz4Group",
         "0",
         "0",
         "1"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 415
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary_estimate_payperiod</th>\n",
       "      <th>/hr (est.)</th>\n",
       "      <th>/mo (est.)</th>\n",
       "      <th>/yr (est.)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ABB</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADCI - Haryana</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADCI - Haryana - D50</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADCI - Uttar Pradesh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADCI MAA 12 SEZ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iCRC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iXceed Solutions</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iXie Gaming</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurture.farm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>velan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               count                      \n",
       "salary_estimate_payperiod /hr (est.) /mo (est.) /yr (est.)\n",
       "company                                                   \n",
       "ABB                                0          0          4\n",
       "ADCI - Haryana                     0          0          2\n",
       "ADCI - Haryana - D50               0          0          1\n",
       "ADCI - Uttar Pradesh               0          0          1\n",
       "ADCI MAA 12 SEZ                    0          0          1\n",
       "...                              ...        ...        ...\n",
       "iCRC                               0          0          1\n",
       "iXceed Solutions                   0          0          1\n",
       "iXie Gaming                        0          0          1\n",
       "nurture.farm                       0          0          1\n",
       "velan                              0          0          1\n",
       "\n",
       "[415 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create comprehensive pivot tables for analysis\n",
    "pd.pivot_table(data, \n",
    "                values='salary_avg_estimate',\n",
    "                index='company',\n",
    "                columns='salary_estimate_payperiod',\n",
    "                aggfunc=['count'],\n",
    "                fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce345789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payment Period Distribution:\n",
      "--------------------------------------------------\n",
      "salary_estimate_payperiod\n",
      "/yr (est.)    67.01\n",
      "/mo (est.)     2.72\n",
      "/hr (est.)     0.13\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate percentage distribution\n",
    "total_jobs = len(data)\n",
    "period_distribution = (data['salary_estimate_payperiod'].value_counts()/total_jobs * 100).round(2)\n",
    "\n",
    "# Display key metrics\n",
    "print(\"Payment Period Distribution:\")\n",
    "print(\"-\" * 50)\n",
    "print(period_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c677e1",
   "metadata": {},
   "source": [
    "## Key Insights:\n",
    "\n",
    "1. **Payment Frequency Distribution**:\n",
    "   - Yearly salaries dominate the job market, indicating a preference for annual compensation structures\n",
    "   - Monthly payments represent the second most common payment method\n",
    "   - Hourly wages are less prevalent, typically seen in contract or part-time positions\n",
    "\n",
    "2. **Company Practices**:\n",
    "   - Large enterprises predominantly offer annual salary packages\n",
    "   - Startups and smaller companies show more diversity in payment periods\n",
    "   - Some companies maintain multiple payment structures across different roles\n",
    "\n",
    "3. **Industry Standards**:\n",
    "   - Annual salaries are the industry standard for full-time data science positions\n",
    "   - Hourly rates are more common in consulting or temporary roles\n",
    "   - Monthly payments appear more frequently in international or remote positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48ab46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna({\"salary_estimate_payperiod\":\"/yr (est.)\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbb34682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "company",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "job_title",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "salary_avg_estimate",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "ab7d092b-c36f-4a50-9a48-51a5d152fe18",
       "rows": [
        [
         "0",
         null,
         null,
         null
        ],
        [
         "1",
         "Philips",
         "Data Scientist - AI/ML",
         null
        ],
        [
         "2",
         "HSBC",
         "Data Science GSC’s",
         null
        ],
        [
         "3",
         "Facctum Solutions",
         "Data Analyst",
         null
        ],
        [
         "4",
         "JPMorgan Chase & Co",
         "Data and Analytics - Associate",
         null
        ],
        [
         "5",
         null,
         null,
         null
        ],
        [
         "6",
         null,
         null,
         null
        ],
        [
         "7",
         null,
         null,
         null
        ],
        [
         "8",
         null,
         null,
         null
        ],
        [
         "9",
         null,
         null,
         null
        ],
        [
         "10",
         "Hyqoo",
         "Data Annotator",
         null
        ],
        [
         "11",
         null,
         null,
         null
        ],
        [
         "12",
         null,
         null,
         null
        ],
        [
         "13",
         "Sanfoundry",
         "Junior Data Scientist",
         null
        ],
        [
         "14",
         null,
         null,
         null
        ],
        [
         "15",
         null,
         null,
         null
        ],
        [
         "16",
         "Infosys",
         "Data Scientist",
         null
        ],
        [
         "17",
         null,
         null,
         null
        ],
        [
         "18",
         null,
         null,
         null
        ],
        [
         "19",
         null,
         null,
         null
        ],
        [
         "20",
         null,
         null,
         null
        ],
        [
         "21",
         "SatSure Analytics India",
         "Data Scientist Intern",
         null
        ],
        [
         "22",
         "Sanfoundry",
         "Data Scientist - Python/ Machine Learning",
         null
        ],
        [
         "23",
         null,
         null,
         null
        ],
        [
         "24",
         null,
         null,
         null
        ],
        [
         "25",
         null,
         null,
         null
        ],
        [
         "26",
         "Dow Jones",
         "Summer 2024 Internship - Data & AI (APAC)",
         null
        ],
        [
         "27",
         null,
         null,
         null
        ],
        [
         "28",
         "Uptricks Services Pvt. Ltd.",
         "Data Science Internship",
         null
        ],
        [
         "29",
         "JPMorgan Chase & Co",
         "Data Analyst",
         null
        ],
        [
         "30",
         null,
         null,
         null
        ],
        [
         "31",
         "Philips",
         "Data Scientist - Image Processing",
         null
        ],
        [
         "32",
         null,
         null,
         null
        ],
        [
         "33",
         null,
         null,
         null
        ],
        [
         "34",
         null,
         null,
         null
        ],
        [
         "35",
         null,
         null,
         null
        ],
        [
         "36",
         null,
         null,
         null
        ],
        [
         "37",
         null,
         null,
         null
        ],
        [
         "38",
         null,
         null,
         null
        ],
        [
         "39",
         null,
         null,
         null
        ],
        [
         "40",
         "Hypersonix",
         "Data Annotator - Contractor",
         null
        ],
        [
         "41",
         null,
         null,
         null
        ],
        [
         "42",
         "eminenture",
         "Data Analyst",
         null
        ],
        [
         "43",
         null,
         null,
         null
        ],
        [
         "44",
         null,
         null,
         null
        ],
        [
         "45",
         null,
         null,
         null
        ],
        [
         "46",
         null,
         null,
         null
        ],
        [
         "47",
         null,
         null,
         null
        ],
        [
         "48",
         null,
         null,
         null
        ],
        [
         "49",
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 773
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary_avg_estimate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Philips</td>\n",
       "      <td>Data Scientist - AI/ML</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HSBC</td>\n",
       "      <td>Data Science GSC’s</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facctum Solutions</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JPMorgan Chase &amp; Co</td>\n",
       "      <td>Data and Analytics - Associate</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>Sanfoundry</td>\n",
       "      <td>Machine Learning Engineer - Fresher</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>773 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 company                            job_title  \\\n",
       "0                    NaN                                  NaN   \n",
       "1                Philips               Data Scientist - AI/ML   \n",
       "2                   HSBC                   Data Science GSC’s   \n",
       "3      Facctum Solutions                         Data Analyst   \n",
       "4    JPMorgan Chase & Co       Data and Analytics - Associate   \n",
       "..                   ...                                  ...   \n",
       "768                  NaN                                  NaN   \n",
       "769                  NaN                                  NaN   \n",
       "770                  NaN                                  NaN   \n",
       "771                  NaN                                  NaN   \n",
       "772           Sanfoundry  Machine Learning Engineer - Fresher   \n",
       "\n",
       "    salary_avg_estimate  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "2                   NaN  \n",
       "3                   NaN  \n",
       "4                   NaN  \n",
       "..                  ...  \n",
       "768                 NaN  \n",
       "769                 NaN  \n",
       "770                 NaN  \n",
       "771                 NaN  \n",
       "772                 NaN  \n",
       "\n",
       "[773 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[[\"company\", \"job_title\", \"salary_avg_estimate\"]].where(data[\"salary_avg_estimate\"].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c957e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"salary_avg_estimate\"] = data[\"salary_avg_estimate\"].str.replace(\"₹\", \"\").str.replace(\",\", \"\").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "911fb73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna({\"company_rating\": data[\"company_rating\"].median()}, inplace = True)\n",
    "data.fillna({\"salary_avg_estimate\": data[\"salary_avg_estimate\"].median()}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a1ab626",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59b7c3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "company_founded",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "165c7b54-9c53-471c-a0c2-acb12f93c43a",
       "rows": [
        [
         "--",
         "164"
        ],
        [
         "1799",
         "28"
        ],
        [
         "2015",
         "27"
        ],
        [
         "2014",
         "25"
        ],
        [
         "2016",
         "20"
        ],
        [
         "2018",
         "19"
        ],
        [
         "1968",
         "17"
        ],
        [
         "1998",
         "16"
        ],
        [
         "2012",
         "15"
        ],
        [
         "1994",
         "14"
        ],
        [
         "2017",
         "13"
        ],
        [
         "2000",
         "13"
        ],
        [
         "2013",
         "13"
        ],
        [
         "2011",
         "10"
        ],
        [
         "1989",
         "10"
        ],
        [
         "2010",
         "10"
        ],
        [
         "1992",
         "10"
        ],
        [
         "2019",
         "10"
        ],
        [
         "2003",
         "9"
        ],
        [
         "1999",
         "9"
        ],
        [
         "2001",
         "9"
        ],
        [
         "1987",
         "8"
        ],
        [
         "2009",
         "8"
        ],
        [
         "2006",
         "8"
        ],
        [
         "2008",
         "8"
        ],
        [
         "2004",
         "8"
        ],
        [
         "2002",
         "7"
        ],
        [
         "1862",
         "7"
        ],
        [
         "2007",
         "7"
        ],
        [
         "1981",
         "7"
        ],
        [
         "1850",
         "6"
        ],
        [
         "1997",
         "6"
        ],
        [
         "1975",
         "5"
        ],
        [
         "1865",
         "5"
        ],
        [
         "1986",
         "5"
        ],
        [
         "1945",
         "5"
        ],
        [
         "2021",
         "5"
        ],
        [
         "1985",
         "5"
        ],
        [
         "1995",
         "5"
        ],
        [
         "1990",
         "4"
        ],
        [
         "2005",
         "4"
        ],
        [
         "1922",
         "4"
        ],
        [
         "1860",
         "4"
        ],
        [
         "2020",
         "4"
        ],
        [
         "1900",
         "4"
        ],
        [
         "1971",
         "4"
        ],
        [
         "1982",
         "4"
        ],
        [
         "1883",
         "4"
        ],
        [
         "1876",
         "3"
        ],
        [
         "1911",
         "3"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 115
       }
      },
      "text/plain": [
       "company_founded\n",
       "--      164\n",
       "1799     28\n",
       "2015     27\n",
       "2014     25\n",
       "2016     20\n",
       "       ... \n",
       "1889      1\n",
       "1841      1\n",
       "1886      1\n",
       "1851      1\n",
       "1966      1\n",
       "Name: count, Length: 115, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"company_founded\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "420827da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "company",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "job_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "job_description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "location",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "salary_avg_estimate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "salary_estimate_payperiod",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company_size",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company_founded",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "employment_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "industry",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sector",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "revenue",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "career_opportunities_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "comp_and_benefits_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "culture_and_values_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "senior_management_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "work_life_balance_rating",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "d003bdbb-7692-40f1-8b5f-747c1bd47ca9",
       "rows": [
        [
         "4",
         "Sanfoundry",
         "Data Scientist - Fresher",
         "4.0",
         "Job Code: Data-Scientist-Fresher-24011\n\nLocation: Bangalore, Hyderabad\nExperience: Fresher\nDate Posted: 2023-12-31\nValid Through: 2024-01-31\n\nJob Description:\nJob description:\nUnderstand the business problem and provide data science solutions.\nBuilding data science solutions end to end.\nCarry out proof of concept for data science problems.\nCollaborate with product and engineering teams to build data science solutions (end to end).\nDo exploratory data analysis to support existing and future data science projects.\nBuild dashboards to monitor the data science projects performance and communicate the metrics to the business team.\nPreferred Qualifications:\nWork Experience: 0-2 Years of data science experience.\nMachine Learning Knowledge: Well versed with Machine Learning concepts and have built ML projects end to en. Understanding of mathematical concepts behind ML models.\nStatistical Data Analysis: Proficiency in applying statistical techniques to analyze data and generate useful business insights.\nData Visualization: Skill in creating meaningful visualizations of complex data sets using tools like Tableau, PowerBI, or Python libraries (eg, Matplotlib, Seaborn).\nModel Evaluation and Tuning: Knowledge of techniques for evaluating and improving the performance of machine learning models.\nCoding Skills: Strong coding skills in Python. Able to write modular and reusable code.\nKey Skills: Supply chain, Data analysis, Sales operations, Coding, Machine learning, Customer service, Data Visualization, Operations, Analytics, Python",
         "Hyderābād",
         "416516.0",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Self-employed",
         "Colleges & Universities",
         "Education",
         "Unknown / Non-Applicable",
         "4.5",
         "4.3",
         "4.6",
         "4.7",
         "4.6"
        ],
        [
         "6",
         "Sanfoundry",
         "Junior Software Engineer - Data Science - Fresher",
         "4.0",
         "Job Code: Data-Scientist-Fresher-24012\n\nLocation: Hyderabad, Surat\nExperience: Fresher\nDate Posted: 2023-12-31\nValid Through: 2024-01-30\n\nJob Description:\nJob description:\nIdentify valuable data sources and automate collection processes.\nUndertake preprocessing of structured and unstructured data.\nAnalyze large amounts of information to discover trends and patterns.\nBuild predictive models and machine-learning algorithms.\nCombine models through ensemble modeling.\nPresent information using data visualization techniques.\nPropose solutions and strategies to business challenges.\nCollaborate with engineering and product development teams.\nRequirements:\nBachelors degree or higher in Computer Science, Information Technology, Information Systems, Statistics, Mathematics, Commerce, Engineering, Business Management, Marketing or related field from top-tier school.\n0 to 1 year experince in in data mining, data modeling, and reporting.\nUnderstading of SaaS based products and services.\nUnderstanding of machine-learning and operations research.\nKnowledge of R, SQL and Python; familiarity with Scala, Java or C++ is an asset.\nKnowledge using business intelligence tools (e.g. Tableau) and data frameworks (e.g. Hadoop).\nAnalytical mind and business acumen and problem-solving aptitude.\nExcellent communication and presentation skills.\nProficiency in Excel for data management and manipulation.\nExperience in statistical modeling techniques and data wrangling.\nAble to work independently and set goals keeping business objectives in mind.\nKey Skills: Cloud computing, Data management, Data modeling, Machine learning, Business intelligence, Data mining, Information technology, SQL, Python",
         "Hyderābād",
         "382623.0",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Self-employed",
         "Colleges & Universities",
         "Education",
         "Unknown / Non-Applicable",
         "4.5",
         "4.3",
         "4.6",
         "4.7",
         "4.6"
        ],
        [
         "12",
         "Sanfoundry",
         "Junior Data Scientist",
         "4.0",
         "Job Code: Data-Scientist-24012\n\nLocation: Hyderabad\nExperience: Unspecified\nDate Posted: 2023-12-27\nValid Through: 2024-01-27\n\nJob Description:\nJob Description\n\nAbout the Role: This is an exciting opportunity for a Junior Data Scientist to join a growing and innovative energy trading company. You will have the opportunity to work on challenging projects, gain valuable industry experience, and develop your skills in a supportive and dynamic environment.\n\nKey Responsibilities:\nDeveloping deep understanding of the industry and delivering key insights by analysing data\nForecasting different fundamentals such as wind power generation and demand\nResearching and implementing new trading strategies\nBuilding and developing new or existing data infrastructure\nYour background:\nBachelor's or master's degree in Computer Science, Mathematics, Statistics, or a related field\nStrong proficiency in programming languages such as Python and R\nExperience with data analysis and manipulation tools such as Pandas, NumPy, and SQL\nKnowledge of data visualization tools such as Matplotlib and Seaborn\nExperience that demonstrates your team-oriented, innovative, and strategic working styles.\nPro-active and flexible work mentality\nInterest in data-driven trading\nFluency in English, both written and spoken.",
         "Hyderābād",
         "539530.5",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Self-employed",
         "Colleges & Universities",
         "Education",
         "Unknown / Non-Applicable",
         "4.5",
         "4.3",
         "4.6",
         "4.7",
         "4.6"
        ],
        [
         "17",
         "Skyla Software Solution",
         "Data Entry Operator",
         "4.0",
         "The ideas that make life easier for millions of small and medium businesses across the globe are born here – at Skyla. We are a hardcore technology company with our own coding language where you get to build the product from the ground up. We nurture and inspire our employees by providing them with a collaborative environment that allows for a fine work life balance along with extensive employee benefits. At Skyla, we welcome people with different passions, interests and ethnicities because we believe each one of us has something unique to offer.\n\nCreativity is Awarded\nA comprehensive rewards and recognition framework which appreciates and acknowledges the contributions of the creative best.\nExperience on your side\nGet an opportunity to work with innovative minds who have been working tirelessly to find the best solutions for SMB's.\nPath breaking projects\nSkyla World initiatives include expanding to Cloud and Mobile and you’ll get to work on building the products from the ground-up\nFine work life balance\nEmployee friendly HR polices endorse work - life balance making it one of the best companies to work for.\n\nSupport\nJob Title:Data Entry Operator\nTechnology:Any Technology\nYear(s) Of Experience:N/A\n\n\nPlease email your CV to careers@skylasoft.com",
         "Jamshedpur",
         "84853.0",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Company - Public",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "4.0",
         "3.0",
         "5.0",
         "4.0",
         "2.0"
        ],
        [
         "18",
         "TVS Supply Chain Solutions",
         "Data Entry Operator",
         "3.4",
         "Department\nWarehouse Operations\nJob posted on\nJan 09, 2024\nEmployee Type\nFixed Term Contract\nExperience range (Years)\n1 year - 2 years\n\n1. Picklist generation\n2. Invoice generation\n3. Packing slip generation\n4. E-way bill preparation\n5. TAT updation\n6. Order creation\n7. GRN creation\n8. MIS report",
         "Bengaluru",
         "255288.0",
         "/yr (est.)",
         "501 to 1000 Employees",
         "--",
         "Company - Private",
         "Shipping & Trucking",
         "Transportation & Logistics",
         "Unknown / Non-Applicable",
         "3.2",
         "2.9",
         "3.0",
         "3.0",
         "3.0"
        ],
        [
         "21",
         "Sanfoundry",
         "Data Scientist - Python/ Machine Learning",
         "4.0",
         "Job Code: Machine-Learning-24015\n\nLocation: Mumbai\nExperience: Fresher-Mid Level\nDate Posted: 2023-12-27\nValid Through: 2024-01-27\n\nJob Description:\nJob Description\nIntelligence Node is a real-time eCommerce price intelligence platform that empowers businesses to drive product level profitability and grow margins using data-driven competitive insights.\nUnlike niche applications or software corporations that have acquired and merged products, Intelligence Node is an independent data powerhouse that has created the world's largest pricing dataset with unmatched accuracy, powered by proprietary AI-driven algorithms packaged in an intuitive and beautiful user interface. It is competitive intelligence and price optimization simplified.\nIntelligence Node maps more than 1 billion unique products across 190,000 brands for more than 1,400 categories across 100+ languages every minute - the dataset feeding the growth of more than $600 billion in retail revenue globally. It is the platform of choice for hundreds of retailers and brands worldwide, including Fortune 500 retailers and category leaders like Nestle, Prada, LIDL, SSENSE, Li & Fung, John Lewis, Lenovo and many others.\nLed by a revolutionary team of experienced executives and leaders from fashion, retail, big data and e-commerce sectors, Intelligence Node is backed by top investors including MegaDelta Capital and Orios Venture Partners.\nFor more information, visit www.IntelligenceNode.com and find us online at Twitter @bigdataNODE.\nAs a Junior Data Scientist, you will work closely with senior data scientists and business stakeholders to identify data-driven insights\nAnalyze retail data sets using statistical techniques and machine learning algorithms\nDevelop predictive models to support business decisions in the retail sector\nCollaborate with senior data scientists and business stakeholders to identify data-driven insights and opportunities\nDevelop and implement data pipelines for data processing and preparation\nSupport the development of reports and dashboards to visualize and communicate data insights\nStay up-to-date with the latest developments in the field of data science and retail analytics.\nRequirements\nA minimum of 2-3 years of experience in Data Science with a strong understanding of Python and its data science libraries such as NumPy, Pandas, and Scikit-learn\nStrong understanding of statistical techniques and their applications in the retail sector\nExperience with machine learning algorithms and their applications in the retail sector\nFamiliarity with retail data sets and the ability to clean, manipulate, and prepare data for analysis\nExcellent communication skills and the ability to work collaboratively in a team environment.\nTechnical Skills Required\nPython: pandas, numpy (expert level)\nframework: TensorFlow, Keras, PyTorch (basics)\nAPIs: Flask, FastAPI\nMachine Learning: understanding of classification and regression-based algorithms\nDeep Learning: Basic knowledge of Neural Nets\nQualification : Bachelor's or master's degree in computer science, Mathematics, Statistics, or related fields",
         "Mumbai",
         "539530.5",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Self-employed",
         "Colleges & Universities",
         "Education",
         "Unknown / Non-Applicable",
         "4.5",
         "4.3",
         "4.6",
         "4.7",
         "4.6"
        ],
        [
         "34",
         "Sanfoundry",
         "Data Scientist",
         "4.0",
         "Job Code: Data-Scientist-240111\n\nLocation: Gurugram\nExperience: Fresher-Mid Level\nDate Posted: 2023-12-28\nValid Through: 2024-01-28\n\nJob Description:\nAs a Data Scientist in ReD, you will be responsible for providing simple and intuitive solutions to the most complex problems in renewables industry. This will involve building machine learning and deep learning models which are coupled with contextual business knowledge. You will work with a world class team of site managers, subject experts, data engineers and data scientists to build the next generation data-driven renewable company.\n\nFurther Responsibilities Will Include\nImplement data science solutions which may involve machine learning or data science techniques to increase and optimize outputs\nAcquire extensive knowledge of renewables industry practices and implement tailor made techniques to solve problems relevant to the same\nAssess the effectiveness and accuracy of data sources and data gathering techniques\nWork closely with business and site teams to identify opportunities for leveraging data to drive business solutions\nCreate data trails to monitor pre-define KRAs and work relentlessly to improve them\nOur Ideal Candidate\nGood programming and logical skills\nWorking knowledge of R or Python programming language. Exposure to distributed and cloud computing is a plus\nKnowledge of statistical approaches, incl. advanced machine learning techniques (e.g. Support Vector Machines, Machine Learning, Linear models, decision trees and forests, boosting, multivariate analysis, stochastic models, and sampling methods) and/or optimization\nExcellent verbal and written communication skills\nAbility to learn and implement new technologies quickly\nAbility to uncover data trends to gain insights about expected behaviour and perform actions accordingly (prescriptive analytics)\nStrong critical thinking and problem solving skills with an analytical mind-set and go-getter attitude\nProgramming Skills\nBasic knowledge of at least 1 programming language - Python or R\nAlgorithms - Theoretical knowledge of at least 3-4 algorithms\nHands on implementation - Developed at least 1-2 models\nFunctional/ Domain Expertise\nWorked for at least 2+ years as part of an analytics team responsible for model building and deployment\nBasic level of articulation of theoretical concepts\nAbility to communicate with cross functional roles is a plus\nTeamwork\nDisplays ability to work both as an individual and as a self-motivated team player\nHas worked in large teams with an agile setup in the past",
         "Gurgaon",
         "469574.0",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Self-employed",
         "Colleges & Universities",
         "Education",
         "Unknown / Non-Applicable",
         "4.5",
         "4.3",
         "4.6",
         "4.7",
         "4.6"
        ],
        [
         "37",
         "Reliance Games",
         "Data Analytics",
         "3.5",
         "CAREERS\nData Analyst\nCompany: Reliance Games\nPosition: Data Analyst\nExperience: 2-5 Years\nLocation: Pune\nWebsite: www.reliancegames.com & www.zapak.com\nWho We Are\nReliance Games is a global mobile developer and publisher with more than 500+ Million downloads and most well-known for critically acclaimed game franchises like WWE Mayhem, Real Steel’s World Robot Boxing, Pacific Rim, Hunger Games, Drone: Shadow Strike, Little Singham, etc.\nJob Description:\nDevelop complex queries and procedures to extract data for game analytics.\nCreate dashboards on a daily basis handling multiple games simultaneously.\nPresent the insights and findings to key stakeholders.\nWork closely with Product Managers to identify analytical questions.\nWork independently on the assigned tasks.\nAssist in development of processes to automate data analysis.\nLearn and adapt to new methods, tools and technologies.\nRelevant experience in SQL Querying, Data Analysis using Microsoft Excel/Google Sheets, Data Visualization using Tableau and Python/R Programming.\nWe’re looking for someone who has\nAnalytical Skills: Data analysts work with large amounts of data: facts, figures, and number crunching. You will need to see through the data and analyze it to find conclusions.\nCommunication Skills: Data analysts are often called to present their findings, or translate the data into an understandable document. You will need to write and speak clearly, easily communicating complex ideas.\nCritical Thinking: Data analysts must look at the numbers, trends, and data and come to new conclusions based on the findings.\nAttention to Detail: Data is precise. Data analysts have to make sure they are vigilant in their analysis to come to correct conclusions.\nMath Skills: Data analysts need basic math skills to estimate numerical data.\nRequirement\nBachelor’s Degree or MBA along with Data Analytical.\nStrong Passion for games and gaming self-direction, drive and leadership.\nStrong analytical capabilities in Excel, with attention to detail.\nExperience in Data Analyst Role in online games, social media or consumer internet company, desirable.\nStrong verbal and written Communication Skills.\nWhat we offer you:\nWork in a studio that has complete P&L ownership of games.\nCompetitive salary and Performance Link Incentives.\nFull medical, accident as well as life insurance benefits.\nGenerous Paid Maternity/Paternity leave.\nEmployee Assistance Programs.\nActive Employee Resource Groups – Women at Zapak.\nFrequent employee events.\nFlexible working hours on many teams.\nCasual dress every single day.\nWork with cool people and impact millions of daily players!.\nKindly share your CV and send it to jobs@reliancegames.com with subject line as Data Analyst.\n\n\nConsumers can find high-quality entertainment created exclusively for their mobile devices wherever they see the ‘RG’ character logo or at www.reliancegames.com",
         "Pune",
         "469012.0",
         "/yr (est.)",
         "51 to 200 Employees",
         "--",
         "Company - Private",
         "Video Game Publishing",
         "Media & Communication",
         "Unknown / Non-Applicable",
         "3.8",
         "3.2",
         "3.4",
         "2.9",
         "3.1"
        ],
        [
         "39",
         "FinAdvantage",
         "FACPL_048 - Data Analyst",
         "4.0",
         "Job Description\n\nContribute to the development of deliverables on client mandates, including, but not limited to:\nDevelop an in-depth understanding of software/tools used by the Client and develop a reference manual,\ncustomization procedure, etc.\nGenerate reports from the above packages.\nUpdate or cleanse data on a need basis.\nClean and curate data for the analysis, including but not limited to removing errors, identifying outliers, and\ntransforming data into a format that can be analyzed.\nCreate automated modules in Excel, to extract data from the client sources to report on the quality of data\ncapture, meaningfulness, adequacy and fitness for purpose.\nSupport client leaders in developing internal quality frameworks and establish a tracking cadence for these.\nManage and improve systems for routine organizational data collection, data visualization, and analytics, and\nstrengthen data use for decision-making, to align with the quality framework.\nDesign and develop periodic Dashboard reports for the Client leadership/management stating trends, patterns,\nand predictions using relevant data.\nConduct and support regular strategic reviews and updates based on Client data.\nCollect, organize, analyze, and report on company-level data to track and articulate the Company’s progress\ntoward key goals, objectives and supporting initiatives.\nParticipate in identifying current and future strategic options and assist in developing submissions, reports or\nbriefs to achieve desired outcomes.\nWork collaboratively with the wider team of Accounting and Product teams to facilitate the implementation of\ndata-driven solutions.\nMaintain a culture of delivery, sustainable high performance and outcomes.\n\nSkills\n\nExcellent knowledge of written and spoken English.\nFAST excel modeling skills to create flexible/scalable models.\n• Advanced power point skills. • Sensitivity to ethical implications of data usage, storage, and analysis.\nCritical thinking and flexibility to shift to changing tasks/priorities.\nAttention to detail and solution-oriented mindset.\nExcellent time management skills.\nCross-cultural awareness and sensitivity; alignment to global work culture.\n\nEducation and Professional Experience\n\nBachelor’s degree in accounting/ finance.\nProgressing towards completion of professional certification (Data Analytics), preferred.\n3 to 5 years of professional experience in Business Consulting, Fintech Services or data driven roles",
         "Bengaluru",
         "1100000.0",
         "/yr (est.)",
         "51 to 200 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "2.0",
         "2.0",
         "1.0",
         "2.0",
         "1.0"
        ],
        [
         "48",
         "Syneos Health Clinical",
         "Data Scientist I",
         "3.8",
         "Description\nSyneos Health® is a leading fully integrated biopharmaceutical solutions organization built to accelerate customer success. We translate unique clinical, medical affairs and commercial insights into outcomes to address modern market realities.\nOur Clinical Development model brings the customer and the patient to the center of everything that we do. We are continuously looking for ways to simplify and streamline our work to not only make Syneos Health easier to work with, but to make us easier to work for.\nWithin Kinetic (a division of Syneos Health) our strength is the efficiency with which we deliver leading- edge products and support to those we serve. We’re proud to set the standard for success in our industry.\nAs a Manager, Data Science, you will play a pivotal role within the Data Science team. You will be responsible for providing technical leadership, overseeing data science projects, and building innovative products.\nDiscover what our 29,000 employees, across 110 countries already know:\nWORK HERE MATTERS EVERYWHERE\nWhy Syneos Health\nWe are passionate about developing our people, through career development and progression; supportive and engaged line management; technical and therapeutic area training; peer recognition and total rewards program.\nWe are committed to our Total Self culture – where you can authentically be yourself. Our Total Self culture is what unites us globally, and we are dedicated to taking care of our people.\nWe are continuously building the company we all want to work for and our customers want to work with. Why? Because when we bring together diversity of thoughts, backgrounds, cultures, and perspectives – we’re able to create a place where everyone feels like they belong.\nJob responsibilities\nQuickly understand Kinetic’s current data and insights products and understand common trends across our services to design repeatable and scalable solutions\nDevelop and code software programs, algorithms and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources to build POC analytic solutions.\nGenerate new product requirements for the engineering group to enhance the analytic capabilities of existing database to support various data science needs.\nPerform analyses utilizing advanced analytical techniques to generate actionable insights and solutions for client services and product development.\nIdentify meaningful insights from large data and metadata sources; interpret and communicates insights and findings from analysis and experiments to product, service, and business managers.\nHave broad expertise or innovative knowledge, use skills to contribute to development of company objectives and principles and to achieve goals in creative and effective ways.\nQualifications\nWhat we’re looking for\nMasters degree in a STEM field required.\n3-4 years applicable experience\nProficiency in SQL for data extraction and manipulation from databases\nStrong programming skills (e.g., Python, R) and experience with data manipulation libraries (e.g., Pandas) and machine learning frameworks (e.g. scikit-learn, TensorFlow, PyTorch)\nProven experience in designing and implementing data pipeline for machine learning.\nExperience with machine learning, statistical analysis, and data manipulation techniques.\nDetailed understanding of how data science models are developed.\nExperience with code version control platforms (GitHub or Azure DevOps)\nMust be organized, detail oriented and work effectively in a fast-paced environment.\nIndependent problem-solving and grit. Willingness to own one’s work, and confidence to push best practices. Strong attention to detail.\nRelevant work experience in healthcare or campaign measurement, marketing analytics and resource optimization in the pharma domain is a plus.\nGet to know Syneos Health\nOver the past 5 years, we have worked with 94% of all Novel FDA Approved Drugs, 95% of EMA Authorized Products and over 200 Studies across 73,000 Sites and 675,000+ Trial patients.\nNo matter what your role is, you’ll take the initiative and challenge the status quo with us in a highly competitive and ever-changing environment. Learn more about Syneos Health.\nAdditional Information:\nTasks, duties, and responsibilities as listed in this job description are not exhaustive. The Company, at its sole discretion and with no prior notice, may assign other tasks, duties, and job responsibilities. Equivalent experience, skills, and/or education will also be considered so qualifications of incumbents may differ from those listed in the Job Description. The Company, at its sole discretion, will determine what constitutes as equivalent to the qualifications described above. Further, nothing contained herein should be construed to create an employment contract. Occasionally, required skills/experiences for jobs are expressed in brief terms. Any language contained herein is intended to fully comply with all obligations imposed by the legislation of each country in which it operates, including the implementation of the EU Equality Directive, in relation to the recruitment and employment of its employees. The Company is committed to compliance with the Americans with Disabilities Act, including the provision of reasonable accommodations, when appropriate, to assist employees or applicants to perform the essential functions of the job.\n#LI-MS5",
         "Remote",
         "539530.5",
         "/yr (est.)",
         "10000+ Employees",
         "--",
         "Company - Private",
         "Biotech & Pharmaceuticals",
         "Pharmaceutical & Biotechnology",
         "Unknown / Non-Applicable",
         "3.5",
         "3.5",
         "3.8",
         "3.5",
         "3.9"
        ],
        [
         "53",
         "JLL",
         "Sustainability Data Analyst",
         "3.9",
         "JLL supports the Whole You, personally and professionally.\n\nOur people at JLL are shaping the future of real estate for a better world by combining world class services, advisory and technology to our clients. We are committed to hiring the best, most talented people in our industry; and we support them through professional growth, flexibility, and personalized benefits to manage life in and outside of work. Whether you’ve got deep experience in commercial real estate, skilled trades, and technology, or you’re looking to apply your relevant experience to a new industry, we empower you to shape a brighter way forward so you can thrive professionally and personally.\n\nWhat this job involves?\n\nShaping the future of real estate for a better world\nAt JLL, we see a Brighter Way forward for our clients, our people, our planet, and our communities. With over 200 years of real estate experience, we are, and always have been, in continual pursuit of brighter ways of working.\n\nWe bring to life see a Brighter Way in all that we do by seeking better, smarter, more innovative ways of working. We approach our work in a warmer, more optimistic, and inclusive way.\nJLL is a global leader in helping clients envision where people will live, work, play, shop, and eat.\n\nWhat this opportunity involves:\nWe seek a Junior Sustainability Data Analyst to join our team. You will support the sustainability data analyst reporting, data management, platform, compliance and reporting functions for a wide range of assets across JLL.\n\nJLL's purpose-driven global sustainability program delivers impact on climate action for sustainable real estate, healthy spaces for all people and thriving communities.\nWe are a rapidly expanding team, and over time we continuously support your growth with development opportunities available within our data and analytics teams.\n\nAn overview of the role:\nAssist the reporting team with insights, analytics, preparing data and presentations.\nAssist the team with delivering projects that will enable clients to meet sustainability reporting objectives.\nDevelop a detailed understanding of JLL’s sustainability reporting application and how we support clients in measuring sustainability performance.\nAssist the team with client delivery milestones to ensure they are being met.\nSounds like you? This is what we are looking for\nA passion for Sustainability and pulling together associated Data and Reporting.\nIntermediate Excel skills.\nInsights, element visualisation, and presenting data.\nExcellent communication skills.\nWhat you can expect from us:\nYou’ll join an entrepreneurial, inclusive culture. One where the best inspire the best. Where like-minded people work naturally together to achieve great things. Join us to develop your strengths and enjoy a fulfilling career full of varied experiences. Keep those ambitions in sight and imagine where JLL can take you.\n\nAs an organisation, we don’t just accept that we are a place of many different people, but we embrace it, we celebrate it, and we proactively support the needs that difference brings. JLL is committed to equal opportunity regardless of race, gender, age, sexual orientation or disability, and that is why, for more than a decade, we continue to rank among the World’s Most Ethical Companies.\n\nWe are dedicated to offering veterans from all ranks and services a successful civilian career as they transition out of the military. We recognise and appreciate the skills acquired in their service careers as vital and transferable to our workforce.\n\nIf this job description resonates with you, we encourage you to apply even if you don’t meet all of the requirements below. We’re interested in getting to know you and what you bring to the table!\n\nPersonalized benefits that support personal well-being and growth:\n\nJLL recognizes the impact that the workplace can have on your wellness, so we offer a supportive culture and comprehensive benefits package that prioritizes mental, physical and emotional health.\n\nAbout JLL –\n\nWe’re JLL—a leading professional services and investment management firm specializing in real estate. We have operations in over 80 countries and a workforce of over 102,000 individuals around the world who help real estate owners, occupiers and investors achieve their business ambitions. As a global Fortune 500 company, we also have an inherent responsibility to drive sustainability and corporate social responsibility. That’s why we’re committed to our purpose to shape the future of real estate for a better world. We’re using the most advanced technology to create rewarding opportunities, amazing spaces and sustainable real estate solutions for our clients, our people, and our communities.\n\nOur core values of teamwork, ethics and excellence are also fundamental to everything we do and we’re honored to be recognized with awards for our success by organizations both globally and locally.\n\nCreating a diverse and inclusive culture where we all feel welcomed, valued and empowered to achieve our full potential is important to who we are today and where we’re headed in the future. And we know that unique backgrounds, experiences and perspectives help us think bigger, spark innovation and succeed together.",
         "Bengaluru",
         "539530.5",
         "/yr (est.)",
         "10000+ Employees",
         "--",
         "Company - Public",
         "Real Estate",
         "Real Estate",
         "$5 to $10 billion (USD)",
         "3.7",
         "3.5",
         "3.8",
         "3.5",
         "3.7"
        ],
        [
         "57",
         "WovV Technologies",
         "Data Engineer (WFH)",
         "4.7",
         "Job Description: Data Engineer\nLocation: Remote\nExperience: 5+ years.\nDurations: 3 months with a very likely extension\nMandatory Technology: DBT, SQL, Snowflake, Stakeholder management and requirements refinement\nPrimary Skills: AWS, Terraform, APIs, Data Warehouse, Data Lakehouse\nShift Details: UK shift\nRoles and Responsibilities:\nEngineering, upgrading, and overseeing data flow systems to ensure efficient data migration into diverse storage systems.\nConstructing and upholding solutions for data warehouses and data lakes.\nForging and implementing data structures that cater to a variety of business needs.\nScripting proficient and robust codes using languages such as SQL.\nSpearheading data solution design with an emphasis on quality, automation, and performance.\nManaging and enhancing data platform pipelines for reliability and scalability.\nGuaranteeing that data is accessible in a condition and timeframe that suits business and analytical needs.\nCollaborating with the Data Governance team to ensure adherence to regulations like GDPR and CISO policies, and integrating data quality directly into conduits.\nEngaging with interdisciplinary teams to comprehend data necessities and providing assistance for data-centric projects.\nEngaging with stakeholders and clients, addressing their data needs with tailored solutions.\nRequired Skills and Experience:\nExtensive experience leading transformations of cloud data platforms.\nDemonstrated success in deploying large-scale data and analytical solutions in cloud environments. Hands-on proficiency in creating comprehensive data pipelines on DBT.\nDeep understanding of contemporary data architectures (Data Lake, Warehouse, and Lakehouse).\nSolid comprehension of data architecture and data modeling techniques.\nEfficient management of data pipelines from a cost perspective.\nFamiliarity with CI/CD-driven data pipelines and infrastructure.\nAgile delivery methodology using Scrum and Kanban frameworks.\nAbility to independently scope, estimate, and deliver tasks on time within an agile framework.\nProactive in enhancing data processes.\nInherent drive to learn and adopt new skills and technologies.\nCapability to excel in a rapidly evolving setting.\nProficiency in managing projects efficiently and addressing challenges collaboratively.\nDriven by goals and a result-focused mindset.\nDetail-oriented with thoroughness.\nStrong communication skills, competent in presenting, informing, and guiding others.\nAptitude for bridging the gap between technical and business-focused groups.\nComfortable engaging with individuals at all organizational levels.\nJob Types: Full-time, Permanent\nSalary: Up to ₹1,000,000.00 per year\nBenefits:\nProvident Fund\nWork from home\nSchedule:\nUK shift\nSupplemental pay types:\nPerformance bonus\nYearly bonus\nWork Location: Remote",
         "Remote",
         "1000000.0",
         "/yr (est.)",
         "Unknown",
         "--",
         "Company - Public",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "4.5",
         "4.5",
         "4.6",
         "4.6",
         "4.6"
        ],
        [
         "58",
         "Rotork",
         "Data Analyst",
         "3.7",
         "Company Description\n\nRotork is the market-leading global flow control and instrumentation company, helping our customers manage the flow or liquids, gases and powders across many industries worldwide.\nOur purpose is Keeping the World Flowing for Future Generations.\nFor over sixty years, the world has relied on us to create the things that keep everything moving. From oil and gas to water and shipping, pharmaceuticals and food- these are the flows on which our modern world depends.\nToday we're respected and admired for our people, performance and products. Our success flows from our commitment to engineering excellence, and that's what we will always pursue, safely and sustainably.\nRotork is going through an exciting period of change and growth, building on our existing market success. It's a great time to join us and make an impact in shaping the future of our business.\nhttp://www.rotork.com\n\nJob Description\n\nWorking as part of the Data team and support team on all aspects of Data management.\nYour responsibilities will include,\nData profiling, Schema mapping, Master Data Management, Data analysis, Data cleansing, Data migration\nData quality checks and advising project teams on potential data issues and actions.\nDeliver data service requests in line with BAU demand.\nTechnical Skills\nEssential:\nRelational Databases – SQL Server, 2016 is a minimum, Oracle nice to have.\nSQL – T-SQL, Stored Procedures, Views, DML, DDL\nETL/ELT – SSIS and Azure Data Factory\nExposure to Azure SQL Database\nOffice – Advanced Excel skills, Word, Visio, PowerPoint\nDesirable:\nData Migration Tools\nMicrosoft 365 Applications - Power BI, PowerApps, SharePoint\nExposure to Microsoft Dynamics or any ERP\n\nQualifications\n\nSoft Skills\nEssential:\nExcellent written and verbal communication skills\nGreat attention to detail\nCommitment to personal excellence\nDesire to expand technical expertise.\nCongenial attitude - team player\nDisciplined work ethic\nDesirable:\nOut-of-the-box thinker with creative problem-solving ability.\nAbility/Experience to establish priorities, work independently, and proceed with objectives without supervision\nPersonal Specification:\nEssential\nDemonstrate integrity and honesty.\nCapable of communicating at all levels\nEthical and transparent leadership, setting an example to others.\nDemonstrate an appetite and ability to work collaboratively in a complex and matrixed business.\nAble to deal with people with different cultures throughout the region.\nAble to thrive in a changing business, embracing ambiguity and solving complex problems.\n\nAdditional Information",
         "Chennai",
         "569738.0",
         "/yr (est.)",
         "1001 to 5000 Employees",
         "--",
         "Company - Public",
         "Electronics Manufacturing",
         "Manufacturing",
         "$500 million to $1 billion (USD)",
         "3.5",
         "3.2",
         "3.5",
         "3.4",
         "3.9"
        ],
        [
         "69",
         "Market Expertise",
         "Data Research Analyst",
         "4.0",
         "We have created a positive work environment for our employees, by encouraging honest dialogue, teamwork, innovative thinking and good work ethics. We nurture talent through effective means such as mentoring, training and providing constructive feedback that are mutually beneficial. We look forward to working with smart ambitious and competent individuals, who can share our vision for success and are also passionate about their work. Apart from being reliable and disciplined, they should be highly-driven and goal-oriented with good communication skills.\nJob Description :\nDeveloping and implementing data analysis, data collection systems and other strategies that optimize statistical efficiency and quality.\nResponsibilities\nInterpret data, analyze results using statistical techniques and provide ongoing reports.\nAcquire data from primary or secondary data sources and maintain databases/data systems.\nMaintain databases, data collection systems, data analytics and other strategies that optimize statistical efficiency, quality and data standardization.\nAcquire data from primary or secondary data sources and maintain databases/data systems\nIdentify, analyze, and interpret trends or patterns in complex data sets.\nFilter and clean data by reviewing system reports, printouts, and performance indicators to locate and correct data inaccuracies.",
         "Pune",
         "304180.0",
         "/yr (est.)",
         "51 to 200 Employees",
         "--",
         "Company - Public",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "4.3",
         "4.0",
         "4.1",
         "3.9",
         "4.0"
        ],
        [
         "72",
         "Veranex, Inc.",
         "Clinical Data Coordinator 2",
         "4.0",
         "Minimum Requirement\nBachelor’s degree or international equivalent in life sciences\n3-5 years of relevant work experience\n\nLevel-Specific Responsibilities:\nPerforms study status tracking, data entry and verification.\n\nPerforms data review and quality control procedures.\n\nDevelops and provides input into project-specific guidelines.\n\nPerforms data review, identification of data issues, and quality control procedures.\n\nParticipates in User Acceptance Testing.\n\nParticipates in the query management and data cleaning process.\n\nArchives all study-related documents.\n\nProvides advise or solutions in area of expertise.\n\nMay participate in audits and inspections.\n\nParticipates in project team meetings.\n\nSkills Required:\nWorking knowledge of clinical research and the drug development process\n\nWorking knowledge of databases, tracking, validation, programming, word-processing and spreadsheet software\n\nWorking knowledge of clinical databases and query management\n\nWorking knowledge of organization procedures and policies and ensures actions comply.\n\nGood written and oral communication skills\n\nGood attention to detail\n\nGood organization and time management skills\n\nAbility to take a proactive approach to work\n\nAbility to solve simple to moderate problems\n\nAbility to multitask and prioritize work\n\nGood ability to work in cross-functional teams\n\nDeveloping professional expertise, applies company policies and procedures to resolve issues.\nWorks on routine problems of moderate scope. Exercises judgment following standard practices and procedures in analyzing situations or data from which answers can be readily obtained. Builds stable working relationships internally.\n\nReceives occasional guidance on day-to-day work and moderate guidance on new projects or assignments.",
         "Bengaluru",
         "539530.5",
         "/yr (est.)",
         "Unknown",
         "--",
         "Company - Private",
         "Chemical Manufacturing",
         "Manufacturing",
         "Unknown / Non-Applicable",
         "3.7",
         "3.4",
         "3.6",
         "3.1",
         "4.2"
        ],
        [
         "74",
         "Navig8 Group",
         "Junior Data Analyst",
         "4.0",
         "Junior Data Analyst\nWe are looking for a junior business analyst to join Integr8 Fuels in India. You are a curious and self-motivated individual with good SQL, Python and BI skills (Tableau or similar), acquired either as part of your completed studies or work experience, who will be keen to learn about the marine fuels industry, understand the role of our business and all associated activities and apply your knowledge and skills to help the company. We particularly value the ability to generate ideas and find solutions. You are a proactive individual who knows how to look for information, separate what is important from what is not and not be afraid to ask questions.\nThe successful candidate will have up to 2-3 years of experience working with SQL, Python (pandas) and a BI tool with proven examples of work and what impact this had. Marine fuels or shipping experience is not necessary as long as you are a quick and willing learner, on the job training will be provided. Excellent English is a must, including the ability to explain complex problems in a simple but logical way as well as the ability to interact and communicate well with your colleagues.\nYou will work normal office hours closely with a Business Analyst based in our Mumbai office and with other teams based both on site and remotely. This is a broad role where you will be able to learn and take part in multiple initiatives, from company’s financial and performance analysis to analysing fuels and having your input into the in-house software products that support operations.\nJob Responsibilities\nDevelop a deep understanding and knowledge of the industry and what the company does\nUnderstand how company’s activities are shown in the data we generate and collect\nLearn and understand the systems we use (including our digital platform ENGINE), the data structure and set-up behind them\nDevelop code, workflows and dashboards to analyse different aspects of company and industry operations, identify issues and suggest where and what improvements can be made\nWork on different datasets, ensuring their accuracy, timeliness, relevance, and structure\nHelp maintain data, workflow execution schedules and systems integrity, spotting bugs and errors, investigating and reporting them as well as proposing solutions to problems\nSupport other teams with their data and analytics requests\nSkills\nExcellent knowledge of SQL, Python (pandas) and a BI tool (Tableau or equivalent), understanding of data structures and databases\nExcellent English, communication and interpersonal skills, the position will require working with different parties\nExceptional attention to detail, curiosity, ability to go beyond 'scratching the surface'\nQuick learner: would need to understanding complex problems and functions of the business and how software relates to these\nAbility to thrive in a fast paced environment, often with little or no structure and supervision\nConfident user of MS Office\nEducation\nEducated to a degree level and up to 2-3 years of experience coding in SQL, Python and BI dashboards\nCompany Information\nIntegr8 is one of the fastest growing companies in the marine fuel space and provides back-to-back bunker trading services to over 650 customers. Integr8 forms part of the larger shipping group, Navig8, which is the world’s largest independent oil tanker pool and commercial management company with offices based in the UK, US, EMEA and Asia Pac.\nBeyond the core shipping and bunkering business, the Group has a number of digital products to offer to its customers with ENGINE being one of them. Since its launch a few years ago, ENGINE has grown into a unique marine fuels intelligence platform which is constantly pushing boundaries in a notoriously opaque market. For decades marine fuel buyers have been in the dark, trying to catch up with scattered and transient information from ports around the world. ENGINE brings shipowners together for more transparency in the marine fuels market.",
         "Mumbai",
         "539530.5",
         "/yr (est.)",
         "201 to 500 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "3.1",
         "4.0",
         "3.3",
         "3.2",
         "3.7"
        ],
        [
         "76",
         "Mapmygenome",
         "Data Scientist",
         "2.8",
         "Develop machine learning algorithms to integrate genomics and biomedical data in healthcare\nMaintain databases with information on genetic variants and associated phenotypes\nAnalyze data coming from high-throughput machines, using advanced computational techniques\nContribute to various research studies undertaken by scientific team",
         "Hyderābād",
         "438887.0",
         "/yr (est.)",
         "51 to 200 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "2.9",
         "2.4",
         "2.5",
         "2.5",
         "2.2"
        ],
        [
         "82",
         "Educate Girls",
         "Data Analyst",
         "3.8",
         "changing operational and environmental requirements. Such changes will be incorporated in the job description on annual basis\nData Analyst – Project Management Unit\nPosition Location: Jaipur, Rajasthan\nEstablished in 2007, Educate Girls' (EG) is a non-profit that focuses on mobilizing communities for\ngirls' education in India's rural and educationally backward areas. Strongly aligned with the 'Right to\nEducation Act' or the 'Samagra Siksha', Educate Girls is committed to the Government's vision to\nimprove access to primary education for children, especially young girls. Educate Girls (EG) currently\noperates successfully in over 20,000+ villages in Rajasthan, Madhya Pradesh and Uttar Pradesh with a\nstrength of 2200+ employees & amp; 13000+ volunteers (Team Balika). By leveraging the\nGovernment’s existing investment in schools and by engaging with a huge base of community\nvolunteers, Educate Girls helps to identify,enrol and retain out-of-school girls and to improve\nfoundational skills in literacy and numeracy for all children (both girls and boys). This helps deliver\nmeasurable results to a large number of children and avoids parallel delivery of services.\n\nOur Values\nGender Equality Being able to treat people equally irrespective of gender\nIntegrity Possess the ability to “know and do” what is right\nExcellence Being outstanding or extremely good, striving to lead by performance\nexcellence\nCollaboration\nWorking effectively and inclusively with a range of people both within and\noutside of the organization\nEmpathy\nBeing able to understand and share the feelings of another and use that\nunderstanding to guide our actions\n\nOur Competencies\nStrategic Thinking – Think big\nyet act focused\nTaking Ownership – Feel\nresponsible & accountable\nAnalytical Thinking – Stay true\nto your data\nDeveloping Talent – Growing and taking people\ntogether\nEnsuring Alignment – Think differently but work\ntogether\n\n\n\nWe are currently establishing a Project Management Unit in collaboration with the Department of\nEducation, Government of Rajasthan. A dedicated project team will be tasked with assisting the\nDepartment of Education in devising and executing strategies aimed at enhancing the quality of\neducation, implementing open schooling initiatives, and providing technical support.\n\n\n\n\n\n\n\n\nThis document reflects the job content at the time of designing the job description and will be subject to periodic change in the light of\nchanging operational and environmental requirements. Such changes will be incorporated in the job description on annual basis\n\nPosition Overview:\nThe Data Analyst is expected to Monitoring, Analyzing data collected from Google forms &\nvarious other mediums that help EG & Department of education to maximise its impact. This\nrole will be instrumental in understanding the trends and developing monitoring and evaluation\ntools.\nPosition Key Responsibilities:\nMonitoring, Analyzing data collected from Google forms & various other\nmediums for Dept. of Education.\nUnderstanding and Analysis UDise data\nInterpret data, analysing results and trends, using statistical techniques where\nappropriate\nGenerate high quality formal analytical reports on key program delivery outcomes\nsuch as Enrolment, Retention and Learning, as per defined calendar\nConduct periodic data quality assessments through pattern and outlier analysis\nDevelop dashboards as per requirements\nAnalyse the data for inaccuracies and outliers and highlight possible risks in\nreporting that data\nBuild efficient and well documented methods for easy, repeatable extraction\nProvide training to the users in case of any new development/update of the\ntechnological solutions\n\n\nDesired Incumbent Profile:\nPreferred Education Background:\nBachelor’s Degree in a relevant data science field or a related field of study.\n\n\nPreferred Work Experience:\nMinimum 3 years of relevant work experience data analytics/data visualization/ data\nmanagement\nBasic to intermediate level of SQL programming experience and demonstrated ability to\ndesign and analyze data in relational databases. Experience with PostgreSQL desirable.\nAt least 1-2 years’ experience in data manipulation/cleaning using basic to intermediate\nPython and/or R.\n\n\n\n\n\n\n\n\n\n\n\nThis document reflects the job content at the time of designing the job description and will be subject to periodic change in the light of\nchanging operational and environmental requirements. Such changes will be incorporated in the job description on annual basis\nPreferred Skill Set:\nDemonstrated analytical and assessment skills of quantitative and qualitative data.\nExcellent oral and written presentation skills.\nIntermediate to advanced proficiency in Excel required, especially in visualization.\nBasic to intermediate skills in SQL and R / Python.\nStrong skills in data analysis and reporting.\nStrong skills in Microsoft Office tools such as Word, Excel, and PowerPoint. Visio\nknowledge desirable.\nKnowledge of statistical analysis software (such as SAS, SPSS, Tableau) and experience\nwith Tableau software would be a bonus.\nDemonstrated experience in dashboard design & development. Experience with Google\nData Studio desirable.\n\n\"Educate Girls is committed to achieving 50/50 gender balance in its staff. Female candidates are strongly\nencouraged to apply for this position.\"",
         "Jaipur",
         "900000.0",
         "/yr (est.)",
         "1001 to 5000 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "3.4",
         "3.6",
         "3.5",
         "3.2",
         "3.5"
        ],
        [
         "87",
         "Sanfoundry",
         "Junior Machine Learning Engineer",
         "4.0",
         "Job Code: Machine-Learning-24016\n\nLocation: Remote\nExperience: Fresher-Mid Level\nDate Posted: 2023-12-31\nValid Through: 2024-01-29\n\nJob Description:\nJob Description\nUnderstand the importance of shipping on-time and meet deadlines\nHelp form and maintain engineering standards, tooling, and processes\nPatiently dive deep to analyze complex challenges and come up with innovative solutions\nTake product ownership and push features over the line\nJob Requirements:\nBachelor’s/Master’s degree in Engineering, Computer Science (or equivalent experience)\nAt least 3+ years of relevant experience as a Machine Learning Engineer\nProlific experience working with Python and Deep Learning\nExtensive experience working with Machine Learning\nAbility to navigate through deadlines and work under pressure\nExcellent problem-solving and multitasking skills",
         "Remote",
         "539530.5",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Self-employed",
         "Colleges & Universities",
         "Education",
         "Unknown / Non-Applicable",
         "4.5",
         "4.3",
         "4.6",
         "4.7",
         "4.6"
        ],
        [
         "91",
         "WCG",
         "Jr. Data & Analytics Engineer",
         "3.2",
         "Description and Requirements\nJOB SUMMARY:\nThe Junior Data & Analytics (D&A) Engineer plays a pivotal role in the development of analytics-driven solutions that optimize and streamline clinical trial processes. Working closely with our data & analytics team and clinical trial experts, this role will harness the power of cutting-edge AI and LLM (Machine Learning and Deep Learning) technologies for enhancing decision-making in the field of clinical trial process. The Junior D&A Engineer is part of a team that is domain agnostic, working hands-on with structured as well as unstructured data, designing algorithms and implementing innovative capabilities that cater to an array of business use cases within the data & analytics function at WCG. As a core member of the data & analytics team, the Junior D&A Engineer designs and builds analytics frameworks, documents best practices, and works with the architecture and infrastructure teams on developing patterns.\n\nEDUCATION REQUIREMENTS:\nBachelor’s degree in a quantitative discipline (i.e. statistics, applied mathematics, computer science, data mining, machine learning, or some other empirical science) with specialization in data science, engineering or related field preferred\nQUALIFICATIONS/EXPERIENCE:\nStrong programming skills, particularly in Python\nProficiency in data manipulation and analysis using SQL and data processing tools (e.g., Apache Spark)\nProficiency with machine learning and deep learning frameworks (e.g., TensorFlow, PyTorch)\nExcellent problem-solving and analytical skills\nStrong communication and teamwork skills\nKnowledge of healthcare and clinical trial processes is a plus.\nFamiliarity in a cloud platform (AWS, Azure)\nFamiliarity with relational and non-relational databases and how they work\nFamiliarity with big data technologies such as DataBricks, Hadoop, and Kafka\nPossess a data security mindset\nESSENTIAL DUTIES/RESPONSIBILITIES: To perform this job successfully, an individual must be able to perform each essential duty and responsibility satisfactorily. The accountabilities listed below are representative of the knowledge, skills, and/or ability required.\nData Collection and Integration: Collaborate with cross-functional teams to collect, clean, and integrate diverse healthcare and clinical trial data from various sources.\nMachine Learning Model Development: Design, develop, and implement machine learning and deep learning models to address specific challenges in clinical trials, such as patient recruitment, data analysis, protocol study, anomaly detection, document entity extraction, document generation, translation, and outcome prediction.\nData Processing: Create data pipelines and workflows to preprocess and transform data into a format suitable for analysis and modeling.\nModel Evaluation: Conduct rigorous testing and evaluation of machine learning models, ensuring accuracy, reliability, and robustness.\nData Visualization: Develop interactive data visualizations and dashboards to communicate insights and outcomes to both technical and non-technical stakeholders.\nDocumentation: Maintain detailed documentation of data engineering and analytics processes, including code, model specifications, and best practices.\nResearch and Innovation: Stay updated on the latest advancements in AI and LLM technologies and explore their potential applications in improving clinical trial processes.\nCollaboration: Work closely with clinical experts, data scientists, and engineers to develop and deploy solutions that directly impact the healthcare and clinical research industry.\nOther duties as assigned by supervisor. These may, on occasion, be unrelated to the position described here.\nTRAVEL REQUIREMENTS: 0% – 5%\n\n#LI-SA1\n\nWCG is proud to be an equal opportunity employer – Qualified applicants will receive consideration for employment without regard to race, color, national origin or ancestry, religion or creed, sex, sexual orientation, gender identity, age, marital status, disability, genetic information, citizenship, veteran status, reprisal or any other legally recognized basis or status protected by federal, state or local law.",
         "Karnataka",
         "539530.5",
         "/yr (est.)",
         "1001 to 5000 Employees",
         "--",
         "Company - Private",
         "Biotech & Pharmaceuticals",
         "Pharmaceutical & Biotechnology",
         "$100 to $500 million (USD)",
         "2.7",
         "3.1",
         "2.7",
         "2.5",
         "2.8"
        ],
        [
         "95",
         "Aptroid Consulting",
         "Junior Data Analyst",
         "3.8",
         "REQUIRED SKILLS\nShould be able to manage big volume of data.\nKnowledge on providing deep insights and develop predictive models as part of CRM/Marketing Analytics\nShould have good knowledge on Statistical concepts like Hypothesis testing, ANOVA/MANAOVA, Probability, Linear Regression, Logistics Regression, Various clustering Techniques and Forecasting.\nUnderstanding of Mathematical models (Navie Bayes, Support Vector Machines)\nKnowledge on R/Python/SAS will be an added advantage\nShould be able to provide analytical solutions to the open end business problems.\nPre-requisites for this Job:\nWilling to join immediately and ready to work in International time zones\nWilling to give 2 years commitment with the company",
         "Hyderābād",
         "379473.0",
         "/yr (est.)",
         "201 to 500 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "3.5",
         "4.3",
         "3.7",
         "3.6",
         "3.1"
        ],
        [
         "96",
         "Wizikey",
         "Junior Data Engineer (Delhi- NCR)",
         "3.7",
         "Company Description\n\nWizikey is a cloud-based marketing and Communications software that uses AI technology to monitor news, provide media insights, and automate reporting. It helps companies track their news presence, gather competitive intelligence, and connect with relevant reporters. With Wizikey, businesses can measure their PR efforts, optimize strategies, and drive better outcomes. Trusted by over 100+ businesses, including Reliance, Infosys, MapmyIndia, Blusmart, Physics Wallah and WebEngage, Wizikey enhances brand visibility globally.\n\nJob Description\n\nRole Overview:\nWe are seeking a passionate and talented Junior Data Engineer to join our team. As a key member of our data team, you will play a crucial role in managing our data systems and pipelines. This position is ideal for individuals who are eager to develop their skills in a fast-paced, innovative environment.\nKey Responsibilities:\n(ETL/ELT) Data Pipeline Development: Design, build, and maintain efficient ETL/ELT data pipelines to support data collection, integration, and extraction processes.\nDatabase Management: Work with various database technologies, both SQL and NoSQL, to store and manage data effectively.\nData Analysis and Reporting: Assist in analyzing data to provide actionable insights. Develop and maintain dashboards and reports for internal teams.\nCollaboration: Work closely with other teams, including engineering, product, and analytics, to understand data needs and implement solutions.\nQuality Assurance: Ensure the accuracy and integrity of data through quality checks and validation processes.\nLearning and Growth: Continuously learn and stay updated with the latest technologies and practices in data engineering.\n\nQualifications\n0-3 years of experience in a data engineering role (including internships or academic projects).\nProficiency in Python.\nFamiliarity with data pipeline and workflow management tools (preferably Apache Airflow).\nBasic understanding of SQL and experience with relational databases.\nKnowledge of big data technologies (e.g., Hadoop, Spark) is a plus.\nKnowledge of cloud platforms (preferably AWS) is a plus.\nStrong problem-solving skills and attention to detail.\nCandidates from Delhi NCR will be preferred.\nExcellent communication and teamwork abilities.\n\nAdditional Information\n\n\"Wizikey encourages and celebrates entrepreneurial culture. When you set out to create a new industry, you need to build a team of immensely talented folks from Technology and Communications and give them the freedom to experiment, learn and keep building. And with every addition of talent, this gets new fuel and the magic happens. And that is why we call ourselves Wizards\"",
         "Gurgaon",
         "619677.0",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "3.7",
         "3.0",
         "3.6",
         "3.4",
         "3.6"
        ],
        [
         "103",
         "Scymes Services Pvt. ltd",
         "Reference Data Analyst",
         "2.5",
         "*The Reference Data Analyst focuses on effective strategic data used across the bank’s systems and infrastructure. Reference data, either sourced externally or generated internally, covers a wide range of critical enterprise-wide information such as pricing, securities, books, financial products, clients, legal entities, accounts, and mandates.\n*The Reference Data Analyst supports various elements of the data lifecycle, from ensuring that data is captured from the best source, to validating and classifying, and operating controls that optimize its quality and maximize coverage. The Reference Data Analyst has responsibility for maintaining robust processes related to data set-up, storage and distribution, as well as system configuration, together with participation in projects designed to streamline infrastructure and improve efficiency.\n*Person should have application knowledge of Applications like SCD, PACE, Aladdin and vendor like Refintivie and Bloomberg.\nJob Types: Full-time, Fresher\nSalary: ₹20,000.00 - ₹25,000.00 per month\nBenefits:\nHealth insurance\nProvident Fund\nSchedule:\nDay shift\nAbility to Relocate:\nBangalore, Karnataka: Relocate before starting work (Required)\nWork Location: In person",
         "Bengaluru",
         "22500.0",
         "/mo (est.)",
         "Unknown",
         "--",
         "Company - Public",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "2.5",
         "2.5",
         "2.5",
         "2.0",
         "2.0"
        ],
        [
         "107",
         "Leegality",
         "Data Analyst",
         "4.0",
         "This is a remote position.\nCompany Mission\n\n\nLeegality is India’s first Document Infrastructure Platform - a radical new digital way for businesses to complete paperwork (agreements, forms and other legal documents). Over the last 7 years, Leegality has changed the way 2000+ Businesses do their paperwork from large enterprises like HDFC, SBI Cards, Federal Bank, ICICI Lombard, Axis Finance, Tata Capital etc. to high-growth companies like Razorpay, Rupeek, Cars24, Dunzo etc.\n\nTo see our impact on customers click here https://www.leegality.com/case-studies\n\n\nCompany Environment\n\n\nLeegality has an Employee Net Promoter Score of 97 - the highest on xto10x’s eNPS Survey for Q1 2022. The highest among 60+ notable startups. This makes us, arguably, the most employee-loved startup in the country\n\n\nCreating a category-defining company - and changing the way businesses perform a critical function like paperwork - requires powerful marketing that resonates with people.\n\n\n\nRequirements\nResponsibilities:\n\nRespond promptly to data requests from diverse teams and stakeholders.\nAnalyze and interpret complex datasets to extract meaningful insights.\nDevelop and maintain interactive and visually appealing dashboards using Tableau Server.\n\nEnsure dashboards effectively communicate key performance indicators (KPIs) and trends.\n\nCollaborate closely with product, sales, tech, and customer success teams to understand their data needs.\n\nProvide analytical support to aid teams in making data driven decisions.\n\nPresent findings and insights to non-technical stakeholders in a clear and compelling manner.\n\nCommunicate data-driven recommendations to support strategic initiatives.\n\nEngage in CEO-level interactions, offering data insights that contribute to high-level decision-making.\n\nUtilize SQL for efficient data extraction, transformation, and analysis.\n\nEmploy advanced Excel/Google Sheets functionalities for data manipulation.\n\nBring a dynamic and adaptable mindset, leveraging any previous startup experience.\n\n\nTechnical Skills:\n\nTableau Server\n\nSQL\n\nAdvance Excel / Google Sheets.\n\n\n\nBenefits\nRecruitment Process:\n\nOn being shortlisted, you would be contacted for the interview process.\n\nWe further have 3 rounds of interviews.\n\nYour final CTC would be decided on the basis of your skills, experience and final assessment.\n\n\n\nApply directly through our career page: https://careers.leegality.com/jobs/Careers\n\nFor more information about us please visit our:\n\nOur Company and Culture: https://bit.ly/3Iqm5SB\n\nOur Website: www.leegality.com/\n\nOur LinkedIn Page: www.linkedin.com/company/leegality/\n\nJob Information\nIndustry\nIT Services\nRemote Job",
         "India",
         "539530.5",
         "/yr (est.)",
         "51 to 200 Employees",
         "--",
         "Company - Private",
         "Information Technology Support Services",
         "Information Technology",
         "Unknown / Non-Applicable",
         "4.9",
         "4.8",
         "5.0",
         "4.9",
         "4.9"
        ],
        [
         "108",
         "Cubera",
         "Data Engineer- Junior",
         "4.0",
         "Cubera is a data company revolutionizing AdTech and Big Data Analytics through data value share paradigm, where the users entrust their data to us. We have perfected the art of understanding, processing, extracting, and evaluating the data that is entrusted to us. We are a gateway for brands to increase their lead efficiency as the world moves towards Web 3.0.\nWhat are you going to do?\nDesign & Develop high performance and scalable solutions that meet the needs of our customers.\nClosely work with the Product Management, Architects and cross functional teams.\nBuild and deploy large-scale systems in Java/Python.\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nCreate data tools for analytics and data scientist team members that assist them in building and optimizing their algorithms.\nFollow best practices that can be adopted in Big Data stack.\nUse your engineering experience and technical skills to drive the features and mentor the engineers.\nJob Category: Development\nJob Type: Full Time\nJob Location: Bangalore\nShare your resume to: hr@cubera.co",
         "Bengaluru",
         "567450.0",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "2.5",
         "2.5",
         "2.5",
         "2.5",
         "3.0"
        ],
        [
         "113",
         "Cell Propulsion",
         "Data Entry Operator",
         "3.8",
         "Job Information\nNumber of Positions\n1\nIndustry\nAdministration\nWork Experience\n1-3 years\nSalary\n10 K+\nCity\nBangalore North\nState/Province\nKarnataka\nZip/Postal Code\n560024\nJob Description\nFor all responsibilities below, the incumbent will be Based at Reception, responsible for receiving visitors/vendors at the front desk by greeting welcoming and directing them appropriately.\n\n\nProvides support for local office administration.\n\n\nSupports financial administration including processing incoming invoices, office expenditures, providing detailed information and processing payments accordingly.\n\n\nTrack, process, and organize various documents, supplies, shipments, samples, invoices, transactions, and communications, as necessary.\n\n\nEnsure proper housekeeping of office premises and take corrective action steps when needed to rectify issues.\n\n\nSupports space planning efforts by coordinating office/desk assignments, obtaining necessary approval(s) and keeping data in system current at all times.\n\n\nDevelop and maintain strong relationships with service vendors and landlords to ensure efficient maintenance of building and grounds.\n\n\nSupports Facilities Management with project coordination for space changes, remodels and expansions as assigned.\n\n\nProvides support for local office administration including but not limited to Purchasing, HR, logistics, IT, and Finance; and\n\n\nMay be responsible for other projects and responsibilities as assigned.\n\n\n\n\nRequirements\nKnowledge and experience in computer systems including Microsoft Windows and Office operating systems.\n\n\nExcellent English verbal and written communication skills in dealing with stakeholders from diverse backgrounds.\n\n\nAble to handle manual duties and perform regular facilities inspections.\n\n\nMust be able to speak Kannada fluently. Basic Hindi is acceptable .\n\n\nFemales are preferred .\n\n\nNote: This position is not for Cell Propulsion. Hiring company is for our partner company - Antrix Corporation Limited located at Hebbal, Bengaluru.",
         "Bengaluru",
         "10000.0",
         "/mo (est.)",
         "1 to 50 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "3.7",
         "2.9",
         "3.3",
         "3.2",
         "3.2"
        ],
        [
         "122",
         "Parentune",
         "Data Transformer(Internship)",
         "4.0",
         "x\nData Transformer\nLocation: Gurgaon\nReporting to: Product Manager\nAbout the role\nWe are looking for a Data Transformer, who can think deep enough to solve the most complex problems, and wide enough to cover all use cases involved. The Associate Product Manager will work with Product Team to ideate, lead, own, and deliver unique, large scale, user-centric products, that delight the end user, while fulfilling the business goals.\n\nResponsibilities & Job Overview\nBrainstorm new product ideas and enhancements to the existing product by collating requests from stakeholders and customers.\nTalk to users and deep dive in data to identify problems and opportunities..\nCome up with hypothesis and design experiments.\nCreate functional and UI specifications, business cases and wireframes.\nAssist in developing product strategy and roadmap, by analysing & prioritize the requirements of users, stakeholders, and the business.\nWrite detailed product specs to communicate what is to be built.\nBig Data Player – provide clear picture of the funnel how it is behaving and ideate next steps.\nGet products and features built in close collaboration with the engineering teams.\nEvaluate new product opportunities for the company, keeping in mind both the customer’s requirements and the company's technology capabilities.\n\nRequirements\nMust be very much capable with data – SQL , python and excel.\nInsanely proactive, imaginative, and innovative.\nYou speak well, write even better.\nYou must display the ability to talkwith tech fluently.\nYou must understand how technology works.\n\nIMPORTANT\nThis is a paid internship position for a minimum period of 6 months. This position would be eligible for a PPO on the basis of performance, once the transformer program duration is successfully completed.\n\nApply Now",
         "Gurgaon",
         "539530.5",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "4.1",
         "3.8",
         "3.9",
         "3.6",
         "3.4"
        ],
        [
         "126",
         "adyog",
         "Data Scientist",
         "4.0",
         "Use data to creatively solve business challenges and uncover transformative opportunities.\nYou’re a problem solver, data expert, analyst, and communicator, who can create new algorithms from scratch. You have an advanced degree in a quantitative field, such as computer science, engineering, physics, statistics or applied mathematics, and have:\nfamiliarity with statistical and data mining techniques\nstrong knowledge of programming languages, with a focus on machine learning and advanced analytics (such as R, Python, and Scala)\nexperience working with large data sets and relational databases\na collaborative spirit and the ability to communicate complex ideas effectively to both colleagues and clients\nexcellent problem-solving skills and the ability to analyze issues, identify causes, and recommend solutions quickly\nSend in your resume data.alchemist@adyog.com",
         "Chennai",
         "632851.0",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Company - Public",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "5.0",
         "4.1",
         "4.1",
         "5.0",
         "4.5"
        ],
        [
         "129",
         "WovV Technologies",
         "Data Engineer (GIS, ArcGIS, ESRI)",
         "4.7",
         "Locations: WFH/Remote/ Chennai/ Mumbai / Vadodara\nJob Summary:\nWovV Technologies, a global business productivity SaaS company and recognized amongst Top 20 Global SaaS Startups is looking for Data Engineer (GIS, ArcGIS, ESRI) professionals at all levels. Permanent WFH/Remote opportunity.\nExperience: 3+ years\nSkills - GIS, ArcGIS, ESRI, Python, MS SQL, Talend, Cloud\nKey Roles and Responsibilities:\nIdeally, a Data Analyst, Data Scientist or Data Engineer who has specific experience in processing geospatial data, experience with GIS, ArcGIS, ESRI.\n3-month project duration initially, this could be extended.\nGood understanding and expertise in working with Geospatial data from GIS systems.\nAbility to analyze, transform and visualize geospatial data.\nExperience in Python & System Databases i,e. (MS SQL), Talend.\nDesign, develop, and maintain data pipelines for collecting, transforming, and loading data into various data stores.\nBuild and maintain data warehousing and data lake solutions.\nDevelop and deploy data models that support various business requirements.\nWrite efficient and scalable code in languages such as Python, Scala, or Java.\nLead the design of data solutions with quality, automation, and performance in mind.\nOwn the data pipelines feeding into the Data Platform ensuring they are reliable and scalable.\nEnsure data is available in a fit-for-purpose and timely manner for business and analytics consumption.\nWork closely with the Data Product Manager to support alignment of requirements and sources of data from line of business systems and other endpoints.\nCommunicate complex solutions in a clear and understandable way to both experts and non-experts.\nInteract with stakeholders and clients to understand their data requirements and provide solutions.\nRequirements:\nExperience with Geospatial data, GIS systems, ArcGIS, ESRI.\nProven track record of delivering large-scale data and analytical solutions in a cloud environment.\nHands-on experience with end-to-end data pipeline implementation on AWS, including data preparation, extraction, transformation & loading, normalization, aggregation, warehousing, data lakes, and data governance.\nExpertise in developing Data Warehouses.\nIn-depth understanding of modern data architecture such as Data Lake, Data Warehouse, Lakehouse, and Data Mesh.\nStrong knowledge of data architecture and data modeling practices, Cost-effective management of data pipelines.\nFamiliarity with CI/CD driven data pipeline and infrastructure (e.g. Bit Bucket).\nAgile delivery approach using Scrum and Kanban methodologies.\nSupporting QA and user acceptance testing processes.\nSelf-driven and constantly seeking opportunities to improve data processes.\nPerks and Benefits:\nWork from Home / Remote Working\nFlexibility in timings\n5 Days working\nTeam Outing\nPerformance Bonus\nEmployee reference bonus policy\nExciting career path with an exponentially growing company\nFun activities\nDeserving compensation\nTo Apply send your CV to careers@wovvtech.com",
         "Remote",
         "539530.5",
         "/yr (est.)",
         "Unknown",
         "--",
         "Company - Public",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "4.5",
         "4.5",
         "4.6",
         "4.6",
         "4.6"
        ],
        [
         "142",
         "Kudos India",
         "Data Analysts",
         "3.9",
         "Interpret and analyze data to identify patterns and correlations among the various data points.\nMapping and tracing data from system to system in order to establish data hierarchy and its use.\nFilter and clean data by reviewing reports, Audit issues to locate and correct data problems.\nTranslate the numbers into words; identify linkages/patterns hidden in the data.\nContribute to development of best practices for the identification, collection, consolidation, validation, tracking and reporting of data.\nOversee development of data definitions, helping to analyze data sets for data anomalies and/ or to verify data quality and content, and producing data quality metrics.\nWork with partners to help identify, analyze and resolve data errors and inconsistencies.\nUnderstand high-level database architectures and work with other data analysts and DBA to ensure reporting logics are consistent with proper information on platform.\nExcellent written and verbal communication skills and demonstrated ability to interact with all members of the organization including senior management.\nAble to multi-task with ease including a demonstrated track record of delivering multiple concurrent complex technical projects on time.\nStrong analytical and problem solving skills; excellent documentation and presentation skills.\nWork with build and component testing staff to ensure deliverables meet business/ technical requirements.\nDevelop use cases and occasionally execute test cases.",
         "India",
         "539530.5",
         "/yr (est.)",
         "Unknown",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "3.8",
         "3.3",
         "3.9",
         "3.9",
         "4.2"
        ],
        [
         "145",
         "Scienaptic Systems",
         "Data Scientist",
         "4.1",
         "Anywhere across India\nJob Type\nFull Time\n\nAbout the Role\nAs a Data Scientist, you will be responsible for ensuring the successful implementation and usage of Scienaptic’s platform across dozens of clients. You will be working with some of the best-in-class Coders, AI/ML Scientists and Business Analytics Consultants in an environment which will encourage you to contribute widely to functional and technological aspects without worrying about conventional job silos.\nRequirements\nDesign tools and monitor the credit performance on client’s portfolios and intervene as needed\nIdentify common risks and possible solutions across Scienaptic’s clients\nDevelop insights to inform strategy improvements and new product use cases\nBecome the resident SME for Scienaptic’s solution and optimal strategic use\nSkills\nAt least 2 years’ experience in a Credit Risk analytical role, ideally for a Fintech or financial institution focused on consumer or small business lending\nExpertise in credit risk management for revolving / non revolving products (e.g., lines of credit, term loans)\nPrior experience of working with at least one of the 3 major US credit bureaus\nStrong analytical skills to understand the business process needs and translate them to system requirement specifications\nProficiency with one or more database querying tools (SQL, R, Python)\nProficiency with analytical tools and visualization software (e.g., Excel, Tableau, MongoDB)\nAttention to detail and quality of the deliverables and a growth mindset\nAbout Us\nScienaptic is the world's leading AI-powered Credit Underwriting platform. Designed by seasoned Chief Risk Officers, its platform is creating industry-leading business impact in terms of lifts such as higher approvals (15-40%) and lower credit losses (10-25%). Last year alone, Scienaptic helped financial institutions evaluate 45 million credit applicants and approve over 15 million of them. Scienaptic's clients include Fortune 100 banks, credit unions, regional banks and FinTech's.",
         "India",
         "539530.5",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "4.1",
         "4.0",
         "4.2",
         "3.9",
         "4.0"
        ],
        [
         "149",
         "WovV Technologies",
         "Data Scientist",
         "4.7",
         "Skills: Python, SQL, Tableau, Power BI, Cloud (Azure, GCP, AWS), ML Algorithms\nKey Responsibilities:\nExperience in leveraging Python and SQL to conduct data science modelling, employing statistical and machine learning algorithms to derive actionable insights.\nResponsibilities will encompass:\nData Modeling: Developing and implementing sophisticated data science models to extract valuable insights from complex datasets.\nAlgorithm Implementation: Applying statistical and machine learning algorithms to solve business problems and enhance decision-making processes.\nVisualization: Utilizing visualization tools like Tableau and PowerBI to present findings in a compelling and understandable manner.\nCloud-Based Deployment: Deploying models on cloud platforms such as MS Azure, GCP, and AWS, optimizing their functionality and scalability.\nTechnical Expertise and Soft Skills:\nBE/ME/Btech/Mtech in Computer Science, Electronics and Communication or a related technical field.\nProficiency in Python, SQL, data science modeling, and statistical analysis.\nStrong grasp of ML algorithms and their implementation in real-world scenarios.\nExperience in visualization tools like Tableau and PowerBI, along with knowledge of deploying\nmodels on cloud platforms (MS Azure, GCP, AWS).\nProven problem-solving skills and a solution-oriented mindset.\nExcellent communication skills to collaborate effectively within teams and with stakeholders.\nStrong business acumen with an interest in business-facing roles.\nAdaptability and a start-up mentality to thrive in a dynamic environment.\nJob Types: Full-time, Permanent\nSalary: Up to ₹1,000,000.00 per year\nBenefits:\nProvident Fund\nSchedule:\nDay shift\nMonday to Friday\nMorning shift\nSupplemental pay types:\nPerformance bonus\nYearly bonus\nWork Location: In person",
         "Pune",
         "1000000.0",
         "/yr (est.)",
         "Unknown",
         "--",
         "Company - Public",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "4.5",
         "4.5",
         "4.6",
         "4.6",
         "4.6"
        ],
        [
         "160",
         "WSD Consultant",
         "Junior Data Scientist/",
         "5.0",
         "Junior Data Scientist/\nBangalore, Mumbai, Pune, Chennai, Noida, Hyderabad\nShare\n\n\ncandidate should qualify checklist written below:\n\nQues: Does the candidate has strong knowledge in Data Science/ Machine Learning/ Artificial Intelligence ? Please mention\n\nQues: Is the candidate willing to work full time as a Data Science program Manager/ Academic and content delivery/ End to End Curriculam delivery and management ?\n\nQues: Does the candidate has Data Science/ Machine learning content development and academic delivery experience/ Faculty Development programs in any Teaching Institution or E - Learning Companies?\n\n\nJob Specifications\n\nExp. 1.0 - 5.0 Year(s)\nAnnual Fixed CTC Min : 7.0 Lacs Max: 15.0 Lacs\nQualification B.Tech/B.E. , Any Post Graduation\nNo of openings 3\nAdditional Doc/Msg\nExperience\n1 - 20 Years\n\nSalary\n4 Lac To 32 Lac 50 Thousand P.A.\n\nIndustry\nIT Software - Application Programming / Maintenance\n\nKey Skills\nData Scientist\n\n\nAbout Company\nCompany Name\nGreat Learning\n\n\nAbout Company\nGreat Learning is an online and hybrid learning company that offers high-quality, impactful, and industry-relevant learning programs to working professionals. These programs help them master data-driven decision-making regardless of the sector or function they work in and secure their career growth into the future. These programs are delivered through a convenient and robust technology-enabled experience with no disruption to their careers.\n\nContact Person\nMiss Anjali Singh\n\nAddress\nGurgaon\n\nMobile\n9540042962\n\nEmail ID\ninfohrwsd@gmail.com",
         "Bengaluru",
         "579655.0",
         "/yr (est.)",
         "Unknown",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "5.0",
         "3.0",
         "4.0",
         "5.0",
         "4.0"
        ],
        [
         "162",
         "Infimetrics",
         "Data Scientist (3+years)",
         "4.0",
         "Job Details\n\nJob Title : Data Scientist (3+years)\n\nExperiance : 3+ years of experience.\n\nLocation : REMOTE\n\nShort Description : Python With Data Science Linear Algebra and Advanced Statistics Machine Learning Performance Metrics Data Analysis With MS-Excel Exp with Hive, Spark, etc (DS/ DA tool)\n\nTechnical Skills : Data Analysis With MS-Excel Exp with Hive, Spark, etc (DS/ DA tool)",
         "Remote",
         "539530.5",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "3.0",
         "3.0",
         "5.0",
         "4.0",
         "5.0"
        ],
        [
         "169",
         "Ventra Health, Inc.",
         "Data Engineer",
         "3.3",
         "Job Summary:\nThe Data Engineer works independently, and collaboratively, to design and implement complex Data & Analytics solutions.\nThe individual in this position interfaces with various functional teams to support the solution delivery. They proactively identify needs or issues, develop strategies, and propose/implement technical solutions. The Data Engineer will grow to become a subject matter expert.\nEssential Functions and Tasks:\nResponsible for designing and implementing Analytical solutions using the Microsoft BI Toolkit (SQL, SSIS, SSAS) to enable the analysis of data to support strategic initiatives and ongoing business requirements.\nThis position is expected to have a proactive approach and create the best solution to address business needs and current infrastructure. Care will be given to provide accurate data to fulfill the requirements of the developed solutions.\nExtract, transform and load company data into the Enterprise Data Warehouse.\nPerform Unit, Integration and Functional testing.\nActively seeks opportunities to expand technical knowledge and capabilities.\nWork with the Development team to establish the best patterns, practices, and standards as new technology arises.\nParticipate in the out of hours support process.\nKnowledge, Skills, and Abilities:\nProven experience as a Data Engineer with in-depth knowledge of SQL Integration Services (SSIS) and expert level SQL programming skills for data manipulation, query optimization, and performance tuning.\nIdeal candidates will have hands-on experience with the full life cycle of Data warehouse design and development including logical and physical data modeling, Kimbell design methodologies, architecture techniques, including ODS, DM, and EDW.\nExcellent problem solving and analytical skills, with keen attention to detail.\nProven ability to manage multiple tasks and priorities efficiently.\nAbility to work effectively in cross-functional teams, collaborating with Product Owners, QA Teams, and business stakeholders.\nExcellent communication skills\nAbility to be flexible and work under high pressure in a complex environment.\nHealthCare/RCM/Financial systems is a major plus.\nExperience with large Health care EMRs is a plus.\nUnderstand and comply with company policies and procedures.\nIND2",
         "India",
         "539530.5",
         "/yr (est.)",
         "1001 to 5000 Employees",
         "--",
         "Company - Private",
         "Healthcare Services & Hospitals",
         "Healthcare",
         "Unknown / Non-Applicable",
         "3.2",
         "3.0",
         "3.2",
         "3.0",
         "3.5"
        ],
        [
         "180",
         "BDIPlus",
         "Data Engineer ||",
         "4.1",
         "Job Role: Data Engineer II\nAbout BDIPlus:\nBDIPlus is a dynamic Strategy Consulting and Software firm dedicated to delivering cutting-edge capabilities and solutions that foster the development of enduring competitive advantages. Our innovative solutions showcase our unparalleled proficiency in technology and our profound domain expertise within the Financial Services and Insurance sectors. By synergizing our unmatched technical skills with a comprehensive grasp of each client’s institutional landscape and distinctive areas for improvement, we empower them to convert data into actionable and well-organized information. This facilitates precise decision-making, increased efficiency and rampant business growth.\nPosition Overview:\nAs a Data Engineer, you will be tasked with crafting, constructing and sustaining the framework necessary for seamless data collection, storage, analysis and the construction of predictive models. Your responsibilities encompass the creation and oversight of databases, the development of efficient data pipelines, and the assurance of the overall reliability and effectiveness of data systems. Essentially, your role involves establishing the foundation that enables organizations to extract valuable insights and make well-informed decisions grounded in data.\nResponsibilities:\nDesign, implement, and optimize scalable data pipelines using Scala and Spark, ensuring efficient big data processing and management.\nUtilize Hive for creating and maintaining data warehouses, facilitating seamless access to structured and semi-structured data.\nLeverage one year of ML model building experience to integrate machine learning solutions into data processing workflows, enhancing data-driven decision-making capabilities.\nCollaborate with cross-functional teams to understand business requirements and translate them into effective data engineering solutions.\nEnsure the reliability, performance, and security of data infrastructure, troubleshooting and resolving issues to maintain a robust data ecosystem.\nRequirements:\nProficient in Java, or Scala for scalable data processing.\nProven 2+ years of Related Field.\nExtensive experience with big data tech (e.g., Apache Spark) and database design (Hive).\nMinimum one year ML model building experience.\nCloud platform expertise (AWS, Azure, GCP) for deploying data solutions.\nCollaborative mindset to work with cross-functional teams.\nStrong problem-solving skills and proactive approach to data issues.\nExcellent communication skills.\nBachelor's degree in relevant field.\nTechnical Skills:\nProgramming skills with experience in Java/ Scala (Must).\nBig Data (Spark) is mandatory\nStrong Experience with Query Language like SQL\nHand-on experience with Machine Learning which includes Supervised Learning: regression, Binary Classification Models (Logistic Regression, Random Forest, XGBoost, SVM, LightGBM etc) and Unsupervised learning: Segmentation (Cluster analysis, Recommender systems etc.)\nExperience in working with large volumes of data.\nData ecosystems - Hadoop, Hive, Python is a plus\nAWS/AZURE\nData modelling and evaluation\nBenefits:\nMedical insurance\nJoin our team and contribute to the development of innovative solutions that make a difference. We offer competitive compensation, a collaborative work environment, and opportunities for professional growth. Kindly share your CV at Priyanka.singh@bdiplus.com\nKindly fill this form its Mandatory: https://forms.office.com/Pages/ResponsePage.aspx?id=p2Sznb86AUWp9dWYX5lFz4alBrukXiRFlL85GBWLzOdURExMVlhRSkcyQThJQk9DM0s0WlJLQVJFVS4u\nJob Types: Full-time, Permanent\nPay: ₹550,171.72 - ₹1,500,000.00 per year\nBenefits:\nHealth insurance\nPaid sick time\nPaid time off\nProvident Fund\nSchedule:\nDay shift\nMonday to Friday\nSupplemental pay types:\nYearly bonus\nExperience:\ntotal work: 2 years (Preferred)\nAbility to Commute:\nBangalore, Karnataka (Required)\nAbility to Relocate:\nBangalore, Karnataka: Relocate before starting work (Required)\nWork Location: In person\nSpeak with the employer\n+91 9643459417",
         "Bengaluru",
         "1025086.0",
         "/yr (est.)",
         "Unknown",
         "--",
         "Company - Private",
         "Information Technology Support Services",
         "Information Technology",
         "Unknown / Non-Applicable",
         "4.1",
         "4.0",
         "4.0",
         "3.8",
         "3.5"
        ],
        [
         "183",
         "Sea and Beyond",
         "Data Entry",
         "5.0",
         "FULL TIME\nData Entry\nYour Profile will be kept confidential\nMumbai\nExperience : 0-1 years\nJob ID : MIS/23/12\nSalary : As per market standards\n\nDear Applicants,\n\nGreetings!\n\nWe are hiring Fresh Graduates for a leading Shipping Company.\n\nLooking for:\nCommerce / BMS / Engineering graduate\nGood communication skills\n\nWarm Regards,\nSea & Beyond",
         "Mumbai",
         "201246.0",
         "/yr (est.)",
         "Unknown",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "5.0",
         "5.0",
         "5.0",
         "5.0",
         "5.0"
        ],
        [
         "185",
         "UnitedHealthcare",
         "Data Analyst 2",
         "3.6",
         "Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together.\nPrimary Responsibilities:\nAnticipate customer needs and proactively develop solutions to meet them\nServe as a key resource on complex and / or critical issues\nSolve complex problems and develop innovative solutions\nProvide explanations and information to others on the most complex issues\nCreate Benchmark ETL and reporting - standard and ad hoc needs\nDevelop, build, and maintain reporting of key metrics\nConsult and document business requirements to support reporting needs\nUnderstand basic relational data structures\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\nRequired Qualifications:\n7+ years of experience writing T-SQL queries, stored procedures, and complex scalar-value functions\n6+ years of experience using SSIS/ADF tools for ETL solutions\n6+ years of experience gathering and documenting requirements\n6+ years of experience with process improvement, workflow, benchmarking, and/or evaluation of business processes\nWorking experience on Databricks, Kafka, Python and ML libraries\nExperience on CI/CD implementation\nExperience on SSIS to ADF migration\nAny source control tool experience like Github/TFS\nAzure cloud server knowledge\nKnowledge on Query performance tuning and optimization\nKnowledge on consuming stream data from hub/snowflake server to on-prem\nProven solid statistics and analytical knowledge\nProven solid written and verbal communication skills\nPreferred Qualifications:\nExperience building dashboard using Power BI\nKnowledge on Microsoft azure cloud environment\nKnowledge on U.S healthcare domain\nAt UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.",
         "Bengaluru",
         "663325.0",
         "/yr (est.)",
         "10000+ Employees",
         "--",
         "Company - Public",
         "Healthcare Services & Hospitals",
         "Healthcare",
         "Unknown / Non-Applicable",
         "3.6",
         "3.5",
         "3.7",
         "3.3",
         "3.6"
        ],
        [
         "187",
         "BridgeLoyalty",
         "Data Analyst Job",
         "4.0",
         "Job Description:\nSenior data analyst, responsible for designing data collection, storage, and retrieval system and processes.\nHe will be responsible for ensuring data collection, transmission, and accuracy of data storage under each line of business / geographical location wise/TL and respondent wise.\nHe will be responsible for Coordinating with Quality Check/TL and Vertical Leads to do data cleaning, Consolidating data, and arranging them in organised manner to convert data into meaningful information and further into business insights.\nPreparing Region wise/City wise/Respondent wise/Demography wise/Product wise analytics\nIdentifying trends if any, patterns or anomalies and Preparing data for creating Charts and presentations.\nCompletion of the project and aligned work in timeline manner.\nExperience: Working knowledge on SPSS, Tableau, MS Excel, PowerPoint\nQualification: Graduate\nLocation: Mumbai\nContact Person: Krishna Kumar\nEmail id: Careers@bridgeloyalty.com",
         "Mumbai",
         "256905.0",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0"
        ],
        [
         "189",
         "Varuna Group",
         "Data Entry Operator",
         "4.0",
         "Group Company: Varuna Warehousing Pvt. Ltd\nDesignation: Data Entry Operator (VW0020)\n\nOffice Location: RB-Sitarganj Sitarganj (Warehouse)\nPosition description:\nAccuracy in data entry\nSAP exposure\nAccuracy in MIS report\nGood in E- Mail communication\nE Way bill update\nVerification of Documents,(cross check Pick list, Invoice/STN and LR copy)\nPrimary Responsibilities:\nData Entry\nEducational qualifications preferred\nCategory: Bachelor's Degree\nAcademic score: 50 %\nKey Performance Indicators:\n100% up to date\nOntime & accurate\nRequired Competencies:\nTyping Speed with accuracy\nExcel\nRequired Knowledge:\nExcel\nSAP\n\nRequired Skills:\nGood behavior\nHonesty\nHard work\nDedication\n\nFunctional Designation Data Entry Operator\nDepartment Operations\nExperience 0 - 4\nLocation Raisen\nCreated 23 Dec, 2023",
         "Raisen",
         "84853.0",
         "/yr (est.)",
         "1001 to 5000 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "2.9",
         "2.7",
         "3.2",
         "3.1",
         "2.5"
        ],
        [
         "191",
         "Ideal Education",
         "Data Caller",
         "4.0",
         "Working at Ideal Education\nWorking at Ideal Education is fulfilling as well as rewarding. And, while we dwell on contributing our best to take our institution further, we also are keen on developing measures to improve consistently.\nOur work allows encouragement for creative ideas and methods to tackle the everyday operations. Along with extra-curricular activities over the annual years that never make work seem like work.\nWe believe that in doing good work lies the highest satisfaction. We are the only coaching classes offering a Training Module that gives our staff and faculty the extra edge to do well in any given circumstance. Hence, the exemplary work satisfaction. With adequate opportunities to align your personal goals with organisation's progress, Ideal Education is always eager to meet people with newer perspectives that will help us offer more better services.\nIf you are compelled, we will be delighted to hear from you.\n\nData Caller\nAs a Data Caller, your job is obtaining and updating lists of individuals' contact details. Calling parents and teachers and informing them with the updates. Addressing grievances, complains with respect and noting important details of each conversation.",
         "India",
         "539530.5",
         "/yr (est.)",
         "501 to 1000 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "5.0",
         "4.0",
         "5.0",
         "5.0",
         "5.0"
        ],
        [
         "193",
         "SmartHelio",
         "Data Scientist",
         "4.0",
         "Join our team as an experienced data scientist with a passion for tackling complex challenges in clean tech and climate science. If you’re also enthusiastic about making a significant impact on the energy transition and wish to be part of a pioneering startup, we would like to have you onboard.\n\nLocation: New Delhi, India",
         "Delhi",
         "539530.5",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Unknown",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "4.0",
         "3.5",
         "4.5",
         "5.0",
         "3.5"
        ],
        [
         "195",
         "InfoKrafts",
         "Data Engineer",
         "5.0",
         "This is a hands-on engineering role with a focus on business and digital transformation. It involves managing and maintaining the Data Architecture and solutions, with operational support and troubleshooting.",
         "Mumbai",
         "591608.0",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "5.0",
         "5.0",
         "4.0",
         "5.0",
         "4.0"
        ],
        [
         "200",
         "Delivery Centric Technologies",
         "Python & Data Modelling",
         "4.0",
         "Job Title: Python & Data Modelling\nLocation: Bangalore\nEmployment Type: Permanent\nResponsibilities:\nData Modeling: Design and implement efficient data models to support various business requirements using Python.\nPython Development: Utilize your expertise in Python programming to create scalable and maintainable code for data processing and manipulation.\nPandas Mastery: Demonstrate proficiency in Pandas library to analyze, clean, and transform large datasets with precision.\nPyData Ecosystem: Leverage the PyData ecosystem tools and libraries to enhance data analysis and visualization capabilities.\nCollaboration: Work closely with cross-functional teams, including data scientists, analysts, and engineers, to understand requirements and deliver effective data solutions.\nRequirements:\nPython Proficiency: Strong hands-on experience with Python programming language and its ecosystem.\nData Modeling Skills: Proven expertise in designing and implementing effective data models for diverse business needs.\nPandas Expertise: In-depth knowledge of Pandas library for data manipulation and analysis.\nPyData Familiarity: Experience working with various PyData tools and libraries for efficient data processing.\nProblem-Solving Skills: Ability to analyze complex problems and develop creative and effective solutions.\nCommunication Skills: Excellent communication skills to collaborate with team members and present findings to non-technical stakeholders.\nAttention to Detail: A keen eye for detail and a commitment to delivering high-quality, accurate results.\nAbout Us:\nWe are Delivery Centric, a dynamic technology company that is transforming the delivery of cloud implementations and Cyber security. We are headquartered in Australia with a global presence. We are tenacious, future-thinking, and highly driven to achieve an ambitious vision to be the leading global provider of innovative technologies for companies that keep our world clean, safe, and secure.\nOur fast-paced and supportive environment will offer you lots of benefits, including a competitive salary, a friendly team, a healthy work environment, and opportunities for training and development.\nWhy work with us?\nWork with diverse technologies, projects, clients and industries.\nSpend your days in a creative and innovative environment.\nShare experience and knowledge as part of an agile and dynamic team\nJob Type: Full-time\nSchedule:\nDay shift\nWork Location: In person",
         "Bengaluru",
         "416655.0",
         "/yr (est.)",
         "51 to 200 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "3.8",
         "3.7",
         "3.6",
         "3.7",
         "4.1"
        ],
        [
         "213",
         "Victoria Realtors",
         "Data Analyst",
         "3.3",
         "· Understand the data points present in all the departments of the organisation.\n· Closely work with the sales department to understand the effectiveness in lead distribution, effectiveness in sales calling, site visit data analysis etc.\n· Closely work with the product development team to develop the best Product market fit.\n· Create and maintain data visualization dashboards, reports, and presentations to communicate findings to internal stakeholders\n· Collaborate with cross-functional teams, including sales, marketing, and finance, to provide data-driven insights and recommendations\n· Identify and implement process improvements to enhance data quality, accuracy, and accessibility\nREQUIREMENTS\n· Bachelor's degree in statistics, mathematics, economics, computer science, or a related field\n· Proven experience in data analysis, preferably in the real estate industry or a related field\n· Proficiency in statistical software (e.g., R, Python), data visualization tools (e.g., Tableau, Power BI), and database querying languages (e.g., SQL)\n· Excellent communication and presentation abilities, with the capacity to convey technical findings to non-technical stakeholders\n· Ability to work effectively in a fast-paced, collaborative environment and manage multiple priorities simultaneously.\n· Natives preferred\n· Job Location : Coimbatore.\nJob Type: Full-time\nSalary: ₹300,000.00 - ₹400,000.00 per year\nSchedule:\nMorning shift\nExperience:\ntotal work: 2 years (Preferred)\nWork Location: In person\nSpeak with the employer\n+91 8089032282",
         "India",
         "350000.0",
         "/yr (est.)",
         "1 to 50 Employees",
         "--",
         "Company - Private",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "3.7",
         "3.7",
         "2.6",
         "3.3",
         "2.3"
        ],
        [
         "220",
         "Toolyt",
         "Data Analyst",
         "4.8",
         "We are looking for a passionate certified Data Analyst. The successful candidate will turn data into information, information into insight and insight into business decisions.\n\nData analyst responsibilities include conducting full lifecycle analysis to include requirements, activities and design. Data analysts will develop analysis and reporting capabilities. They will also monitor performance and quality control plans to identify improvements.\n\nResponsibilities:\nInterpret data, analyze results using statistical techniques and provide ongoing reports\nDevelop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality\nAcquire data from primary or secondary data sources and maintain databases/data systems\nIdentify, analyze, and interpret trends or patterns in complex data sets\nFilter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems\nWork with management to prioritize business and information needs\nLocate and define new process improvement opportunities\nRequirements:\nProven working experience as a data analyst\nBachelors degree in Computer science, Maths or statistics\nAnalyze and organize raw data\nBuild data systems and pipelines\nEvaluate business needs and objectives\nInterpret trends and patterns\nConduct complex data analysis and report on results\nHands on experience with SQL database design (advanced)",
         "Jaipur",
         "40000.0",
         "/mo (est.)",
         "1 to 50 Employees",
         "--",
         "Company - Private",
         "Enterprise Software & Network Solutions",
         "Information Technology",
         "Unknown / Non-Applicable",
         "4.9",
         "4.6",
         "4.9",
         "4.9",
         "4.6"
        ],
        [
         "230",
         "4CRisk.ai Software",
         "Data Quality Analyst",
         "4.0",
         "What is 4CRisk.ai\n\n4CRisk is an AI start-up uniquely positioned to identify and solve the annual $300 billion Risk and Compliance problem for banks, non-bank financial institutions and FinTech companies. Our mission is to help our customers protect brand value and strengthen revenues by reducing risk and cost of compliance. At 4CRisk, technology, data, UI, and products have all been conceived with a customer-centric approach. We believe ‘Culture trumps aptitude’.\nOur Engineering center (4CRisk.ai Software Private Ltd.) in Bangalore, India is looking for bright and passionate candidates who share our vision and wish to be part of a team of talented professionals.\nThe Role & Requirements\nWe are looking for a Data quality Analyst to further use the regulatory data to drive product decisions. Working with cross-functional teams composed of product managers, designers, and engineers, you’ll apply your expertise to deliver customer insights and help shape the products we deliver to our customers. You will leverage rich user data using cutting-edge technology to see your insights turned into real products.\nKey Responsibilities\nPerforming statistical tests on large datasets to determine data quality and integrity.\nEvaluating system performance and design, as well as its effect on data quality.\nCollaborating with AI and Data Engineers to improve data collection and storage processes.\nRunning data queries to identify quality issues and data exceptions, as well as cleaning data.\nGathering data from primary or secondary data sources to identify and interpret trends.\nReporting data analysis findings to management to inform business decisions and prioritize information system needs.\nDocumenting processes and maintaining data records.\nAdhering to best practices in data analysis and collection.\nKeeping abreast of developments and trends in data quality analysis.\nRequired Experience/Skills\nData Quality analysis experience is a must (root-cause analysis, data slicing )\nDesigning, building and executing data quality plans for complex data management solutions built on modern data processing frameworks.\nUnderstanding the data lineage and preparing validation cases to verify the data during every stage of the data processing journey.\nPlanning, designing and conducting validations of the data related implementations to achieve an acceptable result.\nDevelop datasets creation scripts for verification of data at extraction, transformation and loading phases by verifying the data mapping & transformation rules.\nSupport AI and Product Management teams by contributing to the development of the data validation strategy focusing on building the regression suite.\nDocument issues and communicate with data engineers to resolve issues and ensure quality standards.\nConsistent and efficient approach to capturing business requirements and translating them into functional, non-functional, and semantic specifications\nData Profiling, Data Modeling and Data Validation Testing experience a plus\n1 to 3+ years of proven experience.\nExcellent presentation, communication (oral & written) in English, and relationship building skills, across all levels of management and customer interaction\nAbility to work with team members across the globe and across departments.\nLocation:\nBangalore, India\nWe are a start-up of innovators, risk-takers, and trailblazers who celebrate our differences, and know that our unique perspectives make us stronger, smarter, and well-positioned for success. We value and rely on the collective voices of our employees, customers, and community to help guide us as we build a better the company together. Every voice, every perspective matter. That is why we’re proud to be an equal opportunity employer. We do not discriminate based on race, colour, ethnicity, ancestry, religion, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, veteran status, or genetic information.",
         "Bengaluru",
         "539530.5",
         "/yr (est.)",
         "51 to 200 Employees",
         "--",
         "Contract",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "4.3",
         "4.3",
         "4.3",
         "4.8",
         "3.8"
        ],
        [
         "232",
         "Konceive Development Center Pvt. Ltd.",
         "Data Analyst",
         "4.6",
         "Job Description:\nResponsibilities:\nCollect, clean, and analyze data from diverse sources.\nIdentify trends and patterns for strategic decision-making.\nDevelop and present clear visualizations and reports.\nCollaborate with cross-functional teams to meet business objectives.\nUtilize statistical models for forecasting.\nQualifications:\nBachelor’s degree in a relevant field.\nProficient in SQL, Python, or R.\nExperience with data visualization tools (Tableau, Power BI).\nStrong analytical and problem-solving skills.\nExcellent communication skills.\nExperience:\n2+ years\nCertifications:\nDesirable certifications, such as CAP (Certified Analytics Professional) or Microsoft Certified, are a significant advantage.\nJob Types: Full-time, Permanent\nSalary: ₹20,000.00 - ₹50,000.00 per month\nBenefits:\nHealth insurance\nProvident Fund\nSchedule:\nDay shift\nFixed shift\nMonday to Friday\nMorning shift\nWork Location: In person",
         "Panchkula",
         "35000.0",
         "/mo (est.)",
         "1 to 50 Employees",
         "--",
         "Company - Private",
         "Advertising & Public Relations",
         "Media & Communication",
         "Unknown / Non-Applicable",
         "4.5",
         "4.6",
         "4.5",
         "4.5",
         "4.7"
        ],
        [
         "233",
         "InitiateFirst.",
         "Data Scientist",
         "4.7",
         "Requirements:\nExperience: 5+ Years\nSkill Set: Data Science (Python, ML, Cloud)\nExperience with Python programming language\nExperience in machine learning model development\nShould have experience in any Cloud Technology.\nQualification:\nAny Graduate.\nJob Category: Software Engineer\nJob Location: Bangalore\nJob Country: India",
         "Bengaluru",
         "579655.0",
         "/yr (est.)",
         "51 to 200 Employees",
         "--",
         "Self-employed",
         "--",
         "--",
         "Unknown / Non-Applicable",
         "4.5",
         "4.5",
         "4.5",
         "4.5",
         "5.0"
        ],
        [
         "238",
         "Syneos Health Clinical",
         "Senior Clinical Data Associate",
         "3.8",
         "Description\nSenior Clinical Data Associate\nSyneos Health® is a leading fully integrated biopharmaceutical solutions organization built to accelerate customer success. We translate unique clinical, medical affairs and commercial insights into outcomes to address modern market realities.\nOur Clinical Development model brings the customer and the patient to the center of everything that we do. We are continuously looking for ways to simplify and streamline our work to not only make Syneos Health easier to work with, but to make us easier to work for.\nWhether you join us in a Functional Service Provider partnership or a Full-Service environment, you’ll collaborate with passionate problem solvers, innovating as a team to help our customers achieve their goals. We are agile and driven to accelerate the delivery of therapies, because we are passionate to change lives.\nDiscover what our 29,000 employees, across 110 countries already know:\nWORK HERE MATTERS EVERYWHERE\nWhy Syneos Health\nWe are passionate about developing our people, through career development and progression; supportive and engaged line management; technical and therapeutic area training; peer recognition and total rewards program.\nWe are committed to our Total Self culture – where you can authentically be yourself. Our Total Self culture is what unites us globally, and we are dedicated to taking care of our people.\nWe are continuously building the company we all want to work for and our customers want to work with. Why? Because when we bring together diversity of thoughts, backgrounds, cultures, and perspectives – we’re able to create a place where everyone feels like they belong.\nJob responsibilities\nMaintains awareness of the pertinent elements of contract and scope of work for assigned project(s) and communicates status updates to the Project Manager (PM) and/or Biometrics Project Manager (BPM) as necessary.\nReviews and adheres to the requirements of study-specific DMP for assigned project(s) and updates as required.\nCreates and enters test data for User Acceptance Testing (UAT)\nPerforms User Acceptance Testing (UAT) for data entry screens, edits and data review listings, all different roles used in the study and Targeted Source Data Verification (SDV) configuration and matrices.\nReceives and enters lab normal ranges.\nCompletes and submits Clinical Database Management System (CDMS)-specific access forms and/or spreadsheets.\nPerforms reviews of discrepancy (edit check) output and validation listings based on data entered into the clinical database. Based on this review, queries or applies self-evident corrections or other global rulings permitted in cases where queries are not required, per the DVS and/or Data Management Plan (DMP) for the assigned projects. Resolves answered queries and re-queries where appropriate.\nFor paper studies, takes receipt of, and reviews, Data Clarification Forms (DCFs) that have been answered by sites and where appropriate, edits the CRF database accordingly.\nFor paper studies, performs internal QC checks via listing output from database against CRFs and DCFs. Serves as QC Coordinator for paper studies.\nFor paper studies, ensures all CRFs and DCFs received are returned for filing in the Document Control Room per the Data Tracking Guidelines for the assigned projects.\nFor EDC studies, performs DM quality review and/or other internal QC checks as required per applicable Electronic Data Capture (EDC) systems.\nParticipates/Leads in internal meetings.\nAttends in-process review meetings.\nParticipates in internal/external audits as required.\nFiles documentation in the Data Management Study File (DMSF).\nMaintains proficiency in Data Management systems and processes through regular trainings (CDA Knowledge College)\nPerforms SAS mapping QC whereby discrepancies are noted on the SAS mapping test logs.\nOversees the work of other CDAs as required. This may involve on-the-job training, review of work, as well as ensuring the quality of work performed.\nEnsures that data from external databases/datasets such as central and/or local laboratory data, electronic diary data, pharmacokinetic (PK) data, or Interactive Response Technology (IRT) are consistent with data in the clinical database. Uses the specified process to document and query any such discrepancies found with the appropriate party.\nCompletes tasks within timeframe by appropriately prioritizing multiple tasks within or across projects and adapts to timeline or priority changes by reorganizing daily workload. Proactively communicates to project team and management accurate estimates on time to complete tasks, availability to take on new assignments and resourcing conflicts. Minimizes rework by following study instructions, seeking understanding of assignments prior to performing task and anticipating the effect changes may have on data when issuing and resolving queries.\nRuns data cleaning and/or status reports.\nPerforms Serious Adverse Event (SAE) reconciliations.\nLiaises with other groups such as Clinical Programming (CP), Biostatistics and Clinical Operations.\nCreates ad-hoc data cleaning reports used to determine if a validated listing is required including creation of the specification for the validated listing (updates DVS with the listing requirements).\nPerform post-migration testing on screens, edit checks, matrices and role changes as required.\nParticipates in customer and third party meetings distributing relevant information in advance, ensures minutes are promptly and accurately distributed to internal team for review and subsequent edits are applied in order to maintain established currency for sponsor distribution.\nReviews database design specifications (including configuration, data structures, annotated CRFs).\nDesigns and/or reviews CRF/eCRF including eCRF visit structure co-coordinating with team members responsible for the associated database design.\nProvides input into the Data Validation Specification (DVS) including creation of edit checks for assigned forms including any post-production updates to the DVS and listings under guidance.\nProvides support to the PDM on a study as required. May involve soliciting support from and coordinating with other CDAs, taking full responsibility for an aspect of the study delivery, or producing study metrics reports. PDM’s back up for specific activities (including attend sponsor’s meetings to provide updates).\nCreation of Discoverer, BOXI, J-Review Reports\nCreates, updates and reviews study-specific documents such as CRF/eCRF DMP, data import/export agreements, CRF/eCRF Completion Guidelines.\nFills-in the Data Transfer Request Form required for delivering the data to sponsor or vendor.\nReviews queries and self-evident corrections proposed by less experienced DM staff.\nUnderstands the coding process\nUnderstands the purpose of interim, dry run, data cut\nTrains and mentors DM staff providing timely feedback to trainee and management as appropriate.\nTrains project team in project specific requirements.\nProvides EDC training to internal and external team members via Teleconference.\nServes as a platform or process-specific Subject Matter Expert (SME).\nPerforms QC of DMSF after QuickStart® Camp (QSC) and ongoing during the study conduct.\nMay be required to participate in client, internal or agency audits and inspections.\nMay represent the department at business development related meetings.\nQualifications\nWhat we’re looking for\nBA/BS degree in the biological sciences or related disciplines in the natural science/health care field.\nExperience with DM practices and relational database management software systems preferred.\nOracle Clinical, Rave, or Inform systems preferred. Knowledge of clinical data, and ICH/Good Clinical Practices. Knowledge of medical terminology preferred.\nProficiency in navigating MS Windows, as well as use of MS Word, Excel, and email applications. Excellent speed and accuracy of keyboard skills.\nWork experience in clinical research, drug development, data management, or other healthcare environment preferred.\nFamiliarity with medical terminology\nExcellent communication, presentation, interpersonal skills, both written and spoken, with an ability to inform, influence, convince, and persuade\nGood organizational, planning, and time management skills preferred. Ability to multitask under tight deadlines while providing attention to detail. Ability to be flexible and adapt to change. Ability to work independently as well as part of a multi-disciplinary team.\nAbility to perform a leadership functions in DM including effective mentoring skills, and the ability to deal effectively with sponsors and internal customers.\nResponsible for performing activities that are in compliance with applicable Corporate Business Practices, Standard Operating Procedures and Working Instructions and performing other duties as assigned by management. Minimal travel may be required (up to 25%).\nGet to know Syneos Health\nOver the past 5 years, we have worked with 94% of all Novel FDA Approved Drugs, 95% of EMA Authorized Products and over 200 Studies across 73,000 Sites and 675,000+ Trial patients.\nNo matter what your role is, you’ll take the initiative and challenge the status quo with us in a highly competitive and ever-changing environment. Learn more about Syneos Health.\nAdditional Information:\nTasks, duties, and responsibilities as listed in this job description are not exhaustive. The Company, at its sole discretion and with no prior notice, may assign other tasks, duties, and job responsibilities. Equivalent experience, skills, and/or education will also be considered so qualifications of incumbents may differ from those listed in the Job Description. The Company, at its sole discretion, will determine what constitutes as equivalent to the qualifications described above. Further, nothing contained herein should be construed to create an employment contract. Occasionally, required skills/experiences for jobs are expressed in brief terms. Any language contained herein is intended to fully comply with all obligations imposed by the legislation of each country in which it operates, including the implementation of the EU Equality Directive, in relation to the recruitment and employment of its employees. The Company is committed to compliance with the Americans with Disabilities Act, including the provision of reasonable accommodations, when appropriate, to assist employees or applicants to perform the essential functions of the job.",
         "Remote",
         "539530.5",
         "/yr (est.)",
         "10000+ Employees",
         "--",
         "Company - Private",
         "Biotech & Pharmaceuticals",
         "Pharmaceutical & Biotechnology",
         "Unknown / Non-Applicable",
         "3.5",
         "3.5",
         "3.8",
         "3.5",
         "3.9"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 166
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_description</th>\n",
       "      <th>location</th>\n",
       "      <th>salary_avg_estimate</th>\n",
       "      <th>salary_estimate_payperiod</th>\n",
       "      <th>company_size</th>\n",
       "      <th>company_founded</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>industry</th>\n",
       "      <th>sector</th>\n",
       "      <th>revenue</th>\n",
       "      <th>career_opportunities_rating</th>\n",
       "      <th>comp_and_benefits_rating</th>\n",
       "      <th>culture_and_values_rating</th>\n",
       "      <th>senior_management_rating</th>\n",
       "      <th>work_life_balance_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sanfoundry</td>\n",
       "      <td>Data Scientist - Fresher</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Job Code: Data-Scientist-Fresher-24011\\n\\nLoca...</td>\n",
       "      <td>Hyderābād</td>\n",
       "      <td>416516.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>1 to 50 Employees</td>\n",
       "      <td>--</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Colleges &amp; Universities</td>\n",
       "      <td>Education</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sanfoundry</td>\n",
       "      <td>Junior Software Engineer - Data Science - Fresher</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Job Code: Data-Scientist-Fresher-24012\\n\\nLoca...</td>\n",
       "      <td>Hyderābād</td>\n",
       "      <td>382623.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>1 to 50 Employees</td>\n",
       "      <td>--</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Colleges &amp; Universities</td>\n",
       "      <td>Education</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sanfoundry</td>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Job Code: Data-Scientist-24012\\n\\nLocation: Hy...</td>\n",
       "      <td>Hyderābād</td>\n",
       "      <td>539530.5</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>1 to 50 Employees</td>\n",
       "      <td>--</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Colleges &amp; Universities</td>\n",
       "      <td>Education</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Skyla Software Solution</td>\n",
       "      <td>Data Entry Operator</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The ideas that make life easier for millions o...</td>\n",
       "      <td>Jamshedpur</td>\n",
       "      <td>84853.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>1 to 50 Employees</td>\n",
       "      <td>--</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TVS Supply Chain Solutions</td>\n",
       "      <td>Data Entry Operator</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Department\\nWarehouse Operations\\nJob posted o...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>255288.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>501 to 1000 Employees</td>\n",
       "      <td>--</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Shipping &amp; Trucking</td>\n",
       "      <td>Transportation &amp; Logistics</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>String Bio</td>\n",
       "      <td>DATA SCIENTIST</td>\n",
       "      <td>4.3</td>\n",
       "      <td>Location: Bangalore, India\\nContact:Please ema...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>579655.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>1 to 50 Employees</td>\n",
       "      <td>--</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>eCloudChain</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Job-Code: DA-2004\\nWe are looking for a passio...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>539530.5</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>1 to 50 Employees</td>\n",
       "      <td>--</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>Kpro Solutions</td>\n",
       "      <td>Analytics &amp; Data Science</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Experience &amp; Qualification:\\n\\nMinimum of 9 ye...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>483915.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>1 to 50 Employees</td>\n",
       "      <td>--</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>Athena Global Technologies</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Immediate requirement for Data Engineer\\nExper...</td>\n",
       "      <td>Hyderābād</td>\n",
       "      <td>678949.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>51 to 200 Employees</td>\n",
       "      <td>--</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>Sanfoundry</td>\n",
       "      <td>Machine Learning Engineer - Fresher</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Job Code: Machine-Learning-Fresher-24012\\n\\nLo...</td>\n",
       "      <td>Hyderābād</td>\n",
       "      <td>539530.5</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>1 to 50 Employees</td>\n",
       "      <td>--</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Colleges &amp; Universities</td>\n",
       "      <td>Education</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        company  \\\n",
       "4                    Sanfoundry   \n",
       "6                    Sanfoundry   \n",
       "12                   Sanfoundry   \n",
       "17      Skyla Software Solution   \n",
       "18   TVS Supply Chain Solutions   \n",
       "..                          ...   \n",
       "718                  String Bio   \n",
       "720                 eCloudChain   \n",
       "721              Kpro Solutions   \n",
       "722  Athena Global Technologies   \n",
       "725                  Sanfoundry   \n",
       "\n",
       "                                             job_title  company_rating  \\\n",
       "4                             Data Scientist - Fresher             4.0   \n",
       "6    Junior Software Engineer - Data Science - Fresher             4.0   \n",
       "12                               Junior Data Scientist             4.0   \n",
       "17                                 Data Entry Operator             4.0   \n",
       "18                                 Data Entry Operator             3.4   \n",
       "..                                                 ...             ...   \n",
       "718                                     DATA SCIENTIST             4.3   \n",
       "720                                       Data Analyst             4.0   \n",
       "721                           Analytics & Data Science             4.0   \n",
       "722                                      Data Engineer             4.0   \n",
       "725                Machine Learning Engineer - Fresher             4.0   \n",
       "\n",
       "                                       job_description    location  \\\n",
       "4    Job Code: Data-Scientist-Fresher-24011\\n\\nLoca...   Hyderābād   \n",
       "6    Job Code: Data-Scientist-Fresher-24012\\n\\nLoca...   Hyderābād   \n",
       "12   Job Code: Data-Scientist-24012\\n\\nLocation: Hy...   Hyderābād   \n",
       "17   The ideas that make life easier for millions o...  Jamshedpur   \n",
       "18   Department\\nWarehouse Operations\\nJob posted o...   Bengaluru   \n",
       "..                                                 ...         ...   \n",
       "718  Location: Bangalore, India\\nContact:Please ema...   Bengaluru   \n",
       "720  Job-Code: DA-2004\\nWe are looking for a passio...      Remote   \n",
       "721  Experience & Qualification:\\n\\nMinimum of 9 ye...   Bengaluru   \n",
       "722  Immediate requirement for Data Engineer\\nExper...   Hyderābād   \n",
       "725  Job Code: Machine-Learning-Fresher-24012\\n\\nLo...   Hyderābād   \n",
       "\n",
       "     salary_avg_estimate salary_estimate_payperiod           company_size  \\\n",
       "4               416516.0                /yr (est.)      1 to 50 Employees   \n",
       "6               382623.0                /yr (est.)      1 to 50 Employees   \n",
       "12              539530.5                /yr (est.)      1 to 50 Employees   \n",
       "17               84853.0                /yr (est.)      1 to 50 Employees   \n",
       "18              255288.0                /yr (est.)  501 to 1000 Employees   \n",
       "..                   ...                       ...                    ...   \n",
       "718             579655.0                /yr (est.)      1 to 50 Employees   \n",
       "720             539530.5                /yr (est.)      1 to 50 Employees   \n",
       "721             483915.0                /yr (est.)      1 to 50 Employees   \n",
       "722             678949.0                /yr (est.)    51 to 200 Employees   \n",
       "725             539530.5                /yr (est.)      1 to 50 Employees   \n",
       "\n",
       "    company_founded    employment_type                 industry  \\\n",
       "4                --      Self-employed  Colleges & Universities   \n",
       "6                --      Self-employed  Colleges & Universities   \n",
       "12               --      Self-employed  Colleges & Universities   \n",
       "17               --   Company - Public                       --   \n",
       "18               --  Company - Private      Shipping & Trucking   \n",
       "..              ...                ...                      ...   \n",
       "718              --  Company - Private                       --   \n",
       "720              --  Company - Private                       --   \n",
       "721              --  Company - Private                       --   \n",
       "722              --  Company - Private                       --   \n",
       "725              --      Self-employed  Colleges & Universities   \n",
       "\n",
       "                         sector                   revenue  \\\n",
       "4                     Education  Unknown / Non-Applicable   \n",
       "6                     Education  Unknown / Non-Applicable   \n",
       "12                    Education  Unknown / Non-Applicable   \n",
       "17                           --  Unknown / Non-Applicable   \n",
       "18   Transportation & Logistics  Unknown / Non-Applicable   \n",
       "..                          ...                       ...   \n",
       "718                          --  Unknown / Non-Applicable   \n",
       "720                          --  Unknown / Non-Applicable   \n",
       "721                          --  Unknown / Non-Applicable   \n",
       "722                          --  Unknown / Non-Applicable   \n",
       "725                   Education  Unknown / Non-Applicable   \n",
       "\n",
       "     career_opportunities_rating  comp_and_benefits_rating  \\\n",
       "4                            4.5                       4.3   \n",
       "6                            4.5                       4.3   \n",
       "12                           4.5                       4.3   \n",
       "17                           4.0                       3.0   \n",
       "18                           3.2                       2.9   \n",
       "..                           ...                       ...   \n",
       "718                          4.6                       3.3   \n",
       "720                          4.0                       4.0   \n",
       "721                          4.0                       4.0   \n",
       "722                          4.2                       3.5   \n",
       "725                          4.5                       4.3   \n",
       "\n",
       "     culture_and_values_rating  senior_management_rating  \\\n",
       "4                          4.6                       4.7   \n",
       "6                          4.6                       4.7   \n",
       "12                         4.6                       4.7   \n",
       "17                         5.0                       4.0   \n",
       "18                         3.0                       3.0   \n",
       "..                         ...                       ...   \n",
       "718                        4.5                       4.7   \n",
       "720                        5.0                       5.0   \n",
       "721                        4.0                       4.0   \n",
       "722                        4.0                       3.9   \n",
       "725                        4.6                       4.7   \n",
       "\n",
       "     work_life_balance_rating  \n",
       "4                         4.6  \n",
       "6                         4.6  \n",
       "12                        4.6  \n",
       "17                        2.0  \n",
       "18                        3.0  \n",
       "..                        ...  \n",
       "718                       3.6  \n",
       "720                       4.0  \n",
       "721                       4.0  \n",
       "722                       3.9  \n",
       "725                       4.6  \n",
       "\n",
       "[166 rows x 18 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data[\"industry\"] == \"--\") | (data[\"sector\"] == \"--\") | (data[\"company_founded\"] == \"--\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91a37689",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = [\"industry\", \"sector\", \"company_founded\"]\n",
    "mask = (data[columns_to_check].ne(\"--\")).sum(axis=1) >= 2\n",
    "data = data[mask]\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c808bafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ae54be65-a99a-49be-9926-8e03e124879b",
       "rows": [
        [
         "company",
         "0"
        ],
        [
         "job_title",
         "0"
        ],
        [
         "company_rating",
         "0"
        ],
        [
         "job_description",
         "0"
        ],
        [
         "location",
         "0"
        ],
        [
         "salary_avg_estimate",
         "0"
        ],
        [
         "salary_estimate_payperiod",
         "0"
        ],
        [
         "company_size",
         "0"
        ],
        [
         "company_founded",
         "0"
        ],
        [
         "employment_type",
         "0"
        ],
        [
         "industry",
         "0"
        ],
        [
         "sector",
         "0"
        ],
        [
         "revenue",
         "0"
        ],
        [
         "career_opportunities_rating",
         "0"
        ],
        [
         "comp_and_benefits_rating",
         "0"
        ],
        [
         "culture_and_values_rating",
         "0"
        ],
        [
         "senior_management_rating",
         "0"
        ],
        [
         "work_life_balance_rating",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 18
       }
      },
      "text/plain": [
       "company                        0\n",
       "job_title                      0\n",
       "company_rating                 0\n",
       "job_description                0\n",
       "location                       0\n",
       "salary_avg_estimate            0\n",
       "salary_estimate_payperiod      0\n",
       "company_size                   0\n",
       "company_founded                0\n",
       "employment_type                0\n",
       "industry                       0\n",
       "sector                         0\n",
       "revenue                        0\n",
       "career_opportunities_rating    0\n",
       "comp_and_benefits_rating       0\n",
       "culture_and_values_rating      0\n",
       "senior_management_rating       0\n",
       "work_life_balance_rating       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13e8b7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "company",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "job_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "job_description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "location",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "salary_avg_estimate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "salary_estimate_payperiod",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company_size",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "company_founded",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "employment_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "industry",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sector",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "revenue",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "career_opportunities_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "comp_and_benefits_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "culture_and_values_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "senior_management_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "work_life_balance_rating",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "4b782789-b2e5-40b7-9c05-d53c1667316b",
       "rows": [
        [
         "0",
         "ABB",
         "Junior Data Analyst",
         "4.0",
         "Junior Data Analyst\nTake your next career step at ABB with a global team that is energizing the transformation of society and industry to achieve a more productive, sustainable future. At ABB, we have the clear goal of driving diversity and inclusion across all dimensions: gender, LGBTQ+, abilities, ethnicity and generations. Together, we are embarking on a journey where each and every one of us, individually and collectively, welcomes and celebrates individual differences.\n\nYou will be working as Junior Data Analyst and will be part of Process Automation Business Area for Measurement and Analytics division based in Bangalore, India. In this role you will be reporting to COE Data Management and Analytics Manager and will be responsible for performing data analytics activities and developing analytical solutions to enable business in strategy development for existing and potential products, systems or services.\nYour responsibilities\nUsing data systems to help gather, measure, organize and analyze data, providing sound market and competitive intelligence analysis related to market and trends\nPreparing analysis of internal sales, technical and financial data, interprets resulting data using statistical tools and making recommendations to Sales, Marketing, Finance and Product Management\nMaking diagnostic and predictive recommendations to management of existing gaps and new opportunities for growth, identifying global trends and patterns across various dimensions\nExtracting and analyzing data from sales or financial tools and preparing reports for management to give insights on trends, patterns and predictions across geographies\nAssisting in the development of analytical tools and solutions to support management to drive key business decisions. Training business stakeholders on proper usage of dashboards and monitors on-going data quality\nYour background\nB.E or B. Tech or BCA or Bachelor's in Data Science\nMinimum 1 to 2 years of experience in Data Analytics and visualization\nHands on experience in Basic or Advanced Excel, Basic Statistics, ETL tools, Power BI(Basic),Basic SQL,ML and Python\nAbility to adapt, problem solving skills and give recommendations to stakeholders\nEffective time management while handling business critical tasks\nGood teamwork and collaboration with cross functional teams to meet business deliverables\nGood communication skills\nMore about us\nABB's Measurement & Analytics division is among the world's leading manufacturers and suppliers of smart instrumentation and analyzers. With thousands of experts around the world and high-performance digital technology, ABB's team is dedicated to making measurement easy for its industrial and energy customers to let them operate more efficiently and profitably. We look forward to receiving your application (documents submitted in English are appreciated). If you want to discover more about ABB, take another look at our website www.abb.com. It has come to our attention that the name of ABB is being used for asking candidates to make payments for job opportunities (interviews, offers). Please be advised that ABB makes no such requests. All our open positions are made available on our career portal for all fitting the criteria to apply. ABB does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection to recruitment with ABB, even if is claimed that the money is refundable. ABB is not liable for such transactions. For current open positions you can visit our career website https://global.abb/group/en/careers and apply. Please refer to detailed recruitment fraud caution notice using the link https://global.abb/group/en/careers/how-to-apply/fraud-warning Work model: on site #LI-onsite",
         "Bengaluru",
         "325236.0",
         "/yr (est.)",
         "10000+ Employees",
         "1883",
         "Company - Public",
         "Electronics Manufacturing",
         "Manufacturing",
         "$10+ billion (USD)",
         "3.7",
         "3.6",
         "4.0",
         "3.5",
         "3.9"
        ],
        [
         "13",
         "News Corp",
         "Data Analyst",
         "3.6",
         "Job Description :\nJob Title: Analytics Engineer\nJob Location: Bengaluru, Karnataka\nWork Arrangement: Hybrid (3 days per week in office)\nKey Responsibilities:\nCollaborate with cross-functional teams to understand data requirements and objectives.\nExtract, transform, and load (ETL) data from various sources into the Google Cloud Platform environment.\nDevelop and optimize SQL queries for data retrieval and analysis.\nCreate and maintain dashboards using Looker Studio / Data Studio for reporting and visualization of key metrics.\nPerform exploratory data analysis to identify trends, patterns, and anomalies.\nAssist in the development of data-driven solutions and recommendations for business improvements.\nConduct data quality checks and ensure data integrity throughout the entire data lifecycle.\nImplement workflow orchestration (preferably Airflow) to automate data pipelines and enhance operational efficiency.\nSupport the integration and transformation of data from various sources into a centralized data warehouse.\nQualifications:\n2+ years of experience in SQL for data manipulation and analysis.\nFamiliarity with operating in the Google Cloud Platform environment.\nProficient in creating and maintaining reports and dashboards using Looker Studio / Data Studio.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork abilities.\nDetail-oriented with a focus on data accuracy and quality.\nPreferred Qualifications:\nFamiliarity with other data visualization tools (e.g., Tableau, Power BI).\nBasic programming skills (e.g., Python) for data preprocessing and automation.\nKnowledge of statistical analysis and modeling techniques.\nThis job is posted with NTS Technology Services Pvt. Ltd.\nJob Category:\nNews Corp is a global, diversified media and information services company focused on creating and distributing authoritative and engaging content to consumers throughout the world. The company comprises businesses across a range of media, including: news and information services, book publishing, digital real estate services, cable network programming in Australia, and pay-tv distribution in Australia.\nHeadquartered in New York, the activities of News Corp are conducted primarily in the United States, Australia, and the United Kingdom.",
         "Bengaluru",
         "522206.0",
         "/yr (est.)",
         "10000+ Employees",
         "2013",
         "Company - Public",
         "Publishing",
         "Media & Communication",
         "$5 to $10 billion (USD)",
         "3.5",
         "3.4",
         "3.3",
         "3.2",
         "3.5"
        ],
        [
         "25",
         "Infosys",
         "Healthcare Data Analyst",
         "3.8",
         "A day in the life of an Infoscion • As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction. • You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain. • You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews. • You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes. • You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nHealthcare Data analyst ,PL/SQL, SQL, Data mapping, STTM creation, Data profiling, Reports\nnull\nHealthcare,Data Analytics",
         "Chennai",
         "533713.0",
         "/yr (est.)",
         "10000+ Employees",
         "1981",
         "Company - Public",
         "Information Technology Support Services",
         "Information Technology",
         "$10+ billion (USD)",
         "3.8",
         "3.0",
         "4.0",
         "3.5",
         "3.7"
        ],
        [
         "27",
         "JPMorgan Chase & Co",
         "Data Analyst",
         "4.0",
         "JOB DESCRIPTION\n\nYou will be part of a team that will take a problem statement, find & acquire the relevant data, design & build the prototype, the solution, maintain and document.\nJob responsibilities\nExecutes creative software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems\nDevelops secure high-quality production code, and reviews and debugs code written by others\nIdentifies opportunities to eliminate or automate remediation of recurring issues to improve overall operational stability of software applications and systems\nRequired qualifications, capabilities, and skills\nExperience across the data lifecycle\nProgramming languages like Python, Java, Scala, etc.\nAdvanced SQL (e.g., joins and aggregations)\nScripting skills e.g. Python, R and experience with tools such as Alteryx or similar tool\nStrong understanding of data and database methodologies as well as hands on relational and cloud based systems (Databricks and/or AWS Cloud experience)\nBachelor’s degree in a relevant quantitative field (e.g. Engineering, Computer Science, Information Technology, Statistics, Business Analytics, Mathematics)\n2+ years of work experience across broad range of analytics platforms, languages, and tools (Databricks, AWS, SQL, Python and Spark, etc.)\nPreferred qualifications, capabilities, and skills\nStrong understanding of CI/CD Pipelines in a globally distributed environment using Git, Bit-Bucket, Jenkins, etc.\nExperience with the entire Software Development Life Cycle (SDLC) including planning, analysis, development and testing of new applications and enhancements to existing application.\nKnowledge of IAM (Identity Access Management) and ServiceNow\nFamiliarity with modern front-end technologies and building APIs\nStrong experience in cloud technologies\nAdvanced at Git, Scripting, Cloudformation / Terraform\nExperience customizing changes in a tool to generate data products\nCandidates must be able to physically work in our Bengaluru Office in evening shift - 2 PM to 11PM IST. The specific schedule will be determined and communicated by direct management.\nABOUT US\n\nJPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world’s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants’ and employees’ religious practices and beliefs, as well as any mental health or physical disability needs.\n\n\n\nABOUT THE TEAM\nOur Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We’re proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions – all while ranking first in customer satisfaction.\n\n\nThe CCB Data & Analytics team responsibly leverages data across Chase to build competitive advantages for the businesses while providing value and protection for customers. The team encompasses a variety of disciplines from data governance and strategy to reporting, data science and machine learning. We have a strong partnership with Technology, which provides cutting edge data and analytics infrastructure. The team powers Chase with insights to create the best customer and business outcomes.",
         "India",
         "539530.5",
         "/yr (est.)",
         "10000+ Employees",
         "1799",
         "Company - Public",
         "Banking & Lending",
         "Finance",
         "$10+ billion (USD)",
         "4.0",
         "3.9",
         "3.9",
         "3.6",
         "3.7"
        ],
        [
         "28",
         "Alcon",
         "Data Analyst - Digital Health",
         "3.8",
         "Key Responsibilities\nCollect and analyze large data sets to identify trends and insights that inform business decisions\nCollaborate with cross-functional teams to develop and implement data-driven solutions that improve business outcomes\nDevelop dashboards and reports to communicate findings and insights to stakeholders.\nBuild and maintain databases and data systems to ensure accuracy and accessibility of information for analytics\nCreates and modifies computer programs to extract information from company databases\nIdentify opportunities to improve data collection and analysis processes and make recommendations to optimize workflows\nFoster a culture of creativity, collaboration, speed, innovation, and engineering excellence.\nKey Requirements/Qualifications\nMinimum Qualifications\nBachelor’s degree in statistics, data science, or a related technical discipline.\n2+ years of hands-on experience in data preparation and analysis of large-scale datasets.\n2+ years of experience in Dashboard/BI and Data visualization tools (eg. Tableau, Quicksight, power BI)\n2+ year of health care and health care data\nProficient in SQL, Python, R and Excel\nStrong analytical and problem-solving skills, with attention to detail and accuracy.\nThrives in dynamic, cross-functional team environments. Possesses a team-first mindset, valuing diverse perspectives and contributing to a collaborative work culture.\nApproaches challenges with a positive and can-do attitude. Willing to challenge the status quo, demonstrating ability to understand when and how to take appropriate risks to drive performance.\nStrong communication and collaboration skills to deliver business solutions\nAlcon is an Equal Opportunity Employer and takes pride in maintaining a diverse environment. We do not discriminate in recruitment, hiring, training, promotion or other employment practices for reasons of race, color, religion, gender, national origin, age, sexual orientation, gender identity, marital status, disability, or any other reason.",
         "Bengaluru",
         "784161.0",
         "/yr (est.)",
         "10000+ Employees",
         "1945",
         "Company - Public",
         "Biotech & Pharmaceuticals",
         "Pharmaceutical & Biotechnology",
         "$5 to $10 billion (USD)",
         "3.4",
         "3.9",
         "3.6",
         "3.3",
         "3.5"
        ],
        [
         "31",
         "Wipro",
         "Data Analyst",
         "3.7",
         "Bengaluru, India\nGSH\n3036410\nJob Description\nGlobal Sales Enablement and Operations (GSE&O) is engaged in the strategic approach to enhance\nsales performance by improving sales productivity and efficiency, to increase and drive revenues for\nthe company. The key goals of GSE is to manage a healthy funnel, increase deal conversions, driving\norganizational alignment to sales function by designing and managing best-in-class sales processes\nand providing deep data insights for decision making.\n\nGSE is looking for specialist Business Data Analyst who has a passion to be a\npart of a strategic function with a direct impact on growing revenues for the organization. As part of\nthe GSE&O, you will be responsible for designing reports & dashboards, assessing the sales data,\nidentifying key sales patterns, and developing projections that aid in executive decision making.\n\n\nRoles & Responsibilities:\n\nRoles & Responsibilities -\n\nDesign, Execute and Manage Sales Analytics and Reporting globally for sales insights\nProvide actionable insights from huge volume of data (Both Structured and Unstructured)\nusing Data Mining, Data Cleansing techniques.\nCollecting, analyzing, and processing data to measure key performance indicator against\nbusiness objectives and benchmarks for the organization\nCollaborating with senior management to lead and implement process & policy changes\nAnalyze results and set up reports, design dashboards containing relevant KPI, measures that\nenable informed decision making\nDriving user forums and platforms for best practices sharing and knowledge harvesting to\nimprove sales operations\n\n\nQualifications:\n\nRequirments and Qualifications -\n\nPost-graduate degree in Management from premier institutes with more than 8 years of experience\nExperience in Advanced sales analytical and reporting skills\nExperience in sales operations and reporting, executive insight generation\nHands-On experience in working with Power BI / Tableau / Data Visualization tools\nProficient in formulating interactive dashboards/reports involving complex data using\nAdvanced Excel, VBA, etc.\nStrong leadership and communication skills\nKnowledge of current trends and practices relating to sales force effectiveness/ productivity\nanalytics and Insights generation.\nExcellent communication and presentation skills\n\nIf you encounter any suspicious mail, advertisements, or persons who offer jobs at Wipro, please email us at helpdesk.recruitment@wipro.com. Do not email your resume to this ID as it is not monitored for resumes and career applications.\nAny complaints or concerns regarding unethical/unfair hiring practices should be directed to our Ombuds Group at ombuds.person@wipro.com\n\nWe are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, caste, creed, religion, gender, marital status, age, ethnic and national origin, gender identity, gender expression, sexual orientation, political orientation, disability status, protected veteran status, or any other characteristic protected by law.\n\nWipro is committed to creating an accessible, supportive, and inclusive workplace. Reasonable accommodation will be provided to all applicants including persons with disabilities, throughout the recruitment and selection process. Accommodations must be communicated in advance of the application, where possible, and will be reviewed on an individual basis. Wipro provides equal opportunities to all and values diversity.",
         "Bengaluru",
         "494975.0",
         "/yr (est.)",
         "10000+ Employees",
         "1945",
         "Company - Public",
         "Information Technology Support Services",
         "Information Technology",
         "$5 to $10 billion (USD)",
         "3.6",
         "3.2",
         "3.7",
         "3.3",
         "3.5"
        ],
        [
         "32",
         "NatWest Group",
         "Data Analyst, AVP",
         "4.1",
         "Our people work differently depending on their jobs and needs. From hybrid working to flexible hours, we have plenty of options that help our people to thrive.\nThis role is based in India and as such all normal working days must be carried out in India.\nJoin us as a Data & Analytics Analyst\nThis is an opportunity to take on a purpose-led role in a cutting edge Data & Analytics team\nYou’ll be consulting with our stakeholders to understand their needs and identify suitable data and analytics solutions to meet them along with business challenges in line with our purpose\nYou’ll bring advanced analytics to life through visualisation to tell powerful stories and influence important decisions for key stakeholders, giving you excellent recognition for your work\nWe're offering this role at associate vice president level\nWhat you'll do\nAs a Data & Analytics Analyst, you’ll be driving the use of advanced analytics in your team to develop business solutions which increase the understanding of our business, including its customers, processes, channels and products. You’ll be working closely with business stakeholders to define detailed, often complex and ambiguous business problems or opportunities which can be supported through advanced analytics, making sure that new and existing processes are designed to be efficient, simple and automated where possible.\nAs well as this, you’ll be:\nLeading and coaching your colleagues to plan and deliver strategic project and scrum outcomes\nPlanning and delivering data and analytics resource, expertise and solutions, which brings commercial and customer value to business challenges\nCommunicating data and analytics opportunities and bringing them to life in a way that business stakeholders can understand and engage with\nAdopting and embedding new tools, technologies and methodologies to carry out advanced analytics\nDeveloping strong stakeholder relationships to bring together advanced analytics, data science and data engineering work that is easily understandable and links back clearly to our business needs\nThe skills you'll need\nWe’re looking for someone with a passion for data and analytics together with knowledge of data architecture, key tooling and relevant coding languages. Along with advanced analytics knowledge, you’ll bring an ability to simplify data into clear data visualisations and compelling insight using appropriate systems and tooling.\nYou’ll also demonstrate:\nStrong knowledge of data management practices and principles\nExperience of translating data and insights for key stakeholders\nGood knowledge of data engineering, data science and decisioning disciplines\nStrong communication skills with the ability to engage with a wide range of stakeholders\nCoaching and leadership experience with an ability to support and motivate colleagues",
         "Gurgaon",
         "579938.0",
         "/yr (est.)",
         "10000+ Employees",
         "1727",
         "Company - Public",
         "Banking & Lending",
         "Finance",
         "$10+ billion (USD)",
         "3.7",
         "3.5",
         "4.2",
         "3.7",
         "4.3"
        ],
        [
         "38",
         "eminenture",
         "Data Analyst",
         "4.0",
         "Fly high with your expertise in data analysis & have a successful career here.\nEducation:\nMinimum Graduation\n\nData Analyst\nResponsibilities\nExcellent verbal and written skills\nGood command on Excel (Vlookup, Hlookup, Pivot Tables, Conditional Formatting, If Conditioning and other advanced Excel tools like VBA and Macros)\nComfortable with UK –Shift Timing (13:00 to 22:00)\nEfficiency to work on outbound and in-house projects\nEducation\nMinimum Graduation\n\nBenefits\nPerks and Incentives for Incorporating with Us!\nPaid Holidays\nSick Days Off\nHealthcare\nCommuter Benefits\nRelocate\nLife Insurance\nHealth Savings\nLearning & Development\nGlobal Career",
         "Delhi",
         "539530.5",
         "/yr (est.)",
         "51 to 200 Employees",
         "2011",
         "Company - Private",
         "Information Technology Support Services",
         "Information Technology",
         "$1 to $5 million (USD)",
         "3.9",
         "3.4",
         "3.9",
         "3.7",
         "3.7"
        ],
        [
         "39",
         "Intercontinental Exchange",
         "Data Analyst I",
         "3.8",
         "Job Purpose\nConducts analysis, verification, remediation and communicates the quality of publicly recorded information that is strategically abstracted, keyed, and located by third party vendors.\nIntent is to control content, integrity, and reliability of data through process knowledge and research.\nDuties can include, but are not limited to quality control, data analysis, locating, vendor feedback, and reporting functions.\nResponsibilities\nPerform quality control of data using appropriate department applications and procedures.\nAnalyze and verify transmitted vendor data of keyed documents.\nAnalyze and verify completed and corrected documents listed under failed validation or business rules.\nReview and Analyze document images and data fields that need to be captured for various document and court case types.\nRespond and/or provide clarification to vendor questions.\nProvide vendor feedback for errors identified within transmitted data and QC of error rate statistics.\nReview and analyze QC, Upload, Missing document, Suspend, and other related reports.\nIdentify, remediate and/or physical repair of data issues.\nReconcile and document all failures within existing processes and validations.\nAssure that data is accurate and meets department and industry standards.\nPerforms other related duties as assigned.\nKnowledge and Experience\nBasic knowledge of real estate data with an emphasis on public records.\nFamiliarity with the various facets of public record data maintenance.\nKnowledge of county specific nuances relating to recorded document and court cases.\nWorking knowledge of public record data maintenance, applicable applications and proficient in entering data into internal applications.\nKnowledge on Recorder Data, Deed data and foreclosure is required.\nKnowledge on Mortgage document, Deed of trust is required.\nKnowledge on Foreclosure, NOD (Notice of default), NOS (Notice of sale), Title document would be good.\nStrong analytical skills, with experience in quality assurance.\nKnowledge of balancing and correction procedures.\nStrong verbal and written communication skills.\nKnowledge on SQL would be added advance but not mandatory.\nBasic computer skills, including efficiency with MS Office suite applications & emphasis on MS Excel.\nAbility to work collaboratively with others.\nAbility to manage multiple tasks and prioritize to meet deadlines successfully.\nOrganized and results driven.\nDomain knowledge w.r.t US Mortgage recording and assessor side is mandatory.\nSchedule\nThis role offers work from home flexibility of up to 2 days per week.",
         "Hyderābād",
         "469042.0",
         "/yr (est.)",
         "5001 to 10000 Employees",
         "2000",
         "Company - Public",
         "Stock Exchanges",
         "Finance",
         "$5 to $10 billion (USD)",
         "3.5",
         "3.7",
         "3.5",
         "3.3",
         "3.6"
        ],
        [
         "40",
         "Infifresh Foods",
         "Data Analyst",
         "3.6",
         "Job Information\nDepartment Name\nCentral\nIndustry\nFMCG/Foods/Beverage\nRequired Skills\ntableau\ndata visualization\n+1\nEducation\nAny degree\nJob Category\nDesk Job\nWork Experience\n1-3 years\nCity\nBangalore\nState/Province\nKarnataka\nCountry\nIndia\nZip/Postal Code\n560034\nJob Description\nJob Description:**\n\nAs a Data Analyst at Captain Fresh, you will play a crucial role in helping our organization make data-driven decisions. The ideal candidate will have 2+ years of experience and possess strong SQL and Excel skills. You will be responsible for understanding, exploring, analyzing, and merging different datasets using Excel and other data tools. You will also have a deep understanding of relational databases, CRUD operations, star schema, and the ability to combine, extract, and aggregate data to create meaningful insights.\n\nKey Responsibilities:\n\n1. **Data Analysis:**\n\nUse SQL to query and extract data from relational databases.\n\nClean and transform data to ensure its accuracy and consistency.\n\nPerform data analysis and generate actionable insights from diverse datasets.\n\n2. **Excel Expertise:**\n\nUtilize Excel to create and maintain spreadsheets, conduct data manipulation, and build simple dashboards for reporting.\n\nMerge and consolidate data from various sources to support business decision-making.\n\n3. **Database Management:**\n\nWork with relational databases to understand data schemas and perform CRUD operations.\n\nDesign and maintain efficient data pipelines for data extraction and transformation.\n\n4. **Data Visualization:**\n\nCreate easy-to-understand visualizations and reports to communicate data findings effectively.\n\nCollaborate with other teams to build interactive dashboards that provide real-time insights.\n\n5. **Problem Solving and Analysis:**\n\nIdentify trends, patterns, and anomalies in data.\n\nConduct root cause analysis and provide recommendations for process improvement.\n\nRequirements\nQualifications:\n\nBachelor's degree in a relevant field (e.g., Computer Science, Statistics, Data Science).\n\n2+ years of experience as a Data Analyst.\n\nProficiency in SQL and Excel.\n\nStrong understanding of relational databases and data modeling concepts.\n\nKnowledge of CRUD operations and star schema.\n\nAbility to combine, extract, and aggregate data for various analytical purposes.\n\nExcellent problem-solving skills and a curious, inquisitive mindset.\n\nStrong communication and collaboration skills.\n\nAbility to work independently and in a team-oriented environment.\n\nQuick learner and adaptable to new technologies and concepts.",
         "Bengaluru",
         "948683.0",
         "/yr (est.)",
         "Unknown",
         "2019",
         "Company - Private",
         "Fishery",
         "Agriculture",
         "Unknown / Non-Applicable",
         "3.5",
         "3.6",
         "3.6",
         "3.2",
         "3.4"
        ],
        [
         "43",
         "Valenta BPO",
         "Data Analyst",
         "3.2",
         "Data Analyst\nFULL TIME | VALENTA AI | INDIA\n\nJob Information\nIndustry\nIT Services\nCurrent Openings\n1\nWork Experience\n4-5 years\nSalary\n500000-600000\nCity\nBangalore North\nState/Province\nKarnataka\nZip/Postal Code\n560002\nJob Description\nAs a Data Analyst you will play a pivotal role in the visualization and analysis of critical data for high-impact projects. You will be responsible for creating, modifying, and maintaining complex data dashboards, primarily for our clients in the sports and construction sectors. This role requires a high level of proficiency in Power BI, Excel, and other data visualization tools, along with strong communication skills for client interactions.\n\n\n\nKey Responsibilities:\n\nData Visualization and Dashboard Development:\n\nDevelop and maintain data dashboards using Power BI, ensuring accurate and real-time representation of data.\nMake periodic updates to dashboards based on client feedback, including publishing different versions in separate workspaces.\n\nData Validation and Delivery:\n\nEngage in data validation processes to ensure accuracy and reliability of dashboard data.\nDeliver finalized dashboards to clients, ensuring they meet specified requirements.\n\nClient Interaction and Support:\n\nActively participate in weekly meetings to discuss client requirements and feedback.\nRespond to client queries regarding dashboard visualizations and data interpretation.\nTrain clients on understanding and using the dashboards effectively.\nCollaborate with team members, guiding them in client communication and issue resolution.\n\nProject Management and Reporting:\n\nMaintain a task tracker to manage and prioritize ad-hoc tasks and project deliverables.\nEnsure clear communication of project statuses and updates within the team and to stakeholders.\n\nTechnical Skills and Maintenance:\n\nUtilize complex DAX functions and data preparation techniques in Power BI.\nApply SQL queries in Snowflake for advanced data analysis.\nPerform routine maintenance and updates of dashboards based on client feedback and changing requirements.\n\n\nTechnical Skills:\n\nAdvanced Proficiency in Power BI:\n\nExpertise in creating complex dashboards and visualizations tailored to client needs.\nSkilled in using DAX (Data Analysis Expressions) for creating custom calculations and advanced data manipulation.\nExperience with data preparation and modeling within Power BI, including the use of Power Query.\n\nExcel Mastery:\n\nAdvanced skills in Excel for data analysis, including the use of pivot tables, advanced formulas, and data visualization techniques.\nAbility to manage large datasets within Excel and integrate Excel with other data tools.\n\nLucid Charts:\n\nProficiency in using Lucid Charts for creating flowcharts, process diagrams, and other visual representations of data and workflows.\n\nSQL and Database Management:\n\nCompetence in writing and optimizing SQL queries, particularly within Snowflake environments.\nUnderstanding of database structures and the ability to efficiently extract, transform, and load data (ETL).\n\nFamiliarity with Microsoft 365 Suite:\n\nComfortable using various applications within the Microsoft 365 suite, such as SharePoint, Teams, and others for collaboration and data sharing.\nAbility to integrate these tools with data analysis workflows.\n\nData Analysis and Problem Solving:\n\nStrong analytical skills with the ability to derive insights and recommendations from datasets.\nSkill in identifying, analyzing, and resolving complex data-related issues.\n\nCommunication and Training:\n\nAbility to clearly articulate technical concepts to non-technical audiences, including clients.\nExperience in training clients or team members on the use of data tools and dashboards.\n\n\n\nRequirements\nQualifications:\n\nBachelor’s degree in Data Science, Statistics, Computer Science, or a related field.\nProficient in Power BI, Excel (advanced), and Lucid Charts.\nExperience with SQL and familiarity with Snowflake.\nStrong analytical skills and attention to detail.\nExcellent communication and client-handling skills.\nAbility to work in a fast-paced, dynamic environment.\n\nPreferred Experience:\n\nPrior experience in data analysis or a similar role.\nKnowledge of Microsoft 365 suite.\nExperience working with data in the sports and/or construction industries.",
         "Bengaluru",
         "550000.0",
         "/yr (est.)",
         "51 to 200 Employees",
         "2014",
         "Company - Private",
         "HR Consulting",
         "Human Resources & Staffing",
         "Unknown / Non-Applicable",
         "3.2",
         "2.7",
         "3.1",
         "3.1",
         "3.6"
        ],
        [
         "44",
         "PhonePe",
         "Senior Data Analyst",
         "4.1",
         "About PhonePe Group:\nPhonePe is India’s leading digital payments company with 50 crore (500 Million) registered users and 3.7 crore (37 Million) merchants covering over 99% of the postal codes across India. On the back of its leadership in digital payments, PhonePe has expanded into financial services (Insurance, Mutual Funds, Stock Broking, and Lending) as well as adjacent tech-enabled businesses such as Pincode for hyperlocal shopping and Indus App Store which is India's first localized App Store. The PhonePe Group is a portfolio of businesses aligned with the company's vision to offer every Indian an equal opportunity to accelerate their progress by unlocking the flow of money and access to services.\nCulture\nAt PhonePe, we take extra care to make sure you give your best at work, Everyday! And creating the right environment for you is just one of the things we do. We empower people and trust them to do the right thing. Here, you own your work from start to finish, right from day one. Being enthusiastic about tech is a big part of being at PhonePe. If you like building technology that impacts millions, ideating with some of the best minds in the country and executing on your dreams with purpose and speed, join us!\nSDA, Analytics - PhonePe\nWho are we looking for?\nFirst principle problem solver and good executor who is passionate about working with data and various data processing systems. If you’re a curious mind and like to question the status quo, then you’d fit right in with us.\nWhat would you get if you work with us?\nYou'll be closely working with senior analysts in a team related to key principles around data, which includes ingestion, storage and consumption. You'll get to interact with some of the smartest professionals that the country has to offer and get exposure to all facets of building a data platform at scale. You get complete ownership and responsibility of BI processing and products of a specific area - right from problem identification to implementation at the org level.\nWhat would you get to do in this role?\nEnd-to-end BI partnership and collaboration will include designing and implementing everything related to data processing.\nDrive data accuracy, processing efficiency and consumptions convenience of data and platform.\nEfficient in SQL; able to write advance script, optimize and data model for processing.\nCollaborate closely with the business, and product analyst to understand the business problem and translate them into the right data processing problem statement.\nHave a strong problem-solving mindset and be able to apply the right analytical approach for solving the same.\nBe able to influence stakeholders across various functions to drive initiatives & data driven decision making\nAble to work closely with the data engineering team to debug and identify the issue.\n\nWhat do you need to have to apply for this position?\nMinimum 2 years of analytics or BI experience in relevant roles\nStrong problem-solving & analyst-relevant technical skills\nAbility to write complex queries on SQL to manipulate, and consolidate multiple data sources for the purpose of dashboarding and Data processing.\nIntuition for data and ability to handle big data sources\nStrong working knowledge in Data process monitoring tools(like Airflow, Grafana) and visualization tools like PowerBI, Tableau and Qliksense\nUnderstanding data analysis and automation languages such as R and Python is advantageous.\nStrong communication skills; Ability to clearly explain thoughts and ideas either verbally or in written form.\nUnderstanding of dashboarding principles and prior experience building such dashboards.\nPhonePe Full Time Employee Benefits (Not applicable for Intern or Contract Roles)\nInsurance Benefits - Medical Insurance, Critical Illness Insurance, Accidental Insurance, Life Insurance\nWellness Program - Employee Assistance Program, Onsite Medical Center, Emergency Support System\nParental Support - Maternity Benefit, Paternity Benefit Program, Adoption Assistance Program, Day-care Support Program\nMobility Benefits - Relocation benefits, Transfer Support Policy, Travel Policy\nRetirement Benefits - Employee PF Contribution, Flexible PF Contribution, Gratuity, NPS, Leave Encashment\nOther Benefits - Higher Education Assistance, Car Lease, Salary Advance Policy\nWorking at PhonePe is a rewarding experience! Great people, a work environment that thrives on creativity, the opportunity to take on roles beyond a defined job description are just some of the reasons you should work with us. Read more about PhonePe on our blog.",
         "Bengaluru",
         "777892.0",
         "/yr (est.)",
         "5001 to 10000 Employees",
         "2015",
         "Company - Private",
         "Internet & Web Services",
         "Information Technology",
         "Unknown / Non-Applicable",
         "4.0",
         "4.2",
         "4.1",
         "3.7",
         "3.8"
        ],
        [
         "48",
         "Lloyds Technology Centre",
         "Data Analyst",
         "3.3",
         "General information\nName\nData Analyst\nRef #\n1170\nPosting Date\nThursday, January 4, 2024\nCountry\nIndia\nCity\nHyderabad\nPlatform\nEnterprise Risk\nJob Family\nData Science\nKey Skills\nPython, SQL, Java, Scala or Go\nDescription & Requirements\nData Analyst\nEnterprise Risk| Data\nWork for Lloyds Technology Centre who are part of Lloyds Banking Group, the UK's largest digital bank, where you’ll make a genuine difference, be able to develop yourself and be part of a culture where everyone's contribution is recognized.\nOpportunity to be a part of a mission; shaping finance as a force for good - Lloyds Banking Group’s mission is to create a sustainable and inclusive future for people and businesses, shaping finance as a force for good. We, at Lloyds Technology Centre, play a key part in delivering this*. We are also guided by our values in shaping the way we work and how we make decisions. This creates an environment where colleagues love to work and can make a positive impact.\nRange of exclusive benefits and rewards - We value your contributions and will ensure that your total reward experience reflects the expertise you bring and impact you create. We also strive to provide a holistic proposition that meets your wellbeing needs. Our total reward practices help us create an ecosystem where you can thrive, ensuring your essential needs are met so you can focus on your personal growth and future success.\nCareer elevating opportunities - At Lloyds Technology Centre, you will be empowered to take charge of your career journey through personalized career mentorship from experienced mentors, leadership development programs, and stretch assignments. You will be able to access opportunities for continuous learning and exposure to new experiences through job shadowing and cross-functional collaboration on projects.\nFuture skill building opportunities - Being part of Lloyds Banking Group, who are known for their market leading practices in learning and development, Lloyds Technology Centre is committed to help you achieve your personal and professional aspirations. You will have access to role specific learning pathways & training, targeted accelerated development programs and professional certifications & qualifications.\nInclusive and diverse workplace - At Lloyds Technology Centre, you will be part of an inclusive workplace where everyone feels valued, respected, and empowered. We embrace and celebrate diversity at every level of our workforce, valuing and respecting you for your unique identity.\nWhat you’ll do:\nEnterprise Risk platform plays a critical role in creating and running the ecosystem of cross-cutting channel capabilities, driving simplification and modernization, enabling business platforms to focus on the creation of customer and colleague experiences.\nAs a Data Analyst within the Enterprise Risk Platform and the Data Lab, you’ll be working on regular high quality and trusted resource reporting and analysis in line with our reporting timetable for our customers.\nDefining and tracking progress against our Strategic Workforce plan.\nOptimizing resource forecasts, working with GCOO teams to understand variances and encourage best practices.\nCreating resource models in support of the new Platform model, including tracking progress during the adoption.\nAdopting a continuous improvement mindset and looking for opportunities to automate and transform our processes.\nSourcing data from a variety of systems and create analysis to support the team in delivering insight to our customers.\nEngaging with our partners and customers as required to ensure their needs are fulfilled and insight is shared effectively.\n\nWhat you’ll need:\nBachelor's degree (or equivalent) in engineering.\nExcellent skills like Data analysis, problem solving and Data Modelling.\nExperience in handling reporting packages like Business Objects, programming (JavaScript, XML, or ETL frameworks), databases.\nProficient in using programming languages like SQL, Oracle, MATLAB.\nWorking knowledge on database design development, data models, techniques for data mining, and segmentation.\n\nNice to have skills:\nExposure to data visualization software like Tableau, QlikView.\nGood logical reasoning, problem-solving, and communication skills.\nKnowledge of working with any reporting tools like Python or Microsoft Excel or RapidMiner or KNIME or Power BI or Apache Spark or Talend or Splunk.\nAbout working for Lloyds Technology Centre:\nOur new technology Centre in Hyderabad will be home to highly skilled technology and data specialists who will be driving our transformation and delivering great outcomes for Lloyds Banking Group’s customers. Our office is situated in a sought-after location that features easy transport links and excellent facilities, all aimed at enabling you to achieve a great work-life balance.\nWorking with us means being part of our aspirational and transformative journey of redefining the fintech landscape, while building an organization that welcomes all. We’re committed to providing an exceptional employee experience through our policies, practices, and development opportunities to support you in achieving your potential.\nThis is a once in a career opportunity to shape your future and help us make our mark in India. Are you ready to help shape your future, as well as ours?\nJoin us and grow with purpose.\n*Lloyds Technology Centre does not offer financial services in India.",
         "Hyderābād",
         "539530.5",
         "/yr (est.)",
         "10000+ Employees",
         "1695",
         "Company - Public",
         "Banking & Lending",
         "Finance",
         "Unknown / Non-Applicable",
         "3.2",
         "3.4",
         "3.2",
         "2.9",
         "3.2"
        ],
        [
         "51",
         "JLL",
         "Sustainability Data Analyst",
         "3.9",
         "JLL supports the Whole You, personally and professionally.\n\nOur people at JLL are shaping the future of real estate for a better world by combining world class services, advisory and technology to our clients. We are committed to hiring the best, most talented people in our industry; and we support them through professional growth, flexibility, and personalized benefits to manage life in and outside of work. Whether you’ve got deep experience in commercial real estate, skilled trades, and technology, or you’re looking to apply your relevant experience to a new industry, we empower you to shape a brighter way forward so you can thrive professionally and personally.\n\nWhat this job involves?\n\nShaping the future of real estate for a better world\nAt JLL, we see a Brighter Way forward for our clients, our people, our planet, and our communities. With over 200 years of real estate experience, we are, and always have been, in continual pursuit of brighter ways of working.\n\nWe bring to life see a Brighter Way in all that we do by seeking better, smarter, more innovative ways of working. We approach our work in a warmer, more optimistic, and inclusive way.\nJLL is a global leader in helping clients envision where people will live, work, play, shop, and eat.\n\nWhat this opportunity involves:\nWe seek a Junior Sustainability Data Analyst to join our team. You will support the sustainability data analyst reporting, data management, platform, compliance and reporting functions for a wide range of assets across JLL.\n\nJLL's purpose-driven global sustainability program delivers impact on climate action for sustainable real estate, healthy spaces for all people and thriving communities.\nWe are a rapidly expanding team, and over time we continuously support your growth with development opportunities available within our data and analytics teams.\n\nAn overview of the role:\nAssist the reporting team with insights, analytics, preparing data and presentations.\nAssist the team with delivering projects that will enable clients to meet sustainability reporting objectives.\nDevelop a detailed understanding of JLL’s sustainability reporting application and how we support clients in measuring sustainability performance.\nAssist the team with client delivery milestones to ensure they are being met.\nSounds like you? This is what we are looking for\nA passion for Sustainability and pulling together associated Data and Reporting.\nIntermediate Excel skills.\nInsights, element visualisation, and presenting data.\nExcellent communication skills.\nWhat you can expect from us:\nYou’ll join an entrepreneurial, inclusive culture. One where the best inspire the best. Where like-minded people work naturally together to achieve great things. Join us to develop your strengths and enjoy a fulfilling career full of varied experiences. Keep those ambitions in sight and imagine where JLL can take you.\n\nAs an organisation, we don’t just accept that we are a place of many different people, but we embrace it, we celebrate it, and we proactively support the needs that difference brings. JLL is committed to equal opportunity regardless of race, gender, age, sexual orientation or disability, and that is why, for more than a decade, we continue to rank among the World’s Most Ethical Companies.\n\nWe are dedicated to offering veterans from all ranks and services a successful civilian career as they transition out of the military. We recognise and appreciate the skills acquired in their service careers as vital and transferable to our workforce.\n\nIf this job description resonates with you, we encourage you to apply even if you don’t meet all of the requirements below. We’re interested in getting to know you and what you bring to the table!\n\nPersonalized benefits that support personal well-being and growth:\n\nJLL recognizes the impact that the workplace can have on your wellness, so we offer a supportive culture and comprehensive benefits package that prioritizes mental, physical and emotional health.\n\nAbout JLL –\n\nWe’re JLL—a leading professional services and investment management firm specializing in real estate. We have operations in over 80 countries and a workforce of over 102,000 individuals around the world who help real estate owners, occupiers and investors achieve their business ambitions. As a global Fortune 500 company, we also have an inherent responsibility to drive sustainability and corporate social responsibility. That’s why we’re committed to our purpose to shape the future of real estate for a better world. We’re using the most advanced technology to create rewarding opportunities, amazing spaces and sustainable real estate solutions for our clients, our people, and our communities.\n\nOur core values of teamwork, ethics and excellence are also fundamental to everything we do and we’re honored to be recognized with awards for our success by organizations both globally and locally.\n\nCreating a diverse and inclusive culture where we all feel welcomed, valued and empowered to achieve our full potential is important to who we are today and where we’re headed in the future. And we know that unique backgrounds, experiences and perspectives help us think bigger, spark innovation and succeed together.",
         "Bengaluru",
         "539530.5",
         "/yr (est.)",
         "10000+ Employees",
         "--",
         "Company - Public",
         "Real Estate",
         "Real Estate",
         "$5 to $10 billion (USD)",
         "3.7",
         "3.5",
         "3.8",
         "3.5",
         "3.7"
        ],
        [
         "53",
         "Sectigo",
         "Data Analyst",
         "3.4",
         "Who We Are!\nAt Sectigo, we align around our mission and pride ourselves in helping thousands of customers sleep better at night.\n“When people think Online trust management, they think Sectigo because we offer our customers unparalleled peace of mind.”\nHow we show up with each other and our customers every day is just as important, and we win as #OneSectigo by living out our core values - Support, Excellence, Collaboration, Teamwork, Integrity, Growth and Openness. We are committed to investing in our diverse teams where everyone understands their role and how they support our strategic goals, we drive operational excellence through scale and efficiency, and we strive to delight our customers and become the market leader in our industry. If you aspire to join a driven team that holds each other accountable to meeting our lofty goals and you’d like to be part of our growth story in delivering a market leading user experience, we’d like to talk to you.\nWhat We Are Looking For:\nThe Data Analyst reports to the Head of FP&A with significant exposure to executive leadership. They will collaborate cross-functionally to ensure that operational decisions are supported by financial metrics. They will build financial models and complete analyses that will produce insights and help support strategic decisions. They will participate in and drive the forecasting and budgeting process. The FP&A team is a scaling function within the business which will allow this role to participate in a wide range of projects and responsibilities.\nWhat You’ll Be Doing:\nCollaborate with the Head of FP&A and the finance team to produce standard periodic reporting (monthly and weekly financial reports and dashboards), including variance analysis and explanations\nParticipate in the annual budget and reforecast process including project management, engagement with the senior management team, and assessment/presentation of results\nProactively identify and expand ways for the FP&A function to impact the business\nParticipate in the presentation of results to the senior management team\nDevelop KPIs and analyses to track key business metrics such as ARR, customer retention, sales performance, forecasting accuracy, and others\nPartner with the Accounting team to ensure accurate reporting and integration of actuals into forecasts\nPerform various ad hoc analysis as directed by the CEO/CFO\nDevelop relationships across the entire organization to gain a detailed understanding of all aspects of the business operations and trust amongst the senior leaders and provide analytical support as needed\nBe a resource for analysis and due diligence related to M&A activity\nRequirements:\nEducation\nDegree in Finance, Accounting, or similar\nExperience\n2+ years professional experience including financial forecasting\nExperience in private equity, investment banking, or investor relations strongly preferred\nExperience forecasting and analyzing SaaS B2B topline strongly preferred\nTechnical skill set including SQL, PowerBI, or similar data/visualization tools preferred\nTalents and Desired Qualifications:\nStrong analytical skills with a proven track record of delivering actionable insights\nUnderstanding of GAAP\nAdvanced Excel modeling and Power Point\nStrong communication, interpersonal, and presentation skills\nManagement Responsibilities:\nNone\nUDEo4BJaAv",
         "Chennai",
         "636962.0",
         "/yr (est.)",
         "201 to 500 Employees",
         "2008",
         "Company - Private",
         "Computer Hardware Development",
         "Information Technology",
         "Unknown / Non-Applicable",
         "3.0",
         "2.9",
         "3.0",
         "3.0",
         "3.3"
        ],
        [
         "54",
         "Wrike Careers Page",
         "Customer Support Data Analyst",
         "3.8",
         "Wrike is the most powerful work management platform. Built for teams and organizations looking to collaborate, create, and exceed every day, Wrike brings everyone and all work into a single place to remove complexity, increase productivity, and free people up to focus on their most purposeful work.\nOur vision: A world where everyone is free to focus on their most purposeful work, together.\nRole Overview:\nWe're looking for Customer Support Analyst, who will be conducting research and delivering business insights to support data-driven decision-making and optimization of business processes in Customer Support team\nWrike Support is a diverse, multicultural team, responsible for building relationships with our customers and assisting them with any challenges they encounter. In this role, you will be responsible for providing business insights based on data analysis and assisting to implement change management based on those findings, defining key business metrics, and supporting the management team with the reporting and ad-hoc analysis required to make crucial business decisions.\nAt Wrike, we believe that work should be both challenging and fun. We're growing rapidly and provide excellent opportunities for professional growth. We're smart, passionate, friendly, and professional and are looking for the same qualities in you.\nJob Scope and Accountabilities:\nCompile, interpret, and present weekly & monthly reports on key metrics (SLA attainment, customer satisfaction, team load & efficiency, etc.)\nAutomate recurrent reports, create, maintain and improve appropriate dashboards and data sources\nConduct ad hoc analysis and deliver business insights to support data-driven decision-making in both run-the-business and change-the-business activities\nPartner with Operations and Leadership teams to implement new metrics and approaches to resolve possible areas for process improvement\nCollect historical data and directional inputs from stakeholders to forecast team load and capacity\nExperience Requirements\nGood knowledge of SQL, databases (we use Google BigQuery) and statistics basics\nKnowledge of BI tools (we use Tableau, Zendesk Explore) and proven experience in data visualization\nStrong analytical and problem-solving skills\nDesired Skills\nExperience working within a customer support/success organization, understanding customer lifecycle and support processes is a plus\nA tendency to think about overall business value rather than numbers and charts\nCommunicative and friendly; a strong team player, ready to learn\nExperience with Python\nInterpersonal skills:\nDisplay great people skills, connecting effectively with individuals, demonstrating friendliness, empathy, and tact, and maintaining composure under pressure during difficult interpersonal situations.\nCritical thinker, generally curious—a true problem solver\nPassionate about learning and improving every day, and motivated to excel\nOpen to feedback; coachable\nStrong team player\nSelf-starter with strong ownership skills, willing to go above and beyond the job\ndescription\nCreative and innovative\n#LI-JM1\nWho Is Wrike and Our Culture\nWe're a team of innovators and creators who solve the complex work problems of today and tomorrow.\nHybrid work mode\n\nWrike promotes a hybrid work mode and we meet in the office 3 times a week. This work mode supports our culture of collaboration and solving problems fast to deliver business outcomes and win together.\nOur persona\nSmart: We love what we do, and we're great at it because this is our domain. Our combined knowledge in this space is unmatched.\nDedicated: We get up every day focused on helping our customers win. We're committed to helping our teammates win, too!\nApproachable: We're friendly, easy to get along with, considerate, and helpful.\nOur culture and Values\nDeliver Business Outcomes\nBe better than the competition\nMove fast. Then, move faster\nKnow our customers\nWe win together\nHave courage\n\nCheck out our LinkedIn Life Page, Instagram, Wrike Engineering Team, Medium, Meetup.com, Youtube for a feel for what life is like at Wrike.",
         "Bengaluru",
         "562146.0",
         "/yr (est.)",
         "1001 to 5000 Employees",
         "2006",
         "Company - Private",
         "Software Development",
         "Information Technology",
         "Unknown / Non-Applicable",
         "3.4",
         "3.7",
         "3.9",
         "3.3",
         "4.1"
        ],
        [
         "55",
         "Rotork",
         "Data Analyst",
         "3.7",
         "Company Description\n\nRotork is the market-leading global flow control and instrumentation company, helping our customers manage the flow or liquids, gases and powders across many industries worldwide.\nOur purpose is Keeping the World Flowing for Future Generations.\nFor over sixty years, the world has relied on us to create the things that keep everything moving. From oil and gas to water and shipping, pharmaceuticals and food- these are the flows on which our modern world depends.\nToday we're respected and admired for our people, performance and products. Our success flows from our commitment to engineering excellence, and that's what we will always pursue, safely and sustainably.\nRotork is going through an exciting period of change and growth, building on our existing market success. It's a great time to join us and make an impact in shaping the future of our business.\nhttp://www.rotork.com\n\nJob Description\n\nWorking as part of the Data team and support team on all aspects of Data management.\nYour responsibilities will include,\nData profiling, Schema mapping, Master Data Management, Data analysis, Data cleansing, Data migration\nData quality checks and advising project teams on potential data issues and actions.\nDeliver data service requests in line with BAU demand.\nTechnical Skills\nEssential:\nRelational Databases – SQL Server, 2016 is a minimum, Oracle nice to have.\nSQL – T-SQL, Stored Procedures, Views, DML, DDL\nETL/ELT – SSIS and Azure Data Factory\nExposure to Azure SQL Database\nOffice – Advanced Excel skills, Word, Visio, PowerPoint\nDesirable:\nData Migration Tools\nMicrosoft 365 Applications - Power BI, PowerApps, SharePoint\nExposure to Microsoft Dynamics or any ERP\n\nQualifications\n\nSoft Skills\nEssential:\nExcellent written and verbal communication skills\nGreat attention to detail\nCommitment to personal excellence\nDesire to expand technical expertise.\nCongenial attitude - team player\nDisciplined work ethic\nDesirable:\nOut-of-the-box thinker with creative problem-solving ability.\nAbility/Experience to establish priorities, work independently, and proceed with objectives without supervision\nPersonal Specification:\nEssential\nDemonstrate integrity and honesty.\nCapable of communicating at all levels\nEthical and transparent leadership, setting an example to others.\nDemonstrate an appetite and ability to work collaboratively in a complex and matrixed business.\nAble to deal with people with different cultures throughout the region.\nAble to thrive in a changing business, embracing ambiguity and solving complex problems.\n\nAdditional Information",
         "Chennai",
         "569738.0",
         "/yr (est.)",
         "1001 to 5000 Employees",
         "--",
         "Company - Public",
         "Electronics Manufacturing",
         "Manufacturing",
         "$500 million to $1 billion (USD)",
         "3.5",
         "3.2",
         "3.5",
         "3.4",
         "3.9"
        ],
        [
         "57",
         "Wipro Limited",
         "Data Privacy/Data Analyst",
         "3.7",
         "Overview:\n\nRole Purpose\nThe purpose of this role is to interpret data and turn intoinformation (reports, dashboards, interactive visualizations etc) whichcan offer ways to improve a business, thus affecting business decisions.\nDos\n\nManaging the technical scope of the project in line with therequirements at all stages\nGather information from various sources (data warehouses,database, data integration and modelling) and interpret patterns andtrends\nDevelop record management process and policies\nBuild and maintain relationships at all levels within the clientbase and understand their requirements.\nProviding sales data, proposals, data insights and accountreviews to the client base\nIdentify areas to increase efficiency and automation ofprocesses\nSet up and maintain automated data processes\nIdentify, evaluate and implement external services and tools tosupport data validation and cleansing.\nProduce and track key performance indicators\n\nAnalyze the data sets and provide adequate information\nLiaise with internal and external clients to fully understanddata content\nDesign and carry out surveys and analyze survey data as per thecustomer requirement\nAnalyze and interpret complex data sets relating tocustomer’s business and prepare reports for internal and externalaudiences using business analytics reporting tools\nCreate data dashboards, graphs and visualization to showcasebusiness performance and also provide sector and competitorbenchmarking\nMine and analyze large datasets, draw valid inferences andpresent them successfully to management using a reporting tool\nDevelop predictive models and share insights with the clients asper their requirement\n\nStakeholder Interaction\n\nStakeholder Type\n\nStakeholder Identification\n\nPurpose of Interaction\n\nInternal\n\nProject Manager/ Database Lead\n\nRegular reporting & updates\n\nExternal\n\nClients\n\nClient engagement, reviews etc\n\nDisplay\n\nLists the competencies required to perform this role effectively:\nFunctional Competencies/ Skill\nLeveraging Technology – Knowledge of current and upcomingtechnology (automation, tools and systems) to build efficiencies andeffectiveness in own function/ Client organization – Expert\nProcess Excellence - Ability to follow the standards and norms toproduce consistent results, provide effective control and reduction ofrisk – Expert\nTechnical knowledge – knowledge of various programminglanguages/ software (Python, Microsoft Excel, VBA, Matlab, SQL etc) andtools on data analytics - Expert\n\nCompetency Levels\n\nFoundation\n\nKnowledgeable about the competency requirements. Demonstrates (inparts) frequently with minimal support and guidance.\n\nCompetent\n\nConsistently demonstrates the full range of the competencywithout guidance. Extends the competency to difficult and unknownsituations as well.\n\nExpert\n\nApplies the competency in all situations and is serves as a guideto others as well.\n\nMaster\n\nCoaches others and builds organizational capability in thecompetency area. Serves as a key resource for that competency and isrecognised within the entire organization.\n\nBehavioral Competencies\nFormulation &Prioritization\nClient centricity\nExecution Excellence\nPassion for Results\nConfidence\nBusiness Acumen\n\nDeliver\n\nNo.\n\nPerformance Parameter\n\nMeasure\n\n1.\n\nAnalyses data sets and provide relevant information to the client\n\nNo. Of automation done, On-Time Delivery, CSAT score, Zero customerescalation, data accuracy",
         "Bengaluru",
         "494975.0",
         "/yr (est.)",
         "10000+ Employees",
         "1945",
         "Company - Public",
         "Information Technology Support Services",
         "Information Technology",
         "$5 to $10 billion (USD)",
         "3.6",
         "3.2",
         "3.7",
         "3.3",
         "3.5"
        ],
        [
         "61",
         "HP",
         "Data Analyst",
         "4.1",
         "Data Analyst\nTo support HP Proactive Insights Experience Management (PIXM) Services, we are looking for an experienced Data Analyst who can perform advanced data analysis to support HP strategic accounts and provide ongoing value to HP customers in different areas of operations.\nThe role focuses on working with Technical Service Delivery Managers (TDM) and automation engineers to analyze and proactively solve employee experience issues for customers. The role is expected to work collaboratively with TDMs, Professional Services & Customer Success teams.\nThe primary responsibilities of the role are as follows:\nAnalyze, baseline, and benchmark customers’ digital experience scores to drive actionable insights for ongoing operations.\nInfluence customer IT business decisions using proven data analytics and business insights.\nLead data analysis projects from end to end, including business and data requirements gathering, data modeling, data validation, and visualization.\nCollaborate with customers to integrate data analysis into IT operations and streamline their incident and problem management processes.\nStay current on new product features and technologies to the level required for the above activities.\nQualifications\nBA/BS preferred in computer science, computer engineering, or mathematics.\nMinimum of 3-5 years of experience in technical consulting, data analytics, and IT operations roles\nAbility to perform data analysis on large datasets to drive actionable insights and automation.\nProven use of business intelligence and data visualization tools (Power BI, Tableau, AWS Quick sight, SQL, Python, etc.) to deliver insights and analysis.\nExcellent communication skills. Ability to present to senior IT leaders and defend analytical results and recommendations.\nExtensive knowledge of Microsoft Windows, MacOS operating systems and applications\nExperience in workplace management, network management, software productivity, and collaboration tool management\nExperience with enterprise transformation projects (e.g., Win10, Win11, O365 migration)\nGood knowledge of IT infrastructure, networking, and applications\nGood knowledge of IT operations and ITIL methodology\nWorking knowledge of Windows PowerShell, Bash scripting\nAbility to quickly learn new technologies in an unsupervised environment.\nEnthusiasm for working in an international, collaborative, and fast-paced environment and learning new technologies.\nPreferred: Experience with digital employee experience management platforms such as HP Proactive Insights, Nexthink, 1E Tachyon, and VMware Workspace One Intelligence. Experience working in a Scrum Agile development environment.\nFluency in English, with excellent written, digital, and verbal communication skills; knowledge of other languages would be a plus.\n\nAbout HP\n\nYou’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.\nSo are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.\n\nHP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.\n\nOur history: HP’s commitment to diversity, equity and inclusion – it's just who we are.\nFrom the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!",
         "Bengaluru",
         "869528.0",
         "/yr (est.)",
         "10000+ Employees",
         "1939",
         "Company - Public",
         "Computer Hardware Development",
         "Information Technology",
         "Unknown / Non-Applicable",
         "3.7",
         "3.7",
         "4.3",
         "3.8",
         "4.2"
        ],
        [
         "65",
         "JPMorgan Chase & Co",
         "Reference Data Analyst - Team Leader",
         "4.0",
         "JOB DESCRIPTION\n\nJob Summary:- The Client Account Services (CAS) team manages and supports the delivery of a global strategy across Investor Services products for Client Documentation, Accounts, and Entitlements. Additionally, they are expected to contribute to a wider team, provide regular progress updates, maintain an understanding of client requirements / documentations, approach their work with a control-mindset, and demonstrate an understanding/application of policies and procedures. Credentials Strong People management skills (Associate),Good leadership skills and ability to motivate team ,Have strong verbal and written communication skills, Good team player and self-motivated, Strong analytical skills, Desire to work in a fast-paced environment with multiple deliverables, Proven skills in time management, organization, and attention to detail, Proficiency in Microsoft Office suite of applications Preferred experienced with new age tools like Alteryx, Tableau, Xceptor etc.\nJob Responsibilities :-\nAccurate and timely completion of daily by self and team. Ensure all activities are completed within SLA.\nAppropriate research of all exceptions\nWork with upstream teams to fix all exceptions\nTimely response and prompt to all client queries and rush requests\nProcess improvement to generate efficiency saves and improve on the turnaround time to the client\nStakeholder interactions – set appropriate expectations with respect to timelines and details as required\nConduct appropriate testing's for all changes and enhancements in the system\nControl and maintain inventories of all UTs and SOPs.\nRequired qualifications, capabilities and skills -\nPreferred qualifications, capabilities and skills –\nCandidate should have good knowledge and understanding of static/reference data process\nCandidate should have strong attention to detail in review of documents and review capabilities on various product and types.\nCandidate should possess the ability to work under pressure and to meet deadlines\nPrior experience in similar Domain will be an added advantage and should be a requirement at manager and above level\nABOUT US\n\nJPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world’s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants’ and employees’ religious practices and beliefs, as well as any mental health or physical disability needs.\n\n\n\nABOUT THE TEAM\nThe Corporate & Investment Bank is a global leader across investment banking, wholesale payments, markets and securities services. The world’s most important corporations, governments and institutions entrust us with their business in more than 100 countries. We provide strategic advice, raise capital, manage risk and extend liquidity in markets around the world.\n\n\nOperations teams develop and manage innovative, secure service solutions to meet clients’ needs globally. Developing and using the latest technology, teams work to deliver industry-leading capabilities to our clients and customers, making it easy and convenient to do business with the firm. Teams also drive growth by refining technology-driven customer and client experiences that put users first, providing an unparalleled experience.",
         "Bengaluru",
         "657267.0",
         "/yr (est.)",
         "10000+ Employees",
         "1799",
         "Company - Public",
         "Banking & Lending",
         "Finance",
         "$10+ billion (USD)",
         "4.0",
         "3.9",
         "3.9",
         "3.6",
         "3.7"
        ],
        [
         "72",
         "JPMorgan Chase & Co",
         "Reference Data Analyst",
         "4.0",
         "JOB DESCRIPTION\n\nJoin the team that is responsible for leading projects to support strategic and business change initiatives\nDo you enjoy making a difference through project management, risk management, financial analysis, and regulatory reporting? This is an exciting career opportunity to showcase your knowledge, skills, and abilities. You have found the right team.\nAs the Data Transformation Lead, in the Party Reference Data Operations Team you will be responsible for leading projects to support strategic and business change initiatives driven by Reference Data Strategy and Party Reference Data operations. You will work closely with the Client Account Services Party Reference Data leadership team to facilitate information sharing, operational readiness and ongoing BAU effectiveness. You will have solid time management, organizational and effective prioritization skills. You will use your high level of Excel, PowerPoint, and SharePoint skills as well as excellent ability to communicate effectively across multiple lines of business and various leadership levels on a regular basis.\nJob responsibilities\nAnalyze, design, and implement new operating models\nDesign and deliver procedures and training to support new and existing operating models\nPartner with Global Party Reference Data process leads on current state assessment, defining solutions, testing, and implementing operating models for party creation and maintenance\nIdentify operational synergies with Client Onboarding, Know Your Customer (KYC) and other business stakeholders to eliminate redundant and/or manual processes\nWork with Business and Operations Stakeholder management\nAct as the supportive source of knowledge for all Standard Operating Procedures (SOPs) and internal policy documents\nPartner with Reference Data Strategy, Project, and Technology to implement the future state data models and domains, including defining the target operating model within the Utility\nRequired qualifications, capabilities, and skills\n2+ years of experience in data management, process management, business process reengineering or related field\nProject management and change management experience\nProcess modeling experience in defining new or changes to business and operations processes\nPrior writing experience creating documents, papers, policies, or procedures that are professional, comprehensive, and easily understood\nDemonstrated ability to obtain Subject Matter Expert (SME) knowledge of multiple processes\nExperience creating and presenting business updates\nSolid time management, organizational skills, and effective prioritization skills\nPreferred qualifications, capabilities, and skills\nParty reference data\nClient onboarding\nKnow Your Customer (KYC)\nRegulatory mandates such as MIFID, NCMR, EMIR, CCPA\nABOUT US\n\nJPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world’s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants’ and employees’ religious practices and beliefs, as well as any mental health or physical disability needs.\n\n\n\nABOUT THE TEAM\nThe Corporate & Investment Bank is a global leader across investment banking, wholesale payments, markets and securities services. The world’s most important corporations, governments and institutions entrust us with their business in more than 100 countries. We provide strategic advice, raise capital, manage risk and extend liquidity in markets around the world.\n\n\nOperations teams develop and manage innovative, secure service solutions to meet clients’ needs globally. Developing and using the latest technology, teams work to deliver industry-leading capabilities to our clients and customers, making it easy and convenient to do business with the firm. Teams also drive growth by refining technology-driven customer and client experiences that put users first, providing an unparalleled experience.",
         "Bengaluru",
         "657267.0",
         "/yr (est.)",
         "10000+ Employees",
         "1799",
         "Company - Public",
         "Banking & Lending",
         "Finance",
         "$10+ billion (USD)",
         "4.0",
         "3.9",
         "3.9",
         "3.6",
         "3.7"
        ],
        [
         "74",
         "BookMyShow",
         "Data Analyst - I",
         "3.9",
         "Your Profile\nResponsible for churning out regular as well as ad-hoc reports for multiple business verticals\nWorking on customer segmentation, retention and growth metrics to help CRM/Product teams with relevant target groups for upcoming Movies/Events\nCoordinating with teams to understand business problems and work towards solving them\nShould be able to make sense of large datasets, and derive actionable insights from it\nShould be able to comprehend business problem and solve it using the available data\nShould be able to work with product, technology and marketing teams to identify gaps in the data capture strategy. Implement processes that ensure data consistency and accuracy\nShould be able to identify trends/patterns within the data, draw insights out of it and share actionable steps with business teams\nFlexible to work as an individual contributor as well as a Team player\nYour Checklist\nShould have good understanding on writing SQL queries, joins etc. and also advanced Excel functions\nMinimum 2 years’ experience on data / Product analysis in leading eCommerce and/ or Internet companies\nExperience with Google Analytics\nGood understanding of any data visualisation tool like Tableau/Data Studio/Power BI/Quicksight/Qliksense\nPast experience of working on any data warehouse like Amazon Redshift / Snowflake / BigQuery would be an added advantage\nExperience on Python/R and statistical modelling would be an added advantage",
         "Mumbai",
         "375292.0",
         "/yr (est.)",
         "501 to 1000 Employees",
         "1999",
         "Company - Private",
         "Ticket Sales",
         "Arts, Entertainment & Recreation",
         "Unknown / Non-Applicable",
         "3.8",
         "3.5",
         "3.6",
         "3.5",
         "3.4"
        ],
        [
         "75",
         "Opalforce, Inc.",
         "HR Data Analyst",
         "4.0",
         "Job Title: HR Data Analyst\nLocation: Bangalore\nHigh Level JD:\nCollect, compile, and analyze HR Data for trends with regard to recruitment practices, retention/turnover and apply\nthis data to make recommendatios\nExcellent knowledge of Microsoft Office Suite, and Excel and the ability to createcharts,spreadsheets and presentations\nCompiling reports of data results and presenting these to senior managers.\nA minimum of 2 years experience in similar role.\nAbility to work with cross-functional teams\n\nAbout Opalforce, Inc. :\n\nOpalForce Inc is a renowned Google cloud partner. A global leader with more than 20 years of experience in providing enterprise-level solutions to industries sectors like Healthcare, Entertainment, Retail, Government, Education, and Manufacturing. OpalForce has proven expertise in Google Cloud Platform, Infrastructure Modernization, Cloud Migration, Scaling Production Workloads, Kubernetes on GCP, Production Machine Learning, Gsuite Licenses. OpalForce has started its journey in Santa Clara, California in the year 2000, and associated with Fortune 1000 companies to transform and innovate them with Data and machine intelligence, & Cloud consulting, and engineering.",
         "Bengaluru",
         "529150.0",
         "/yr (est.)",
         "51 to 200 Employees",
         "2001",
         "Company - Private",
         "Information Technology Support Services",
         "Information Technology",
         "Unknown / Non-Applicable",
         "4.5",
         "4.5",
         "4.5",
         "4.5",
         "4.5"
        ],
        [
         "76",
         "Ignitho",
         "Data Analyst Associate",
         "4.3",
         "Location:\nChennai, Tamil Nadu\n\nOpenings:\n5\n\nSalary Range:\n\nDescription:\nRoles and responsibilities:\nAssist in solving data-related problems and challenges within the organization.\nAssist in building and validating predictive models under the guidance of senior analysts or data scientists.\nPerform basic data analysis using tools like Excel, SQL, or statistical software.\nGenerate descriptive statistics and visualizations to understand the data.\nBe open to learning and seeking guidance from experienced professionals.\nEmployment details:\nTraining period : 6 months ( With Stipend )\nThere will be a 2-year employment agreement which they must sign, we will not take any originals of their educational certificates.\nOpen to work in UK Shift (3PM to 12AM)",
         "Chennai",
         "835332.0",
         "/yr (est.)",
         "51 to 200 Employees",
         "2016",
         "Company - Private",
         "Information Technology Support Services",
         "Information Technology",
         "Unknown / Non-Applicable",
         "4.4",
         "4.1",
         "4.3",
         "4.1",
         "4.4"
        ],
        [
         "77",
         "LKQ India",
         "Data Analyst",
         "3.8",
         "Location : Bengaluru\n\nShift Timings : General Shift\nSkills :\nBRD\nrequirement gathering\ndata validation\ndata analyze\nCoordination between User & development team. database data warehousing\nvisualization experience.\nJob Description :\nCollects, analyzes and validates data in the enterprise data warehouse against source systems and data governance policies to ensure proper data availability to business team members.\nWork with business teams and BI team to determine and document requirements for requests or issues related to data, reports, analysis, and training.\nIdentifies opportunities for transitioning ad hoc analysis into standardized reporting and dashboarding.\nCreates procedures and best practices for users of business intelligence tools.\nAssists team leads in team delivery management practices to ensure timely delivery of work to the business.\nCommunicates progress and changes being made to the business intelligence tools to business team members.\nDocuments data governance policies and practices\nAssumes other duties as assigned.",
         "Bengaluru",
         "611782.0",
         "/yr (est.)",
         "10000+ Employees",
         "1998",
         "Company - Public",
         "Automotive Parts & Accessories Stores",
         "Retail & Wholesale",
         "$5 to $10 billion (USD)",
         "3.7",
         "3.6",
         "3.9",
         "3.6",
         "3.9"
        ],
        [
         "78",
         "NEAR",
         "Big Data Analyst",
         "4.3",
         "Description\nYou will be joining Near, one of the fastest-growing Enterprise SaaS companies, and experience a true start-up culture with the freedom to experiment and innovate. At Near, we believe that great culture is not just about work; it’s work + life. We not only encourage our employees to dream big but also give them the freedom and the tools to do so.\nYour responsibilities will include extracting data from various sources, and interpreting and analyzing it to deliver actionable insights. You will also be responsible for presenting it effectively to enable data-driven decision-making through reporting, analysis, and optimization.\nThis will be a work-from-office role, based in our state-of-the-art office in Koramangala, Bangalore.\nA Day in the Life\nAbility to pull out reports from a complex web of Data Lakes, Data Warehouses, and Data Marts.\nDevelop techniques to analyze and work with big data tools and frameworks.\nBuild reusable and optimized code for future use.\nCollaborate with product managers and suggest appropriate solutions for analytics deliverables.\nEnsure all the deliverables are undertaken in a timely manner with zero errors.\nGet feedback and improve on efficiency.\nHelp the team synthesize both quantitative and qualitative data into insights that deepen our understanding of our product performance and user behavior.\nWhat You Bring to the Role\n0 -3 Years of experience in Data warehousing tools and technologies.\nExceptional skills in SQL, Python/ Pyspark.\nExceptional problem-solving, analytical, and organizational skills with an eye for detail.\nPassionate about learning new technologies.\nExperience with any Data Visualization or BI/reporting tool will be a plus.\nWork experience with AWS or any other cloud platform will be a plus.\nExperience in big data will a plus.",
         "Bengaluru",
         "774730.0",
         "/yr (est.)",
         "51 to 200 Employees",
         "2012",
         "Company - Private",
         "Enterprise Software & Network Solutions",
         "Information Technology",
         "Unknown / Non-Applicable",
         "4.2",
         "4.0",
         "4.3",
         "3.8",
         "4.3"
        ],
        [
         "89",
         "Infosys",
         "FHIR Data Analyst",
         "3.8",
         "A day in the life of an Infoscion • As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction. • You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain. • You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews. • You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes. • You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n• At least 1 year of experience in HL7 FHIR implementation. • Deep knowledge of HL7 FHIR 4.0.1 standard • Knowledge of FHIR implementation guides like DaVinci, CARIN, US Core etc. • Experience performing data mapping of Source data sets to FHIR resources • Analyzes the business needs, defines detailed requirements, and provides potential solutions/approaches with the business stakeholders • Strong experience and understanding of Agile Methodologies • Strong written and oral communication and interpersonal skills • Strong analytical, planning, organizational, time management and facilitation Skills • Strong understanding and experience of SDLC and documentation skills • Proficiency in Microsoft Suite (Word, Excel, Access, PowerPoint, Project, Visio, Outlook), Microsoft SQL Studio, JIRA\nnull\nHealthcare->FHIR/ HL 7",
         "Bengaluru",
         "468897.0",
         "/yr (est.)",
         "10000+ Employees",
         "1981",
         "Company - Public",
         "Information Technology Support Services",
         "Information Technology",
         "$10+ billion (USD)",
         "3.8",
         "3.0",
         "4.0",
         "3.5",
         "3.7"
        ],
        [
         "91",
         "Quantiphi",
         "Data Analyst",
         "4.3",
         "While technology is the heart of our business, a global and diverse culture is the heart of our success. We love our people and we take pride in catering them to a culture built on transparency, diversity, integrity, learning and growth.\n\nIf working in an environment that encourages you to innovate and excel, not just in professional but personal life, interests you- you would enjoy your career with Quantiphi!\nRole: Analyst - Sales Operations\nExperience Level: 2 to 4 Years\nWork location: Mumbai or Bangalore\nRole & Responsibilities:\n Analyzing and developing sales operations policies and procedures.\n Working with sales representatives from different regions/practices to confirm process, collect timely\ninputs towards projections and identify opportunities for improvement.\n Assisting in managing compliance program and sales operations help desk.\n Maintaining existing sales reports and designs new reports as needed.\n Participates in the evaluation, selection, and implementation of a decision-support tool.\n Acting as primary liaison on sales force automation projects/trends.\n Coordinating with cross functional teams to timely report revenue forecasts, establishing high levels of\nquality, accuracy, and consistency.\n Assist with maintaining the functional areas of data management, forecasting, contacts, leads,\nopportunities, dashboards and reports, and ensuring data integrity throughout our CRM system.\n Establishing effective analysis of sales force trends and performance in an effort to identify greater\nefficiencies and better manage and understand process bottlenecks and inconsistencies throughout the\nentire sales lifecycle.\n Performing ad hoc analysis for senior management to provide data support for business decisions.\n Evaluating new tools and platforms to improve reporting and sales operations.\nIf you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!",
         "Mumbai",
         "647159.0",
         "/yr (est.)",
         "1001 to 5000 Employees",
         "2013",
         "Company - Private",
         "Business Consulting",
         "Management & Consulting",
         "Unknown / Non-Applicable",
         "4.1",
         "4.0",
         "4.0",
         "4.0",
         "3.5"
        ],
        [
         "94",
         "Agile CRM Inc.",
         "Data Analyst",
         "3.1",
         "Advanced experience using MS SQL, Excel and dashboard reporting is required.\nAdvanced experience with Power BI is required.\nExcellent process-documentation skills.\nDemonstrated analytical and problem-solving skills are a must.\nExperience in data analytics and reporting, market trends/analysis, etc.\nExcellent verbal and written communication skills for effective communication to internal teams\nAbility to manage multiple projects simultaneously, while meeting regular deadlines\nDemonstrated ability to work independently, and take initiative.\nFlexibility to work overtime hours (nights and weekends) when required.\nA minimum of experience in business intelligence and reporting, or a related field.\nExceptional analytical and conceptual thinking skills.\nExperience creating detailed reports and giving presentations.\nA track record of following through on commitments.\nExcellent planning, organizational, and time management skills.\nA history of leading and supporting successful projects.\nJob Type: Full-time\nPay: ₹700,000.00 - ₹900,000.00 per year\nBenefits:\nHealth insurance\nSchedule:\nFixed shift\nMonday to Friday\nMorning shift\nExperience:\ntotal work: 3 years (Required)\nAbility to Commute:\nMadhapur, Hyderabad, Telangana (Required)\nAbility to Relocate:\nMadhapur, Hyderabad, Telangana: Relocate before starting work (Required)\nWork Location: In person",
         "India",
         "800000.0",
         "/yr (est.)",
         "51 to 200 Employees",
         "2013",
         "Company - Private",
         "Enterprise Software & Network Solutions",
         "Information Technology",
         "Unknown / Non-Applicable",
         "2.9",
         "2.8",
         "2.8",
         "3.0",
         "2.9"
        ],
        [
         "97",
         "Leegality",
         "Data Analyst",
         "4.0",
         "This is a remote position.\nCompany Mission\n\n\nLeegality is India’s first Document Infrastructure Platform - a radical new digital way for businesses to complete paperwork (agreements, forms and other legal documents). Over the last 7 years, Leegality has changed the way 2000+ Businesses do their paperwork from large enterprises like HDFC, SBI Cards, Federal Bank, ICICI Lombard, Axis Finance, Tata Capital etc. to high-growth companies like Razorpay, Rupeek, Cars24, Dunzo etc.\n\nTo see our impact on customers click here https://www.leegality.com/case-studies\n\n\nCompany Environment\n\n\nLeegality has an Employee Net Promoter Score of 97 - the highest on xto10x’s eNPS Survey for Q1 2022. The highest among 60+ notable startups. This makes us, arguably, the most employee-loved startup in the country\n\n\nCreating a category-defining company - and changing the way businesses perform a critical function like paperwork - requires powerful marketing that resonates with people.\n\n\n\nRequirements\nResponsibilities:\n\nRespond promptly to data requests from diverse teams and stakeholders.\nAnalyze and interpret complex datasets to extract meaningful insights.\nDevelop and maintain interactive and visually appealing dashboards using Tableau Server.\n\nEnsure dashboards effectively communicate key performance indicators (KPIs) and trends.\n\nCollaborate closely with product, sales, tech, and customer success teams to understand their data needs.\n\nProvide analytical support to aid teams in making data driven decisions.\n\nPresent findings and insights to non-technical stakeholders in a clear and compelling manner.\n\nCommunicate data-driven recommendations to support strategic initiatives.\n\nEngage in CEO-level interactions, offering data insights that contribute to high-level decision-making.\n\nUtilize SQL for efficient data extraction, transformation, and analysis.\n\nEmploy advanced Excel/Google Sheets functionalities for data manipulation.\n\nBring a dynamic and adaptable mindset, leveraging any previous startup experience.\n\n\nTechnical Skills:\n\nTableau Server\n\nSQL\n\nAdvance Excel / Google Sheets.\n\n\n\nBenefits\nRecruitment Process:\n\nOn being shortlisted, you would be contacted for the interview process.\n\nWe further have 3 rounds of interviews.\n\nYour final CTC would be decided on the basis of your skills, experience and final assessment.\n\n\n\nApply directly through our career page: https://careers.leegality.com/jobs/Careers\n\nFor more information about us please visit our:\n\nOur Company and Culture: https://bit.ly/3Iqm5SB\n\nOur Website: www.leegality.com/\n\nOur LinkedIn Page: www.linkedin.com/company/leegality/\n\nJob Information\nIndustry\nIT Services\nRemote Job",
         "India",
         "539530.5",
         "/yr (est.)",
         "51 to 200 Employees",
         "--",
         "Company - Private",
         "Information Technology Support Services",
         "Information Technology",
         "Unknown / Non-Applicable",
         "4.9",
         "4.8",
         "5.0",
         "4.9",
         "4.9"
        ],
        [
         "99",
         "Airtel India",
         "Data Analyst",
         "4.1",
         "About the Company:\nAirtel is a leading telecom solutions provider in the country with over 300m subscribers. Our endeavor to keep building a value chain for our customers gives us various problem statements to solve for. We as an Organization strongly believe in data driven decision making and thrive on solving complex business problems with Analytics & Data Science.\nAbout the Position:\nWe are looking for a Data Analyst to join our Product & Business Analytics team at Airtel .\nThis role will be leading analytics for a division/pod. You will be expected to drive data driven decision making with senior management, own metrics for a pod.\nSolves business/product problems with accuracy and reliability\nCreate and explain data driven insights to a varied set of stakeholders\nChallenge the status quo and shape the analytics roadmap\nDrive clarity and bring forth creative solutions\nResponsibilities:\nDevelop an in-depth understanding of user journeys on Airtel Digital Channels and generate data driven insights & recommendations to help product/business in meticulous decision making\nEnd-to-end ownership of key metrics, work with respective stakeholders to understand areas we need to measure and ensure the needle is moving in the right direction\nDevelop strong hypothesis , execute A/B experiments and identify area of opportunities with strong confidence level\nWork cross-functionally to define problem statements, collect data, build analytical models and make recommendations\nIdentify and implement streamlined processes for data reporting, dashboarding and communication\nCollaborate with Product for data tracking and implementation of tools like Clickstream, Google analytics, Branch, Moengage etc.\nQualifications and Skills Requirements:\nB.Tech/BE degree in Computer Science/related technical field or Statistics/Operations Research background\n2+ years of experience in core business analytics/product analytics\nHands on experience on SQL, Python\nExperience of working on large datasets (big data)\nExcellent with data visualization and strong understanding of Tableau, Superset\nStrong problem-solving skills and should be able to engage senior management with data\nExperience with data structures & feature modelling. Ability to effectively manage and communicate data mart plans to internal customers\nEffective communication skills and experience working across multiple teams will be a plus (data engineering, data analytics, product squads)",
         "Gurgaon",
         "464758.0",
         "/yr (est.)",
         "10000+ Employees",
         "1995",
         "Company - Public",
         "Telecommunications Services",
         "Telecommunications",
         "$10+ billion (USD)",
         "4.0",
         "3.9",
         "3.9",
         "3.7",
         "3.6"
        ],
        [
         "107",
         "Bursys",
         "Data Analyst Tranee",
         "3.7",
         "We are seeking a highly motivated and detail-oriented individual to join our team as a Data Analyst. As a Data Analyst, you will be responsible for collecting, processing, and analyzing data to provide valuable insights that will drive business decision-making.\nKey Responsibilities:\nCollect and gather data from various sources, ensuring data accuracy and completeness.\nClean, preprocess, and organize raw data for analysis.\nCommunicate findings to team members and management effectively.\nMaintain and update databases, ensuring data integrity and security.\nWork closely with cross-functional teams to understand business requirements and goals.\nStay updated on industry trends and advancements in data analytics.\nQualifications:\nBachelor’s degree in a relevant field such as Statistics, Computer Science, or related discipline.\nExcellent communication skills and Knowledge of machine learning concepts and algorithms.\nStrong analytical and problem-solving skills.\nJob Type: Full Time\nJob Location: Panchkula India",
         "Panchkula",
         "529150.0",
         "/yr (est.)",
         "51 to 200 Employees",
         "2005",
         "Company - Private",
         "Information Technology Support Services",
         "Information Technology",
         "Unknown / Non-Applicable",
         "3.7",
         "3.5",
         "3.5",
         "3.5",
         "3.5"
        ],
        [
         "108",
         "Tata Consultancy Services",
         "Data Analyst",
         "3.8",
         "Job Description\nJob Description\nDear Candidate\nGreetings from TCS !!!\n\nWe are looking for SAS SQL- DATA ANALYST\n\nExperience Range: 2-6 Years\nLocation: Chennai,Kolkata\n\nDesired Competencies (Technical/Behavioral Competency)\n\n1. Advanced/Expert SAS/SQL,\n2. Advanced/Expert Data Analysis,\n3. Advanced/Expert Excel\n\n1. Advanced Critical Thinking\n2. Intermediate Knowledge of Pharmacy Claims & Drug Data\n\nDesired Candidate Profile\nQualifications :BACHELOR OF TECHNOLOGY",
         "Chennai",
         "398701.0",
         "/yr (est.)",
         "10000+ Employees",
         "1968",
         "Company - Public",
         "Software Development",
         "Information Technology",
         "$10+ billion (USD)",
         "3.7",
         "3.1",
         "3.9",
         "3.3",
         "3.8"
        ],
        [
         "110",
         "ICON",
         "Lab Data Analyst I",
         "3.8",
         "At ICON, it’s our people that set us apart. Our diverse teams enable us to become a better partner to our customers and help us to fulfil our mission to advance and improve patients’ lives.\n\nOur ‘Own It’ culture is driven by four key values that bring us together as individuals and set us apart as an organisation: Accountability & Delivery, Collaboration, Partnership and Integrity. We want to be the Clinical Research Organisation that delivers excellence to our clients and to patients at every touch-point. In short, to be the partner of choice in drug development.\n\nThat’s our vision. We’re driven by it. And we need talented people who share it.\nIf you’re as driven as we are, join us. You’ll be working in a dynamic and supportive environment, with some of the brightest and the friendliest people in the sector, and you’ll be helping shape an industry.\n\nJOB-DESCRIPTION\nRecognize, exemplify and adhere to ICON's values which center around our commitment to People, Clients and Performance.\nAs a member of staff, the employee is expected to embrace and contribute to our culture of process improvement with a focus on streamlining our processes adding value to our business and meeting client needs.\nTravel (approximately <10%) domestic and/or international.\nMain contact for the Sponsors in all matters regarding our laboratories Global Data Services team.\nSign off on behalf of all ILS Global Data Services on study related agreements.\nAttend all meetings as required.\nPerform review of client specifications/requirements and offer suggestions and solutions that are agreeable to both the client and ILS Global Data Services team.\nManage and perform setup of each study to ensure that there are no issues when it comes to creating, QC’ing, and transmitting protocol specific data files in accordance with client specifications and departmental procedures.\nCommunicate directly with external clients to ensure that study goals are met. Complete GDS paper work and documentation in accordance with GDS SOP’s. (Data Export Agreement)\nConfigure database settings to ensure proper data content (data mapping).\nPartner with the Project Management team and external clients to add, remove, update and troubleshoot ICOLABS users and user accounts for various protocols in according to GDS ICOLABS SOP’s.\nWork within the ICOLABS system to add, remove; update users and user accounts for different protocols.\nTrain other Global Data Services staff as needed.\nPerforms additional relevant responsibilities as requested by management.\n\nTo perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\nKnowledge of Microsoft Office.\nExcellent oral and written communication skills.\nExperience with databases, medical terminology, clinical trials, or the ICOLIMS, Business Objects systems preferred.\nAbility to deal with high stress, excellent organizational skills, ability to multitask, good communication skills, and to be detail oriented.\nAll requirements for the Lab Data Analyst in-training position have been completed. (Or an alternative combination of experience, education, and training determined by management to be equivalent to the foregoing)\n\nBenefits of Working in ICON:\nOur success depends on the quality of our people. That’s why we’ve made it a priority to build a culture that rewards high performance and nurtures talent.\n\nWe offer very competitive salary packages. And to keep them competitive, we regularly benchmark them against our competitors. Our annual bonuses reflect delivery of performance goals – both ours and yours.\nWe also provide a range of health-related benefits to employees and their families and offer competitive retirement plans – and related benefits such as life assurance – so you can save and plan with confidence for the years ahead.\nBut beyond the competitive salaries and comprehensive benefits, you’ll benefit from an environment where you are encouraged to fulfil your sense of purpose and drive lasting change.\n\nICON is an equal opportunity and inclusive employer and is committed to providing a workplace free of discrimination and harassment. All qualified applicants will receive equal consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.\nIf, because of a medical condition or disability, you need a reasonable accommodation for any part of the application process, or in order to perform the essential functions of a position, please let us know through the form below.",
         "Bengaluru",
         "421485.0",
         "/yr (est.)",
         "10000+ Employees",
         "1990",
         "Company - Public",
         "Biotech & Pharmaceuticals",
         "Pharmaceutical & Biotechnology",
         "$2 to $5 billion (USD)",
         "3.6",
         "3.5",
         "3.7",
         "3.4",
         "3.7"
        ],
        [
         "111",
         "Nineleaps Technology Solutions Pvt Ltd",
         "Data Analyst",
         "4.2",
         "Nineleaps is a boutique technology-consulting firm, that helps funded ventures and enterprises accelerate their product development and data efforts. We are 450+ people strong and based out of Bangalore, India. Over the past 8 years, our community of engineers has delivered over 200 intuitive and pragmatic solutions to our clients’ more complex challenges. We have gained multiple levels of expertise by working with market leaders, technology giants, and the latest disruptors of many industries such as Retail, e-Business, Advertising, Finance, Transportation, Healthcare, and Education.\nWe are looking for skilled Data Analysts who can analyze data and derive data insights to assist organizations in making decisions.\nPrimary Responsibilities:\nEnhancing data collection procedures\nProcessing, cleansing & verifying the integrity of data\nUtilizing cloud databases & on-prem Microsoft databases\nExamine and correct data that has been corrupted\nStatistical analysis\nWork with management and technical teams to establish essential metrics and KPIs, as well as their analytical requirements\nData visualization\nDeveloping business intelligence reports & dashboards\nProvide actionable information to various stakeholders to assist them in making well-informed decisions\nDetermine and implement data acquisition and integration logic to guarantee optimal performance and scalability of the systems\nDesired Skills and Experience:\nStrong understanding of data mining, data models, segmentation techniques, and database design development\nAbility to structure, analyze, and extract data according to business requirements\nStrong SQL Experience, Power BI / Tableau / Google Data Studio\nApplied statistics or experimentation (i.e. A/B testing) experience in an industry setting\nExperience with programming languages and frameworks such as Python, JavaScript, XML, and ETL\nStrong analytical abilities, including the ability to gather, organize, analyze, and distribute large volumes of data with precision and attention to detail\n(Or) Send in your resumes to jobs@nineleaps.com\nOverview\nLocation\nBangalore\nExperience\n1 to 4 years",
         "Bengaluru",
         "572576.0",
         "/yr (est.)",
         "201 to 500 Employees",
         "2014",
         "Company - Private",
         "Information Technology Support Services",
         "Information Technology",
         "Unknown / Non-Applicable",
         "4.1",
         "3.8",
         "4.0",
         "3.8",
         "3.8"
        ],
        [
         "117",
         "UBS",
         "Data Analyst",
         "3.9",
         "India\nBusiness development, Business management, administration and support, Risk\nGroup Functions\nJob Reference #\n284924BR\nCity\nHyderabad\nJob Type\nFull Time\nYour role\nDo you have strong analytical skills? Are you familiar with statistical concepts and able to transform data into bespoke content? Do you want to help us expand our data platform?\n\nWe’re looking for a data analyst to:\n\nprovide analytics and reporting support on a broad range of risk and operational data for our business and senior management\nassess and enhance the data collection procedure and automate reporting processes using SQL, VBA\nTransform, visualize data and create dashboards using Excel, Alteryx, Power BI & Tableau\nrun the necessary operational and data governance services required for data integrity\nenhance existing data repository to ensure stakeholders’ efficient access to data\nresearch and explore new data sources and analytics tools to continuously enhance our solutions\nYour team\nYou’ll be working in the Metrics and Reporting team in Hyderabad. Our team works closely with our business stakeholders, Automation and IT teams to support the availability and analytics of high quality operational and business data. As a data analyst, you'll play an important role in assessing the business and operational risks and overseeing the reporting of issues and their sustainable remediation.\nYour expertise\nideally 2-3 years’ relevant experience of working in data analysis in financial services, data operations, data platforms\nstrong analytical and coding skills, familiar with programming languages such as Python\ninterested in digitalization and have an affinity with technology like Power BI, Alteryx Tableau, SQL, Data science skills\nbachelor’s/master’s degree in finance, business administration or related field\nHand on experience in verifying data accuracy, using statistical tools to analyze complex data to interpret trends and patterns and creating data systems and reports for management.\ninquisitive, able to challenge effectively and manage conflicting stakeholder needs\nself-driven and focused on the details, with a good understanding of the banking industry\nstrong communicator, with a collaborative personality and the ability to complete tasks independently\nHow we hire\nThis role requires an assessment on application. Learn more about how we hire: www.ubs.com/global/en/careers/experienced-professionals.html",
         "Hyderābād",
         "727518.0",
         "/yr (est.)",
         "10000+ Employees",
         "1862",
         "Company - Public",
         "Investment & Asset Management",
         "Finance",
         "$10+ billion (USD)",
         "3.6",
         "3.5",
         "3.9",
         "3.5",
         "3.8"
        ],
        [
         "119",
         "S&P Global",
         "Data Analyst",
         "4.1",
         "About the Role:\nGrade Level (for internal use): 08\nThe Rol e: Data Analyst\n\nThe Team : You will be part of the Data Team, which deals with the collection, processing, and analysis of data from the energy and agriculture industries. The team provides and maintains accurate, complete, and timely datasets, taking into account the global interests and needs of business units internally and externally.\n\nThe Impact : We provide the highest quality content that is essential for our clients to make decisions with conviction. As a Data Analyst, you will use our internal tools to support the integrity and comprehensiveness of the data set obtained from internal and external public research sources, such as government and regulatory documents, industry journals, and analyst reports.\n\nResponsibilities:\nWork with SQL and Python tools to solve problems and ensure database quality\nCollecting, analyzing, extracting, and entering high-quality data (financial and non-financial data) into work tools in accordance with the guidelines' criteria\nFilter Data by reviewing reports and performance indicators to identify and correct code problems\nHave a solid working understanding of the processes, the dataset's operation, and the available work tools\nProviding input and ideas for new collection methods and product enhancements related to the dataset\nAchieve predetermined individual and team objectives, including providing results at the highest level.\nWhat We’re Looking For:\nBachelor’s degree in information technologies, computer science or equivalent\nA drive to learn and master new technologies and techniques\nStrong mathematical & numeracy skills.\nStrong analytical and communication skills\nGood understanding of Python and SQL languages\nIntermediate understanding of databases such as SQL Server or Snowflake\nUnderstanding of reporting & data visualization tools such as PowerBI and Tableau.\nUnderstanding of ETL framework and ETL tools (e.g. Alteryx)\nProblem-solving skills\nStrong attention to detail\nAbility to work in an agile team environment\nFlexible Working: We pride ourselves on our agility and diversity, and we welcome requests to work flexibly. For most roles, flexible hours and/or an element of remote working are usually possible. Please talk to us at interview about the type of arrangement that is best for you. We will always try to be adaptable wherever we can.\n\nReturn to Work: Have you taken time out for caring responsibilities and are now looking to return to work? As part of our Return to Work initiative (link to career site page when available), we are encouraging enthusiastic and talented returners to apply, and will actively support your return to the workplace.\n\nAbout S&P Global Commodity Insights\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\nWe’re a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights’ coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world’s foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world’s leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit http://www.spglobal.com/commodity-insights .\n\nWhat’s In It For You?\n\nOur Purpose:\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology–the right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence®, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\nWe're more than 35,000 strong worldwide—so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We’re committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We’re constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values:\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits:\nWe take care of you, so you can take care of business. We care about our people. That’s why we provide everything you—and your career—need to thrive at S&P Global.\n\nOur benefits include:\nHealth & Wellness: Health care coverage designed for the mind and body.\n\nFlexible Downtime: Generous time off helps keep you energized for your time on.\n\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\n\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\n\nFamily Friendly Perks: It’s not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\n\nBeyond the Basics: From retail discounts to referral incentive awards—small perks can make a big difference.\nFor more information on benefits by country visit: https://www.spglobal.com/en/careers/our-culture/\n\nDiversity, Equity, and Inclusion at S&P Global:\nAt S&P Global, we believe diversity fuels creative insights, equity unlocks opportunity, and inclusion drives growth and innovation – Powering Global Markets. Our commitment centers on our global workforce, ensuring that our people are empowered to bring their whole selves to work. It doesn’t stop there, we strive to better reflect and serve the communities in which we live and work, and advocate for greater opportunity for all.\n\n-----------------------------------------------------------\n\nEqual Opportunity Employer\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person.\n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law.\n\n----------------------------------------------------------- ANLYTC203 - Entry Professional (EEO Job Group)\n\nJob ID: 293364\nPosted On: 2024-01-01\nLocation: Bangalore, Karnataka, India",
         "Bengaluru",
         "539530.5",
         "/yr (est.)",
         "10000+ Employees",
         "1860",
         "Company - Public",
         "Research & Development",
         "Management & Consulting",
         "$10+ billion (USD)",
         "3.8",
         "3.8",
         "4.2",
         "3.8",
         "4.2"
        ],
        [
         "122",
         "Kovai.co",
         "Data Analyst",
         "4.5",
         "Kovai.co, the fastest growing SaaS organization is a premier enterprise software company offering multiple products at scale both in the enterprise arena and in the B2B SaaS space.\nWe are a technology partner of choice for many of the world’s leading enterprises to manage and monitor their Microsoft BizTalk and Azure Serverless environments.\n\nWe always stand out from the crowd with our product team consisting of thinkers and innovators who are redefining the way robust Enterprise Software and SaaS products are built. Headquartered in London, U.K and with a development center in Coimbatore, India, our engineers have niche skills and in-depth domain knowledge.\n\nTrusted by over 1,000+ businesses around the world.\n\nKey Products:\n\nBizTalk360\nServerless360\nDocument360\nChurn360\nKovai.co Awards & Recognitions:\n\nKovai.co wins the title “Bootstrap Champ” at The Economic Times Startup Awards 2021.\nWinner of Bootstrapped SaaS start-up of the year by SaaS BOOMi.\nNASSCOM Recognizes Document360 at Emerge 50 Awards 2021.\n\nWe wish to be known for our values of integrity, teamwork and excellence. As we grow, we ensure that our culture remains at the heart of Kovai.co.\nWe are constantly on the lookout for smart people who are passionate about building great products, designing great experiences, building scalable platforms, and making customers happy. If you’re looking to make an impact, Kovai.co is the place for you. If this describes you, feel free to have a look at our openings in our career page and apply to be a part of the $30 million journey!\n\nOpportunity: Data Analyst\n\nWhat you’ll do on the job:\n\nCollecting and interpreting data, analysing results, identifying patterns and trends in data sets.\nSegregation, classification, and analysis of data.\nWriting reports based on insights gained from data analysis\nWriting Python code for achieving the business outcomes\nBuilding visualisations and metrics for showcasing business metrics\nPerforming data quality checks for quality assurance\nDocumenting data analysis, business logics and metrics methodology\nSupports innovative analytical thinking & solutions that results into improved business performance.\n\nWho’ll be a good fit:\n\nProven work experience of 1-2 years as a Data Analyst.\nStrong in mathematics - statistics, probability, algebra, set theory, time series, calculus, graph theory, etc.\nExperience with data wrangling, web scraping, data schema preparation\nData extraction, Data cleaning, Data cleansing, Data transformation, Report building and automation.\nProficient in SQL, Python coding & BI Tool such as Tableau ,PowerBI\nWrite scalable scripts to fetch or modify data from API endpoints.\nExcellent critical thinking, verbal, and written communication skills.\nData science background is an advantage, Exposure to cloud technology is preferred\n\nPerks:\n\nCollaborative and fun team.\nFlat organizational structure.\nRewards and recognition.\nHealth care benefits.\nUpskill allowance.\nLocated at the heart of the city with world class infrastructure\n\nIf this excites you, apply for this opportunity, and the team would love to get on a call with you to discuss further.",
         "Coimbatore",
         "438621.0",
         "/yr (est.)",
         "51 to 200 Employees",
         "2010",
         "Company - Private",
         "Enterprise Software & Network Solutions",
         "Information Technology",
         "Unknown / Non-Applicable",
         "4.6",
         "4.1",
         "4.3",
         "4.5",
         "4.5"
        ],
        [
         "130",
         "Hudson's Bay Company",
         "Jr. Data Analyst",
         "3.6",
         "Job Description\nDay in the Life:\nThe Data Analyst will be part of a dynamic team that supports the execution of initiatives in Data Analytics. They work cross functionally with teams to develop and deliver projects that will drive success at The Bay. They will interact with project stakeholders, business leaders and collaborate with a team of data experts to leverage data and perform deep dive analyses and build data/reporting products. They will explore new insights and innovation by leveraging advanced analytics to derive data backed insights. This is the perfect role that provides an opportunity to gain further advanced experience in retail business initiatives and understanding of retail analytics and insights.\n\nWhat You Will Do:\nAssist in defining project scope and objectives and developing plans to monitor and track progress.\nCommunication of the analysis processes and insights to stakeholders and other analytics teams across the organization.\nPartner and align with stakeholders across the banners to brainstorm, test and develop analysis that delivers quick insights to confirm or deny business hypotheses.\nProactively identify analytical opportunities that will lead to improved business results\nActively contribute to enhancing the analytics team’s foundational and scalable processes and best practices for data extraction, manipulation, analysis, provision of insights, and the mechanism to drive actions or decision making\nParticipate in defining analytic requirements for new projects and contribute to the delivery of new dashboards and reports.\nProvide ad-hoc analysis support\n\nWhat You Will Need:\nUniversity degree required, preferred in Statistics, Business Analytics, or related field\nExperience in Business Intelligence/Visualization experience\n2+ year in SQL and/or Python\nStrong data mining and analytical skills and able to translate findings into clear, actionable insights and recommendations\nGood communication skills with the ability to explain technical concepts to a diverse audience in business terms.\nAbility to “storytell” through analytics and presentation materials\nCreative thinker and solutions oriented, committed to driving business improvements\nAbility to handle multiple demands and competing priorities.\nProactive approach to problem-solving, working collaboratively, and supporting the team.\nAbility to be flexible and adaptable while operating independently and as part of a team.\nJob Qualifications\nThe Fabric of Hudson’s Bay\nHudson's Bay has established a reputation for quality and style through an unrivaled assortment of products and categories including fashion, home, beauty, food concepts, and more. Hudson's Bay operates under the HBC brand portfolio. Founded in 1670, HBC is North America's oldest company. Hudson’s Bay helps Canadians live their best style of life by operating thebay.com featuring Marketplace, one of the largest premium life & style digital platforms in Canada, with a seamless connection to a network of Hudson’s Bay stores from coast to coast.\nAt Hudson’s Bay, smart, high-performing team members will challenge you to learn and grow every day.\nWe’d love for you to join us in our mission to help Canadians live their best style of life! Stay connected with us on Instagram, Facebook, Twitter, TikTok, and LinkedIn\nThank you for your interest In Hudson’s Bay. We look forward to reviewing your application.\nAs an equal opportunity employer, Hudson’s Bay is committed to providing you with a barrier-free, inclusive and accessible workplace to lead a brilliant career.",
         "Bengaluru",
         "539530.5",
         "/yr (est.)",
         "10000+ Employees",
         "1670",
         "Company - Public",
         "Department, Clothing & Shoe Stores",
         "Retail & Wholesale",
         "$10+ billion (USD)",
         "3.2",
         "3.1",
         "3.4",
         "3.1",
         "3.5"
        ],
        [
         "132",
         "iXceed Solutions",
         "Data Analyst",
         "4.7",
         "Job Title: Data Analyst (Product Development domain)\nLocation: Gurugram, Haryana\nRole Type: Permanent (Onsite)\nMust Required:\nData Analyst should be from Product Development domain\nOne should have an idea of Data Engineering as well, as in Knowledge of Data Development using Python etc.\nJob Description:\nCompany Overview: Veera is a dynamic and innova/ve browser company commi3ed to\nleveraging data-driven insights to op/mize decision-making and drive business success. We\nare seeking a skilled and mo/vated Data Analyst to join our growing team and contribute to\nthe company's strategic objec/ves.\nPosi9on Overview: As a Data Analyst at Veera, you will play a crucial role in transforming\nraw data into ac/onable insights that inform key business decisions. You will work closely\nwith cross-func/onal teams to gather, analyze, and interpret data, providing valuable\nrecommenda/ons that enhance our overall performance and compe//veness in the market.\nResponsibilities:\nData Collection and Processing:\nCollect, clean, and pre-process large datasets from various sources to ensure data accuracy\nand reliability.\nImplement data quality checks and valida/on procedures to maintain the integrity of our\ndata.\nData Analysis:\nPerform exploratory data analysis to iden/fy trends, pa3erns, and correla/ons in the data.\nConduct sta/s/cal analyses to extract meaningful insights and provide data-driven\nrecommenda/ons.\nReporting and Visualization:\nCreate and maintain dashboards and reports to communicate key performance indicators\nand trends to stakeholders.\nU/lize visualiza/on tools (e.g., ReDash, Tableau, Power BI) to present complex data in an\naccessible and understandable format.\nCollaboration:\nWork closely with cross-func/onal teams to understand business requirements and provide\nanaly/cal support for decision-making processes.\nCollaborate with Engineering and other func/ons to ensure data infrastructure meets\nanaly/cal needs.\nContinuous Improvement:\nStay abreast of industry trends, best prac/ces, and emerging technologies in data analysis.\nPropose and implement improvements to exis/ng processes to enhance the efficiency and\neffec/veness of data analysis ac/vi/es.\nRequirements:\nDegree in Data Science, Sta/s/cs, Mathema/cs, Computer Science, or a related field.\n3-5 years of proven & relevant experience as a Data Analyst or in a similar role.\nStrong proficiency in data analysis tools and programming languages (e.g., SQL, Python).\nExperience with data visualiza/on tools (e.g., ReDash, Tableau, Power BI).\nExposure & Experience with Data Engineering is an added advantage\nSolid understanding of sta/s/cal concepts and methods.\nExcellent communica/on and presenta/on skills.\nA3en/on to detail and the ability to work independently as well as collabora/vely within a\nteam.\nJob Types: Full-time, Permanent\nSalary: ₹1,500,000.00 - ₹2,500,000.00 per year\nApplication Question(s):\nOne should have an idea of Data Engineering as well, as in Knowledge of Data Development using Python?\nExperience:\ntotal work: 5 years (Required)\nProduct Development domain: 2 years (Required)\nProduct Analyst: 2 years (Required)\nSQL: 1 year (Required)\nPython: 3 years (Required)\nData visualization: 2 years (Required)\nB2C: 2 years (Required)\nMobile applications: 2 years (Required)\nWork Location: In person",
         "Gurgaon",
         "2000000.0",
         "/yr (est.)",
         "501 to 1000 Employees",
         "2012",
         "Company - Private",
         "Staffing, Recruitment & Subcontracting",
         "Human Resources & Staffing",
         "$5 to $25 million (USD)",
         "4.7",
         "4.4",
         "4.7",
         "4.6",
         "4.7"
        ],
        [
         "135",
         "kaleidofin",
         "Data Analyst",
         "3.4",
         "Who we are?\nKaleidofin is a fintech with a focus on the informal sector, we provide solutions tailored to the customer’s goals and are intuitive to use. We are working towards creating fair and transparent financial solutions that can target millions of customers and enterprises in India that don’t have easy access to formal financial planning.\n‍\nIn a very short time span, global investors such as Oiko Credit, Flourish, Omidyar Network andBlume Ventures have supported Kaleidofin’s well thought out business model. The firm’s latest round was Series B of funding led by The Dell Foundation & The Bill & Melinda Gates foundation inMay 2022. Kaleidofin is recertified as a Great Place to Work from September’22.\n‍\nThe company won the Amazon AI Conclave award for Fintech, was one of only ten startups chosen for the Google LaunchPad Accelerator program in 2019, was recognized as India’s Most InnovativeWealth, Asset and Investment Management Service/Product by the Internet & Mobile Association of India (IAMAI) and was selected to present at United Nations General Assembly Special Task ForceEvent.\n‍\nWith its focus to harness mobile technology to deliver a paperless experience as well as its focus to harness technology and analytics to predict the right product for the right customer, Kaleidofin aims to become a leading FinTech player bringing financial solutions to everyone.\n‍\nTo know more about Kaleidofin, do visit our site https://kaleidofin.com\nWhat you’ll do?\nWe are looking for a developer to design and deliver strategic data-centric insights leveraging the next generation analytics and BI technologies. We want someone who is data-centric and insight-centric, less report centric. We are looking for someone wishing to make an impact by enabling innovation and growth; someone with passion for what they do and a vision for the future.\nBe the analytical expert in Kaleidofin, managing ambiguous problems by using data to execute sophisticated quantitative modeling and deliver actionable insights.\nDevelop comprehensive skills including project management, business judgment, analytical problem solving and technical depth.\nBecome an expert on data and trends, both internal and external to Kaleidofin.\nCommunicate key “state of the business” metrics and develop dashboards to enable teams to understand business metrics independently.\nCollaborate with stakeholders across teams to drive data analysis for key business questions, communicate insights and drive the planning process with company executives.\nAutomate scheduling and distribution of reports and support auditing and value realization.\nPartner with enterprise architects to define and ensure proposed Business Intelligence solutions adhere to an enterprise reference architecture.\nDesign robust data-centric solutions and architecture that incorporates technology and strong BI solutions to scale up and eliminate repetitive tasks.\nLocation: Chennai/Bangalore, mandatory work from office - hybrid\n‍\nWho you need to be?\nExperience leading development efforts through all phases of SDLC.\n3+ years experience designing Analytics and Business Intelligence solutions and working on developing meaningful visualizations.\nAbility to handle complex data structures and have strong command on Writing Complex DAX function, Power Query, Power Apps.\nExperience in Building Power BI Data Model, Dashboard & Reports using various Chart, Slicer, Pivot and Custom Visuals.\nStrong command on Power bi / Tableau is a must.\nHands on experience in SQL, data management, and scripting (preferably Python).\nStrong data visualization design skills, data modeling and inference skills.\nHands-on and experience in managing small teams.\nFinancial services experience preferred, but not mandatory.\nStrong knowledge of architectural principles, tools, frameworks, and best practices.\nExcellent communication and presentation skills to communicate and collaborate with alllevels of the organization.\nIf you fit the bill, write to me at\ncareers@kaleidofin.com\nJob location\nChennai/Bangalore",
         "Chennai",
         "539530.5",
         "/yr (est.)",
         "51 to 200 Employees",
         "2017",
         "Company - Private",
         "Investment & Asset Management",
         "Finance",
         "$5 to $25 million (USD)",
         "3.1",
         "2.7",
         "2.7",
         "3.0",
         "2.4"
        ],
        [
         "140",
         "Simplex Services",
         "Data Analyst/Power BI Reporting",
         "4.0",
         "Simplex requires a Power BI Developer to build and maintain BI and analytics solutions that transform data into knowledge.\nThe right candidate must have business understanding and problem-solving skills with experience in data and business analysis. The candidate should also have good analytical ability alongside excellent communication skills.\nEmployment Type: Full-time\nRoles & Responsibilities:\nPower BI Report development.\nAnalyse data and display it in reports to aid decision-making.\nUse SQL queries to get the best results.\nFor a better understanding of the data, use filters and visualizations.\nConvert business needs into technical specifications and establish a timetable for job completion.\nAnalytical thinking for translating data into informative reports and visuals.\nConnecting data sources, importing data, and transforming data for Business intelligence.\nBuilding Analysis Services reporting models.\nShould be able to develop tabular and multidimensional models that are compatible with data warehouse standards.\nVery good communication skills must be able to discuss the requirements effectively with the client teams, and with internal teams.\nGood to have:\nYou are an ambitious and self-motivated professional.\nYou have a healthy competitive nature and always strive to do your best.\nYou are committed to learning every day and take personal pride in the work that you deliver.\nYou are calm under pressure, and you have a strong work ethic.\nMulti-tasking comes easily to you, and you do not need to learn how to manage your time effectively.\nMust have:\nShould be proficient in software development.\nMandate to have experience with BI tools and systems such as Power BI, Tableau, and SAP.\nKnowledge of Microsoft BI Stack.\nPrior experience in data-related tasks.\nMastery of data analytics .\nBe familiar with MS SQL Server BI Stack tools and technologies, such as SSRS and TSQL, Power Query, MDX, PowerBI, and DAX .\nAnalytical thinking for converting data into relevant reports and graphics.\nCapable of enabling row-level data security.\nKnowledge of Power BI application security layer models .\nProficient in doing advanced-level computations on the data set.\nSimplex-Services, headquartered in Brighton, formed in 2014, specialises in information technology consulting and the implementation of IT services across all industries. Our agile, dynamic teams of IT and business strategy enablers allow us to cut across the limitations of large-scale organisations by delivering services and solutions with speed, precision and flexibility. With over 100 years of corporate experience amongst our founding members, we offer our clients immediate advice and tailored solutions to your strategy. With a fresh approach to IT and practical awareness that any solution needs to be end user-centric and aligned to a strategy, Simplex Services have the capabilities to resolve small- and large- scale projects, whether in-house or through smart source frameworks.\nShare your resume at careers@simplex-services.com, if interested.\nJob Location: UK & India",
         "India",
         "539530.5",
         "/yr (est.)",
         "1 to 50 Employees",
         "2014",
         "Company - Private",
         "Enterprise Software & Network Solutions",
         "Information Technology",
         "Unknown / Non-Applicable",
         "4.6",
         "4.5",
         "4.9",
         "5.0",
         "4.7"
        ],
        [
         "153",
         "iCRC",
         "BI Data Analyst",
         "3.9",
         "Duties and Responsibilities include but are not limited to:\n\nWhat will you be doing?\n\nInteract with internal and client teams to identify questions and issues for data analysis and experiments.\nDevelop and code software programs, algorithms and automated processes to cleanse, integrate and evaluate large data sets from multiple disparate sources.\nWork with massive and complex data sets from multiple sources, utilising big data tools and techniques to analyse, provide insight and validate hypotheses.\nPerforming deep dive analyses of experiments through reliable modelling methods that include numerous explanatory variables and covariates.\nTranslating analytical insights into concrete, actionable recommendations for business, process or product improvements.\nMaking recommendations for the collection of new data or the refinement of existing data sources and storage.\nCollaborate with subject matter experts to deliver and solve real-world engineering problems with ML and data science.\nCollaborate across the business to influence change and bring ideas to life.\nCreate solutions that will benefit the wider community.\nContribute to creating a more sustainable future.\n\nWhat we'll love about you\n\nMinimum Educational Qualification- Engineering Graduate\n1 - 5 years' experience in a data science environment (experience may be corporate, research/government or academia)\nAbility to manipulate and analyse complex, high-volume, high dimensionality data and metadata from varying sources.\nPassion for empirical research and for answering hard questions with data.\nKnowledge of analytical tools and big data technologies (Map/Reduce, Hadoop, Hive, etc).\nFamiliarity with relational/non-relational data manipulation, machine learning, and scientific statistical analysis.\nAbility to communicate complex quantitative analysis in a clear, precise, and actionable manner.\nFlexible analytical approach that allows for results at varying levels of precision.\nSolid understanding and experience with programming logic and various paradigms.\nGood Communication Skills",
         "Bengaluru",
         "460906.0",
         "/yr (est.)",
         "10000+ Employees",
         "1863",
         "Non-profit Organisation",
         "Civic, Welfare & Social Services",
         "Non-profit & NGO",
         "Unknown / Non-Applicable",
         "3.3",
         "3.6",
         "3.8",
         "3.1",
         "3.5"
        ],
        [
         "157",
         "Cargill",
         "Data Analyst - Price hub",
         "4.1",
         "Want to build a stronger, more sustainable future and cultivate your career? Join Cargill's global team of 155,000 employees who are committed to safe, responsible and sustainable ways to nourish the world. This position is in Cargill’s food ingredients and bio-industrial business, where we anticipate trends around taste, nutrition and safety to innovate and provide solutions to manufacturers, retailers and foodservice companies.\nJob Purpose and Impact\n\nThe Data Analyst - Price hub is responsible for delivering robust data support along with analytic inputs in business requirements and data management activities to support different business units within Cargill. In this role, you will contribute to the continuous process improvement on price data management, standardization and harmonization and you will be actively involved in developing and implementing action plans on business performance optimization. Key Accountabilities\n\nDeploy a basic understanding of the trading questions that data can help to solve. Mapping upstream data source fields to Price Hub attributes; along with formulating transformation rules for new/derived columns in mapping\nWork with data engineers to give them the requirement for them to create the data output flow. Test the output data within Price Hub to validate if it is as per the said rules and check for anomalies and relay them back to data engineers for corrections. Create documentation for the newly mapped data source for consumers for easy understanding and create adhoc SQL queries for consumers based on their requirements. Handle basic issues and problems under direct supervision, while escalating more complex issues to appropriate staff.\nOther duties as assigned Qualifications\n\nMinimum Qualifications\nBachelor's degree in a related field or equivalent experience\nMinimum of two years of experience in a related field Experience in MS office suite (Outlook, Excel, Word, Power Point) and entry level SQL\nPreferred Qualifications Experience in managing and planning pricing metadata such as data mining and analysis of price types such as exchange futures prices, options, cash, bids and offer price etc. Ability to make case across and help people learn the basic of data connection to web-based portal to power excel via Oracle Database Connectivity (ODBC)",
         "Gurgaon",
         "612372.0",
         "/yr (est.)",
         "10000+ Employees",
         "1865",
         "Company - Private",
         "Crop Production",
         "Agriculture",
         "Unknown / Non-Applicable",
         "3.8",
         "3.9",
         "4.1",
         "3.6",
         "3.8"
        ],
        [
         "159",
         "Amber Internet solutions",
         "Data Analyst",
         "4.3",
         "Data\nData Analyst\nPune, Maharashtra | Full Time\n\nAbout Amber\nLong-term accommodation booking platform for students (think booking.com for student housing). Amber helps 80M students worldwide, find and book full-time accommodations near their universities, without the hassle of negotiation, non-standardized and cumbersome paperwork, and broken payment process.\nWe are the leading student housing platform globally, with 1M student housing units listed in 6 countries and across 80 cities. We are growing rapidly and targeting $2B in annual gross bookings value by 2024.\n\nIf you are passionate about making international mobility and living, seamless and accessible, then - Join us in building the future of student housing!\n\nRoles and Responsibilities Include:-\nDeveloping and maintaining databases, data systems\nPerforming analysis to assess the quality and meaning of data\nGenerate dynamic dashboards and reports\nIdentify, analyze, and interpret patterns and trends in complex data sets that could be helpful for the diagnosis and prediction\nAssigning numerical value to essential business functions so that business performance can be assessed and compared over periods.\nAnalyzing local, national, and global trends that impact both the organization and the industry Preparing reports for the management stating trends, patterns, and predictions using relevant data\n\nSkills required:-\nMinimum 2-3 years of proven experience in Data analytics.\nStrong mathematical skills to help collect, measure, organize, and analyze data.\nKnowledge and hands-on experience with programming languages like SQL and Python.\nExperience in ETL framework. Knowledge of data visualization software like Tableau, Metabase, etc.\nKnowledge of AWS Data-warehouse- Redshift is a plus.\nProblem-solving skills\nAccuracy and attention to detail\nGood verbal and Written communication skills\n\nHow to apply:-\nReach out to aarti.s@amberstudent.com\nor visit the following link - amberstudent.com/careers",
         "Pune",
         "494268.0",
         "/yr (est.)",
         "501 to 1000 Employees",
         "2016",
         "Company - Private",
         "Internet & Web Services",
         "Information Technology",
         "$5 to $25 million (USD)",
         "4.2",
         "3.9",
         "4.3",
         "4.1",
         "4.0"
        ],
        [
         "161",
         "SUPERDNA 3D LAB",
         "Data Analyst",
         "4.0",
         "Responsibilities\n\nUsing automated tools to extract data from primary and secondary sources\nRemoving corrupted data and fixing coding errors and related problems\nDeveloping and maintaining databases, data systems – reorganizing data in a readable format\nPerforming analysis to assess quality and meaning of data\nFilter Data by reviewing reports and performance indicators to identify and correct code problems\nUsing statistical tools to identify, analyze, and interpret patterns and trends in complex data sets that could be helpful for the diagnosis and prediction\nAssigning numerical value to essential business functions so that business performance can be assessed and compared over periods of time.\nAnalyzing National, and global trends that impact both the organization and the industry\nPreparing reports for the management stating trends, patterns, and predictions using relevant data\nWorking with 3D Technical leads, and management heads to identify process improvement opportunities, propose system modifications, and devise data governance strategies.\nPreparing final analysis reports for the stakeholders to understand the data-analysis steps, enabling them to take important decisions based on various facts and trends.\n\nSkills\n\nKnowledge of programming language\nMust have good analytical skills, logical ability and Reasoning skills\nMust have experience in Advance Excel and Google spreadsheets\nGood to have knowledge of Google cloud platform and data studio.\nAny Data Visualization Tool (Power BI/Tableau) Will be a plus.",
         "Chandigarh",
         "467012.0",
         "/yr (est.)",
         "1 to 50 Employees",
         "2017",
         "Company - Private",
         "Information Technology Support Services",
         "Information Technology",
         "Unknown / Non-Applicable",
         "4.1",
         "3.5",
         "4.2",
         "4.0",
         "4.2"
        ],
        [
         "164",
         "UnitedHealthcare",
         "Data Analyst 2",
         "3.6",
         "Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together.\nPrimary Responsibilities:\nAnticipate customer needs and proactively develop solutions to meet them\nServe as a key resource on complex and / or critical issues\nSolve complex problems and develop innovative solutions\nProvide explanations and information to others on the most complex issues\nCreate Benchmark ETL and reporting - standard and ad hoc needs\nDevelop, build, and maintain reporting of key metrics\nConsult and document business requirements to support reporting needs\nUnderstand basic relational data structures\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\nRequired Qualifications:\n7+ years of experience writing T-SQL queries, stored procedures, and complex scalar-value functions\n6+ years of experience using SSIS/ADF tools for ETL solutions\n6+ years of experience gathering and documenting requirements\n6+ years of experience with process improvement, workflow, benchmarking, and/or evaluation of business processes\nWorking experience on Databricks, Kafka, Python and ML libraries\nExperience on CI/CD implementation\nExperience on SSIS to ADF migration\nAny source control tool experience like Github/TFS\nAzure cloud server knowledge\nKnowledge on Query performance tuning and optimization\nKnowledge on consuming stream data from hub/snowflake server to on-prem\nProven solid statistics and analytical knowledge\nProven solid written and verbal communication skills\nPreferred Qualifications:\nExperience building dashboard using Power BI\nKnowledge on Microsoft azure cloud environment\nKnowledge on U.S healthcare domain\nAt UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.",
         "Bengaluru",
         "663325.0",
         "/yr (est.)",
         "10000+ Employees",
         "--",
         "Company - Public",
         "Healthcare Services & Hospitals",
         "Healthcare",
         "Unknown / Non-Applicable",
         "3.6",
         "3.5",
         "3.7",
         "3.3",
         "3.6"
        ],
        [
         "168",
         "Nangia & Co LLP",
         "Data Analyst",
         "4.0",
         "Job Description\n\nDesignation\nData Analyst\n\nLocation\nChennai\n\nEducation\nPostgraduate in Econometrics, Economics, Statistics, Computer Science or related field from a recognized institution\nExperience\nrelated field from a recognized institution Minimum of 5 years’ experience. She/He should have relevant experience in handling survey and secondary datasets. She/he should have experience handling spatial datasets, cleaning, mapping, and analysing them through relevant software like Python, R, etc.\nApply Here",
         "Chennai",
         "275768.0",
         "/yr (est.)",
         "501 to 1000 Employees",
         "1984",
         "Private Practice / Firm",
         "Accounting & Tax",
         "Finance",
         "$100 to $500 million (USD)",
         "3.5",
         "3.4",
         "3.3",
         "3.0",
         "3.5"
        ],
        [
         "179",
         "iXie Gaming",
         "Game Data Analyst",
         "4.3",
         "Job Information\nRSD NO\n6840\nIndustry\nGame Development\nMin Experience\n3\nMax Experience\n8\nCity\nBangalore North\nState/Province\nKarnataka\nCountry\nIndia\nZip/Postal Code\n560002\nJob Description\nUse data to deliver fundamental insights on the player and game performance. Identify and formalise problems related to player behaviour, acquisition and churn. Communicate your impactful findings in visual form to technical and non-technical team members demonstrating impactful and visual insights. ROLE & RESPONSIBILITIES -Analyze and acquire a deep understanding of game features/user behaviour/metric trends through data and create impactful, actionable and valuable insights to drive product decisions -Understanding business requirements and implementing analytical solutions & techniques. -Designing and analysing experiments to test new product ideas and define the KPIs that measure the game’s performance in a meaningful way. -Develop algorithms and predictive models to solve critical business problems and test feasibility of solution approach -Own the design, development, and maintenance of ongoing reports, dashboards, etc. -Developing tools and libraries that will help the team to improve efficiency.\nIndium Software is an Equal Opportunity Employer and does not discriminate on the basis of race or ethnicity, religion, sex, national origin, age, veteran disability or genetic information or any other reason prohibited by law in employment.",
         "Bengaluru",
         "649084.0",
         "/yr (est.)",
         "501 to 1000 Employees",
         "2011",
         "Company - Private",
         "Information Technology Support Services",
         "Information Technology",
         "Unknown / Non-Applicable",
         "4.3",
         "3.9",
         "4.3",
         "4.2",
         "4.4"
        ],
        [
         "180",
         "JPMorgan Chase & Co",
         "Know Your Customer - Client Data Analyst",
         "4.0",
         "JOB DESCRIPTION\n\nAs a Know Your Customer Analyst in Wholesale Know Your Customer Operations, you will be responsible to review the records completed by production team to ensure all client Know Your Customer records are compliant with regulatory standards, and will ensure high quality and timely completion of all client-level due diligence You will assist in end to end operations review and the client-facing team. You will perform a research to ensure a client’s Know Your Customer profile is appropriately updated and any discrepancies or issues with the profile escalated to the appropriate lines of business.\nYou will be responsible for ascertaining that all Know Your Customer and Anti Money Laundering policies are adhered to and will be responsible for conducting reviews of client’s Know Your Customer documentation stored in internal repositories and publicly available information, performing the necessary screening against relevant search engines and sanction lists and communicate any additional deficiencies back to the business. Know Your Customer Analyst , you are expected to stay current with all regulatory changes and requirements around client Know Your Customer Analyst, Suitability and Documentation.\nJob responsibilities\nReview of confidential client data via publicly available and internal sources with understanding of the firm’s Know Your Customer requirements when reviewing documentation inclusive of Customer Identification Program (CIP), Standard Due Diligence (SDD), Enhanced Due Diligence (EDD), Local Due Diligence (LDD), Specialized Due Diligence(SpDD) and Product Due Diligence requirements (PDD)\nCompare and contrast differences within Know Your Customer records, highlight amendments required to production team and also highlight escalate to compliance in case of Anti Money Laundering Red flags\nEnsure the review of the records are complete within the due date and also Service Level Agreement deadlines and comment on trends/behavior relating to account due diligence and activity review\nExhibit the highest standards of customer service to our internal and external customers (inclusive of confidentiality) and meet firm wide quality standards\nHandling and maintenance of confidential client documentation and communicating/escalating issues to management when applicable\nBe part of the holistic client review team to follow up on outstanding alignment items within various Line of Business and other teams to close loop on the alignment\nBe flexible to work on other migration/remediation projects including Screening resolution , other Line of business holistic alignment or Quality Assurance and ability to comprehend the Know Your Customer risk factors and draft the overall risk summary for the client\nABOUT US\n\nJPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world’s most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants’ and employees’ religious practices and beliefs, as well as any mental health or physical disability needs.\n\n\n\nABOUT THE TEAM\n\nThe Corporate & Investment Bank is a global leader across investment banking, wholesale payments, markets and securities services. The world’s most important corporations, governments and institutions entrust us with their business in more than 100 countries. We provide strategic advice, raise capital, manage risk and extend liquidity in markets around the world.",
         "Bengaluru",
         "657267.0",
         "/yr (est.)",
         "10000+ Employees",
         "1799",
         "Company - Public",
         "Banking & Lending",
         "Finance",
         "$10+ billion (USD)",
         "4.0",
         "3.9",
         "3.9",
         "3.6",
         "3.7"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 151
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_description</th>\n",
       "      <th>location</th>\n",
       "      <th>salary_avg_estimate</th>\n",
       "      <th>salary_estimate_payperiod</th>\n",
       "      <th>company_size</th>\n",
       "      <th>company_founded</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>industry</th>\n",
       "      <th>sector</th>\n",
       "      <th>revenue</th>\n",
       "      <th>career_opportunities_rating</th>\n",
       "      <th>comp_and_benefits_rating</th>\n",
       "      <th>culture_and_values_rating</th>\n",
       "      <th>senior_management_rating</th>\n",
       "      <th>work_life_balance_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABB</td>\n",
       "      <td>Junior Data Analyst</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Junior Data Analyst\\nTake your next career ste...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>325236.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>10000+ Employees</td>\n",
       "      <td>1883</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Electronics Manufacturing</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>$10+ billion (USD)</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>News Corp</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>3.6</td>\n",
       "      <td>Job Description :\\nJob Title: Analytics Engine...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>522206.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>10000+ Employees</td>\n",
       "      <td>2013</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>Media &amp; Communication</td>\n",
       "      <td>$5 to $10 billion (USD)</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Infosys</td>\n",
       "      <td>Healthcare Data Analyst</td>\n",
       "      <td>3.8</td>\n",
       "      <td>A day in the life of an Infoscion • As part of...</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>533713.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>10000+ Employees</td>\n",
       "      <td>1981</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Information Technology Support Services</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>$10+ billion (USD)</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>JPMorgan Chase &amp; Co</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>4.0</td>\n",
       "      <td>JOB DESCRIPTION\\n\\nYou will be part of a team ...</td>\n",
       "      <td>India</td>\n",
       "      <td>539530.5</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>10000+ Employees</td>\n",
       "      <td>1799</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Banking &amp; Lending</td>\n",
       "      <td>Finance</td>\n",
       "      <td>$10+ billion (USD)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Alcon</td>\n",
       "      <td>Data Analyst - Digital Health</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Key Responsibilities\\nCollect and analyze larg...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>784161.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>10000+ Employees</td>\n",
       "      <td>1945</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Biotech &amp; Pharmaceuticals</td>\n",
       "      <td>Pharmaceutical &amp; Biotechnology</td>\n",
       "      <td>$5 to $10 billion (USD)</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>Creative Synergies Group</td>\n",
       "      <td>Data Analyst-Power BI</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Role\\nData Analyst-Power BI\\n\\nExperience\\n9 –...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>761723.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>1001 to 5000 Employees</td>\n",
       "      <td>2011</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Software Development</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>Heads Up For Tails</td>\n",
       "      <td>Inventory Manager- Data Analyst</td>\n",
       "      <td>4.3</td>\n",
       "      <td>About Us\\nHeads Up For Tails is a one-stop pet...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>690600.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>1001 to 5000 Employees</td>\n",
       "      <td>2008</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Pet &amp; Pet Supplies Stores</td>\n",
       "      <td>Retail &amp; Wholesale</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>KRSNAA Diagnostics</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>3.3</td>\n",
       "      <td>Experience: 1-5 Yrs\\nJob Location: Pune Head O...</td>\n",
       "      <td>Pune</td>\n",
       "      <td>182166.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>--</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Medical Testing &amp; Clinical Laboratories</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Quest Global</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Quest Global is an organization at the forefro...</td>\n",
       "      <td>Belgaum</td>\n",
       "      <td>415258.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>10000+ Employees</td>\n",
       "      <td>1997</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Information Technology Support Services</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>$500 million to $1 billion (USD)</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>Thoucentric</td>\n",
       "      <td>Data Analyst (Advanced SQL Experience)</td>\n",
       "      <td>4.1</td>\n",
       "      <td>About us\\nThoucentric is the Consulting arm of...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>921654.0</td>\n",
       "      <td>/yr (est.)</td>\n",
       "      <td>201 to 500 Employees</td>\n",
       "      <td>2015</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Business Consulting</td>\n",
       "      <td>Management &amp; Consulting</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      company                               job_title  \\\n",
       "0                         ABB                     Junior Data Analyst   \n",
       "13                  News Corp                            Data Analyst   \n",
       "25                    Infosys                 Healthcare Data Analyst   \n",
       "27        JPMorgan Chase & Co                            Data Analyst   \n",
       "28                      Alcon           Data Analyst - Digital Health   \n",
       "..                        ...                                     ...   \n",
       "580  Creative Synergies Group                   Data Analyst-Power BI   \n",
       "586        Heads Up For Tails         Inventory Manager- Data Analyst   \n",
       "597        KRSNAA Diagnostics                            Data Analyst   \n",
       "598              Quest Global                            Data Analyst   \n",
       "606               Thoucentric  Data Analyst (Advanced SQL Experience)   \n",
       "\n",
       "     company_rating                                    job_description  \\\n",
       "0               4.0  Junior Data Analyst\\nTake your next career ste...   \n",
       "13              3.6  Job Description :\\nJob Title: Analytics Engine...   \n",
       "25              3.8  A day in the life of an Infoscion • As part of...   \n",
       "27              4.0  JOB DESCRIPTION\\n\\nYou will be part of a team ...   \n",
       "28              3.8  Key Responsibilities\\nCollect and analyze larg...   \n",
       "..              ...                                                ...   \n",
       "580             3.8  Role\\nData Analyst-Power BI\\n\\nExperience\\n9 –...   \n",
       "586             4.3  About Us\\nHeads Up For Tails is a one-stop pet...   \n",
       "597             3.3  Experience: 1-5 Yrs\\nJob Location: Pune Head O...   \n",
       "598             3.7  Quest Global is an organization at the forefro...   \n",
       "606             4.1  About us\\nThoucentric is the Consulting arm of...   \n",
       "\n",
       "      location  salary_avg_estimate salary_estimate_payperiod  \\\n",
       "0    Bengaluru             325236.0                /yr (est.)   \n",
       "13   Bengaluru             522206.0                /yr (est.)   \n",
       "25     Chennai             533713.0                /yr (est.)   \n",
       "27       India             539530.5                /yr (est.)   \n",
       "28   Bengaluru             784161.0                /yr (est.)   \n",
       "..         ...                  ...                       ...   \n",
       "580  Bengaluru             761723.0                /yr (est.)   \n",
       "586  Bengaluru             690600.0                /yr (est.)   \n",
       "597       Pune             182166.0                /yr (est.)   \n",
       "598    Belgaum             415258.0                /yr (est.)   \n",
       "606  Bengaluru             921654.0                /yr (est.)   \n",
       "\n",
       "               company_size company_founded    employment_type  \\\n",
       "0          10000+ Employees            1883   Company - Public   \n",
       "13         10000+ Employees            2013   Company - Public   \n",
       "25         10000+ Employees            1981   Company - Public   \n",
       "27         10000+ Employees            1799   Company - Public   \n",
       "28         10000+ Employees            1945   Company - Public   \n",
       "..                      ...             ...                ...   \n",
       "580  1001 to 5000 Employees            2011  Company - Private   \n",
       "586  1001 to 5000 Employees            2008  Company - Private   \n",
       "597                 Unknown              --  Company - Private   \n",
       "598        10000+ Employees            1997  Company - Private   \n",
       "606    201 to 500 Employees            2015  Company - Private   \n",
       "\n",
       "                                    industry                          sector  \\\n",
       "0                  Electronics Manufacturing                   Manufacturing   \n",
       "13                                Publishing           Media & Communication   \n",
       "25   Information Technology Support Services          Information Technology   \n",
       "27                         Banking & Lending                         Finance   \n",
       "28                 Biotech & Pharmaceuticals  Pharmaceutical & Biotechnology   \n",
       "..                                       ...                             ...   \n",
       "580                     Software Development          Information Technology   \n",
       "586                Pet & Pet Supplies Stores              Retail & Wholesale   \n",
       "597  Medical Testing & Clinical Laboratories                      Healthcare   \n",
       "598  Information Technology Support Services          Information Technology   \n",
       "606                      Business Consulting         Management & Consulting   \n",
       "\n",
       "                              revenue  career_opportunities_rating  \\\n",
       "0                  $10+ billion (USD)                          3.7   \n",
       "13            $5 to $10 billion (USD)                          3.5   \n",
       "25                 $10+ billion (USD)                          3.8   \n",
       "27                 $10+ billion (USD)                          4.0   \n",
       "28            $5 to $10 billion (USD)                          3.4   \n",
       "..                                ...                          ...   \n",
       "580          Unknown / Non-Applicable                          3.9   \n",
       "586          Unknown / Non-Applicable                          4.0   \n",
       "597          Unknown / Non-Applicable                          3.4   \n",
       "598  $500 million to $1 billion (USD)                          3.7   \n",
       "606          Unknown / Non-Applicable                          3.8   \n",
       "\n",
       "     comp_and_benefits_rating  culture_and_values_rating  \\\n",
       "0                         3.6                        4.0   \n",
       "13                        3.4                        3.3   \n",
       "25                        3.0                        4.0   \n",
       "27                        3.9                        3.9   \n",
       "28                        3.9                        3.6   \n",
       "..                        ...                        ...   \n",
       "580                       3.6                        3.8   \n",
       "586                       3.7                        3.9   \n",
       "597                       3.0                        3.1   \n",
       "598                       3.2                        3.8   \n",
       "606                       3.5                        4.5   \n",
       "\n",
       "     senior_management_rating  work_life_balance_rating  \n",
       "0                         3.5                       3.9  \n",
       "13                        3.2                       3.5  \n",
       "25                        3.5                       3.7  \n",
       "27                        3.6                       3.7  \n",
       "28                        3.3                       3.5  \n",
       "..                        ...                       ...  \n",
       "580                       3.9                       3.8  \n",
       "586                       3.8                       3.8  \n",
       "597                       3.0                       2.9  \n",
       "598                       3.5                       3.7  \n",
       "606                       4.1                       4.0  \n",
       "\n",
       "[151 rows x 18 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"job_title\"].str.contains(\".*[D|d]ata [A|a]nalyst.*\", regex = True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e138d",
   "metadata": {},
   "source": [
    "## Personally I feel this dataset can be honed further more. but it's time cosuming process. so for my current project i'm stopping here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87b8f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ccad2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff914234",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"company_founded\"] = test_data.where(test_data[\"company_founded\"] != \"--\")[\"company_founded\"]\n",
    "test_data.dropna(subset=[\"company_founded\"], inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "796ad71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(r\"D:\\4.Data Wrangling\\Datasets\\Ds_jobs\\cleaned_train_data.csv\", index=False)\n",
    "test_data.to_csv(r\"D:\\4.Data Wrangling\\Datasets\\Ds_jobs\\cleaned_test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71bf946b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60e1ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(r\"D:\\4.Data Wrangling\\Datasets\\Ds_jobs\\cleaned_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b89cb4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "3aaf39c4-63bd-4929-bc2f-888434ca2c36",
       "rows": [
        [
         "company",
         "0"
        ],
        [
         "job_title",
         "0"
        ],
        [
         "company_rating",
         "0"
        ],
        [
         "job_description",
         "0"
        ],
        [
         "location",
         "0"
        ],
        [
         "salary_avg_estimate",
         "0"
        ],
        [
         "salary_estimate_payperiod",
         "0"
        ],
        [
         "company_size",
         "0"
        ],
        [
         "company_founded",
         "0"
        ],
        [
         "employment_type",
         "0"
        ],
        [
         "industry",
         "0"
        ],
        [
         "sector",
         "0"
        ],
        [
         "revenue",
         "0"
        ],
        [
         "career_opportunities_rating",
         "0"
        ],
        [
         "comp_and_benefits_rating",
         "0"
        ],
        [
         "culture_and_values_rating",
         "0"
        ],
        [
         "senior_management_rating",
         "0"
        ],
        [
         "work_life_balance_rating",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 18
       }
      },
      "text/plain": [
       "company                        0\n",
       "job_title                      0\n",
       "company_rating                 0\n",
       "job_description                0\n",
       "location                       0\n",
       "salary_avg_estimate            0\n",
       "salary_estimate_payperiod      0\n",
       "company_size                   0\n",
       "company_founded                0\n",
       "employment_type                0\n",
       "industry                       0\n",
       "sector                         0\n",
       "revenue                        0\n",
       "career_opportunities_rating    0\n",
       "comp_and_benefits_rating       0\n",
       "culture_and_values_rating      0\n",
       "senior_management_rating       0\n",
       "work_life_balance_rating       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "916903a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Analyst'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['job_title'].iloc[112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "547ad768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "112",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "990365f4-2f2f-446f-8892-a6ad74f52f34",
       "rows": [
        [
         "company",
         "Anion Healthcare BPO"
        ],
        [
         "job_title",
         "Data Analyst"
        ],
        [
         "company_rating",
         "4.2"
        ],
        [
         "job_description",
         "Qualification:\nAny graduate with 2-5 years experience in data analysis, decision support, including demonstrated proficiency with analytical software.\nProficient in PL/SQL, PowerPivot, Excel, VBA, Macros, MySQL DB and Microsoft PowerPoint.\nStrong communication skills and should be fluent in written and spoken English.\nJOB ROLE & RESPONSIBILITIES:\nPerform complex data analysis in support of ad-hoc and standing customer requests.\nDeliver data products in report/ presentation format, or verbally, to management's specifications and timelines.\nWrite excel and SQL programs to analyze and extract valuable information from healthcare data.\nPerform basic statistical analyses for projects and reports.\nDevelop graphs, reports, and presentations of project results.\nCreate and present quality dashboards.\nPromote an image of a high quality organization through expertise and responsiveness.\nTakes responsibility for assignment completion and follow-through.\nGood working relationships with internal and external customers."
        ],
        [
         "location",
         "India"
        ],
        [
         "salary_avg_estimate",
         "539530.5"
        ],
        [
         "salary_estimate_payperiod",
         "/yr (est.)"
        ],
        [
         "company_size",
         "1 to 50 Employees"
        ],
        [
         "company_founded",
         "1999"
        ],
        [
         "employment_type",
         "Company - Private"
        ],
        [
         "industry",
         "Computer Hardware Development"
        ],
        [
         "sector",
         "Information Technology"
        ],
        [
         "revenue",
         "Unknown / Non-Applicable"
        ],
        [
         "career_opportunities_rating",
         "4.1"
        ],
        [
         "comp_and_benefits_rating",
         "4.1"
        ],
        [
         "culture_and_values_rating",
         "4.1"
        ],
        [
         "senior_management_rating",
         "4.2"
        ],
        [
         "work_life_balance_rating",
         "3.9"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 18
       }
      },
      "text/plain": [
       "company                                                     Anion Healthcare BPO\n",
       "job_title                                                           Data Analyst\n",
       "company_rating                                                               4.2\n",
       "job_description                Qualification:\\nAny graduate with 2-5 years ex...\n",
       "location                                                                   India\n",
       "salary_avg_estimate                                                     539530.5\n",
       "salary_estimate_payperiod                                             /yr (est.)\n",
       "company_size                                                   1 to 50 Employees\n",
       "company_founded                                                             1999\n",
       "employment_type                                                Company - Private\n",
       "industry                                           Computer Hardware Development\n",
       "sector                                                    Information Technology\n",
       "revenue                                                 Unknown / Non-Applicable\n",
       "career_opportunities_rating                                                  4.1\n",
       "comp_and_benefits_rating                                                     4.1\n",
       "culture_and_values_rating                                                    4.1\n",
       "senior_management_rating                                                     4.2\n",
       "work_life_balance_rating                                                     3.9\n",
       "Name: 112, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.iloc[112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9dc3ab5",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "(mysql.connector.errors.DatabaseError) 2005 (HY000): Unknown MySQL server host 'MySql.@localhost' (11003)\n(Background on this error at: https://sqlalche.me/e/20/4xp6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMySQLInterfaceError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\mysql\\connector\\connection_cext.py:354\u001b[0m, in \u001b[0;36mCMySQLConnection._open_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmysql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcnx_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cmysql\u001b[38;5;241m.\u001b[39mconverter_str_fallback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_converter_str_fallback\n",
      "\u001b[1;31mMySQLInterfaceError\u001b[0m: Unknown MySQL server host 'MySql.@localhost' (11003)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:146\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mloaded_dbapi\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3298\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3277\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[0;32m   3278\u001b[0m \n\u001b[0;32m   3279\u001b[0m \u001b[38;5;124;03mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3296\u001b[0m \n\u001b[0;32m   3297\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:449\u001b[0m, in \u001b[0;36mPool.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m \n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1264\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[1;32m-> 1264\u001b[0m     fairy \u001b[38;5;241m=\u001b[39m \u001b[43m_ConnectionRecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:713\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 713\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:179\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:390\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:675\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalize_callback \u001b[38;5;241m=\u001b[39m deque()\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:901\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:897\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarttime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbapi_connection \u001b[38;5;241m=\u001b[39m connection \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    898\u001b[0m pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, connection)\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:646\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[1;34m(connection_record)\u001b[0m\n\u001b[0;32m    644\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[1;32m--> 646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:625\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[1;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcargs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DBAPIConnection:\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloaded_dbapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\mysql\\connector\\pooling.py:322\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CMySQLConnection \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_pure:\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCMySQLConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MySQLConnection(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\mysql\\connector\\connection_cext.py:142\u001b[0m, in \u001b[0;36mCMySQLConnection.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\mysql\\connector\\abstracts.py:1605\u001b[0m, in \u001b[0;36mMySQLConnectionAbstract.connect\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisconnect()\n\u001b[1;32m-> 1605\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1607\u001b[0m charset, collation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1608\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1609\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1610\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\mysql\\connector\\connection_cext.py:360\u001b[0m, in \u001b[0;36mCMySQLConnection._open_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(err, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 360\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m get_mysql_exception(\n\u001b[0;32m    361\u001b[0m         err\u001b[38;5;241m.\u001b[39merrno, msg\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mmsg, sqlstate\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39msqlstate\n\u001b[0;32m    362\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InterfaceError(\u001b[38;5;28mstr\u001b[39m(err)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: 2005 (HY000): Unknown MySQL server host 'MySql.@localhost' (11003)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_engine\n\u001b[0;32m      3\u001b[0m engine  \u001b[38;5;241m=\u001b[39m create_engine(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmysql+mysqlconnector://root:1298504@MySql.@localhost:3306/ds_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m      6\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m     jobs_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(query, conn)\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3274\u001b[0m, in \u001b[0;36mEngine.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Connection:\n\u001b[0;32m   3252\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[0;32m   3253\u001b[0m \n\u001b[0;32m   3254\u001b[0m \u001b[38;5;124;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3271\u001b[0m \n\u001b[0;32m   3272\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:148\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mraw_connection()\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mloaded_dbapi\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 148\u001b[0m         \u001b[43mConnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception_noconnection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2439\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception_noconnection\u001b[1;34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[0m\n\u001b[0;32m   2437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[0;32m   2438\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2441\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:146\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mloaded_dbapi\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    148\u001b[0m         Connection\u001b[38;5;241m.\u001b[39m_handle_dbapi_exception_noconnection(\n\u001b[0;32m    149\u001b[0m             err, dialect, engine\n\u001b[0;32m    150\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3298\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraw_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PoolProxiedConnection:\n\u001b[0;32m   3277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[0;32m   3278\u001b[0m \n\u001b[0;32m   3279\u001b[0m \u001b[38;5;124;03m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3296\u001b[0m \n\u001b[0;32m   3297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:449\u001b[0m, in \u001b[0;36mPool.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PoolProxiedConnection:\n\u001b[0;32m    442\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m \n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1264\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_checkout\u001b[39m(\n\u001b[0;32m   1258\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1261\u001b[0m     fairy: Optional[_ConnectionFairy] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ConnectionFairy:\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[1;32m-> 1264\u001b[0m         fairy \u001b[38;5;241m=\u001b[39m \u001b[43m_ConnectionRecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1267\u001b[0m             threadconns\u001b[38;5;241m.\u001b[39mcurrent \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(fairy)\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:713\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    711\u001b[0m     rec \u001b[38;5;241m=\u001b[39m cast(_ConnectionRecord, pool\u001b[38;5;241m.\u001b[39m_do_get())\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 713\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     dbapi_connection \u001b[38;5;241m=\u001b[39m rec\u001b[38;5;241m.\u001b[39mget_connection()\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:179\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection()\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inc_overflow():\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:390\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ConnectionPoolEntry:\n\u001b[0;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:675\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pool \u001b[38;5;241m=\u001b[39m pool\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalize_callback \u001b[38;5;241m=\u001b[39m deque()\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:901\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:897\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarttime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 897\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbapi_connection \u001b[38;5;241m=\u001b[39m connection \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    898\u001b[0m     pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, connection)\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:646\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[1;34m(connection_record)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[1;32m--> 646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:625\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[1;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcargs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DBAPIConnection:\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloaded_dbapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\mysql\\connector\\pooling.py:322\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(ERROR_NO_CEXT)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CMySQLConnection \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_pure:\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCMySQLConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MySQLConnection(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\mysql\\connector\\connection_cext.py:142\u001b[0m, in \u001b[0;36mCMySQLConnection.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\mysql\\connector\\abstracts.py:1605\u001b[0m, in \u001b[0;36mMySQLConnectionAbstract.connect\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisconnect()\n\u001b[1;32m-> 1605\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1607\u001b[0m charset, collation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1608\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1609\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1610\u001b[0m )\n\u001b[0;32m   1611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m charset \u001b[38;5;129;01mor\u001b[39;00m collation:\n",
      "File \u001b[1;32mc:\\Users\\Mathan\\anaconda3\\envs\\das\\Lib\\site-packages\\mysql\\connector\\connection_cext.py:360\u001b[0m, in \u001b[0;36mCMySQLConnection._open_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MySQLInterfaceError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(err, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 360\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m get_mysql_exception(\n\u001b[0;32m    361\u001b[0m             err\u001b[38;5;241m.\u001b[39merrno, msg\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mmsg, sqlstate\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39msqlstate\n\u001b[0;32m    362\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InterfaceError(\u001b[38;5;28mstr\u001b[39m(err)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_handshake()\n",
      "\u001b[1;31mDatabaseError\u001b[0m: (mysql.connector.errors.DatabaseError) 2005 (HY000): Unknown MySQL server host 'MySql.@localhost' (11003)\n(Background on this error at: https://sqlalche.me/e/20/4xp6)"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine  = create_engine(\"mysql+mysqlconnector://root:1298504@MySql.@localhost:3306/ds_jobs\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    query = \"SELECT * FROM jobs\"\n",
    "    jobs_data = pd.read_sql(query, conn)\n",
    "    print(jobs_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "das",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
